{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "start.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPGUb6ioBJyFj1NrykzwBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amir-D-Shadow/Google-Colab/blob/main/start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRE50iEku0BT"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUe9YXzPHbT1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from numba import jit\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import cv2\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnvmYrenHnhU"
      },
      "source": [
        "def foo(a):\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if a > 8:\n",
        "\n",
        "      break\n",
        "\n",
        "    a = a + 1\n",
        "\n",
        "    yield a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12q0tZuBpIR1"
      },
      "source": [
        "for i in foo(3):\n",
        "\n",
        "  print((i is None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDYJFWHR2bl"
      },
      "source": [
        "class CBS(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding='valid',**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBS,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #convolution layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by sigmoid\n",
        "      output_sigmoid = tf.keras.activations.sigmoid(BN_x)\n",
        "\n",
        "      return output_sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63_vysbRo8s7"
      },
      "source": [
        "class CBL(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding='valid',**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBL,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #convolution layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by Leaky relu\n",
        "      output_leaky_relu = self.output_leaky_relu(BN_x)\n",
        "\n",
        "      return output_leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faQvhqe7plsl"
      },
      "source": [
        "#activation Mish\n",
        "def Mish(x):\n",
        "\n",
        "   softplus = tf.math.softplus(x)\n",
        "   tanh_s = tf.math.tanh(softplus)\n",
        "\n",
        "   return (x * tanh_s)\n",
        "\n",
        "\n",
        "#CBM Module\n",
        "class CBM(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBM,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_Mish = tf.keras.layers.Lambda(Mish)\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Convolution 2D layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_Mish = self.output_Mish(BN_x)\n",
        "\n",
        "      return output_Mish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhMdg-bNpY7y"
      },
      "source": [
        "#Backbone CSPX\n",
        "class CSPX(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,CSPX_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      CSPX_info -- dictionary containing information: num_of_res_unit , res_unit block info , CBM block info , CBL_info\n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "      ------ CBL_1 ------ CBL_2 ------ res_unit * X ------ CBL_3 -----\n",
        "                     |                                               |\n",
        "                     |                                               |______\n",
        "                     |                                                ______  Concat --- BN --- leaky relu --- CBM_1 \n",
        "                     |                                               |\n",
        "                     |                                               |\n",
        "                     -------------------------------- CBL_4 ----------\n",
        "      \"\"\"\n",
        "\n",
        "      #initialization\n",
        "      super(CSPX,self).__init__(**kwargs)\n",
        "\n",
        "      #extract num_of_res_unit\n",
        "      self.num_of_res_unit = CSPX_info[\"num_of_res_unit\"]\n",
        "\n",
        "      #define layers\n",
        "\n",
        "      #res_unit\n",
        "      self.res_unit_seq = {}\n",
        "\n",
        "      #Important: When defining the CSPX layer, remember to define res unit info (dictionary key) in the form of res_unit_i : i start from 1\n",
        "      for i in range(1,self.num_of_res_unit+1):\n",
        "\n",
        "         #Extract res_unit_i info\n",
        "         res_unit_info = CSPX_info[f\"res_unit_{i}\"]\n",
        "\n",
        "         #define resunit layer\n",
        "         self.res_unit_seq[f\"res_unit_{i}\"] = res_unit(res_unit_info)\n",
        "         \n",
        "\n",
        "      #CBL_1\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_1\"]\n",
        "      \n",
        "      self.CBL_1 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_2\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_2\"]\n",
        "      \n",
        "      self.CBL_2 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_3\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_3\"]\n",
        "      \n",
        "      self.CBL_3 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_4\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_4\"]\n",
        "      \n",
        "      self.CBL_4 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #BN\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      #leaky relu\n",
        "      self.leaky_relu_x = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      #CBM_1\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBM_1\"]\n",
        "\n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      x = inputs\n",
        "\n",
        "      #CBL_1\n",
        "      CBL_1 = self.CBL_1(x,train_flag)\n",
        "\n",
        "      #CBL_2\n",
        "      CBL_2 = self.CBL_2(CBL_1,train_flag)\n",
        "\n",
        "      #res_unit block\n",
        "      res_unit_block = CBL_2\n",
        "      \n",
        "      for i in range(1,self.num_of_res_unit+1):\n",
        "\n",
        "         res_unit_block =  (self.res_unit_seq[f\"res_unit_{i}\"])(res_unit_block,train_flag) \n",
        "\n",
        "      #CBL3\n",
        "      CBL_3 = self.CBL_3(res_unit_block,train_flag)\n",
        "      \n",
        "      #CBL_4\n",
        "      CBL_4 = self.CBL_4(CBL_1,train_flag)\n",
        "\n",
        "      #Concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[CBL_3,CBL_4],axis=-1)\n",
        "\n",
        "      #Batch Normalization\n",
        "      BN_x = self.BN_x(mid_concat,training=train_flag)\n",
        "\n",
        "      #leaky_relu_x\n",
        "      leaky_relu_x = self.leaky_relu_x(BN_x)\n",
        "\n",
        "      #output_CBM\n",
        "      output_CBM = self.CBM_1(leaky_relu_x,train_flag)\n",
        "\n",
        "      return output_CBM\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "#Neck CSPX\n",
        "class CSPX_Neck(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,NECK_info,**kwargs):\n",
        "      \n",
        "      \"\"\"\n",
        "      NECK_info -- dictionary containing information: num_of_CBL, CBM block info , CBL block info , conv2D info\n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "      ----------- CBL * X ------ conv2D_1 ----------------\n",
        "         |                                               |\n",
        "         |                                               |______\n",
        "         |                                                ______  Concat --- BN --- leaky relu --- CBM_1 \n",
        "         |                                               |\n",
        "         |                                               |\n",
        "         -------------------------------- conv2D_2 -------\n",
        "         \n",
        "      \"\"\"\n",
        "      \n",
        "      #initialization\n",
        "      super(CSPX_Neck,self).__init__(**kwargs)\n",
        "\n",
        "      #Get num_of_CBL\n",
        "      self.num_of_CBL = NECK_info[\"num_of_CBL\"]\n",
        "\n",
        "      #define layers\n",
        "\n",
        "      #CBL_X\n",
        "      self.CBL_seq = {}\n",
        "\n",
        "      for i in range(1,self.num_of_CBL+1):\n",
        "\n",
        "         filters,kernel_size,strides,padding = NECK_info[f\"CBL_{i}\"]\n",
        "\n",
        "         self.CBL_seq[f\"CBL_{i}\"] = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "\n",
        "      #Conv2D_1\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"conv2D_1\"]\n",
        "      \n",
        "      self.conv2D_1 = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      #conv2D_2\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"conv2D_2\"]\n",
        "\n",
        "      self.conv2D_2 = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      #Batch Normalization\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "      \n",
        "      #leaky relu\n",
        "      self.leaky_relu_x = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      #CBM_1\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"CBM_1\"]\n",
        "\n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #CBL_X\n",
        "      CBL_block = inputs\n",
        "      \n",
        "      for i in range(1,self.num_of_CBL+1):\n",
        "\n",
        "         CBL_block = (self.CBL_seq[f\"CBL_{i}\"])(CBL_block,train_flag)\n",
        "\n",
        "\n",
        "      #conv2D_1\n",
        "      conv2D_1 = self.conv2D_1(CBL_block)\n",
        "\n",
        "      #conv2D_2\n",
        "      conv2D_2 = self.conv2D_2(inputs)\n",
        "\n",
        "      #concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[conv2D_1,conv2D_2],axis=-1)\n",
        "\n",
        "      #BN_x\n",
        "      BN_x = self.BN_x(mid_concat,training=train_flag)\n",
        "\n",
        "      #leaky relu\n",
        "      leaky_relu_x = self.leaky_relu_x(BN_x)\n",
        "\n",
        "      #CBM_1\n",
        "      output_CBM = self.CBM_1(leaky_relu_x,train_flag)\n",
        "\n",
        "      return output_CBM\n",
        "\n",
        "\n",
        "#revised CSP\n",
        "class rCSP(tf.keras.Model):\n",
        "\n",
        "\n",
        "   def __init__(self,rCSP_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      rCSP_info -- dictionary containing information:  CBL info \n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "                   ------- CBL_2 --- CBL_3 --- CBL_4 --- SPP_1 --- CBL_5 -----\n",
        "                   |                                                         |\n",
        "                   |                                                         |______\n",
        "         CBL_1  ---|                                                          ______  Concat --- CBL_7\n",
        "                   |                                                         |\n",
        "                   |                                                         |\n",
        "                   --------------------------- CBL_6 -------------------------\n",
        "         \n",
        "      \"\"\"\n",
        "      #initialization\n",
        "      super(rCSP,self).__init__(**kwargs)\n",
        "\n",
        "      #CBL_1\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_1\"]\n",
        "      \n",
        "      self.CBL_1 = CBL(filters,kernel_size,strides,padding)\n",
        "      \n",
        "      #CBL_2\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_2\"]\n",
        "\n",
        "      self.CBL_2 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_3\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_3\"]\n",
        "\n",
        "      self.CBL_3 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_4\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_4\"]\n",
        "\n",
        "      self.CBL_4 = CBL(filters,kernel_size,strides,padding)     \n",
        "\n",
        "      #SPP\n",
        "      self.SPP_1 = SPP()\n",
        "\n",
        "      #CBL_5\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_5\"]\n",
        "\n",
        "      self.CBL_5 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_6\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_6\"]\n",
        "\n",
        "      self.CBL_6 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_7\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_7\"]\n",
        "\n",
        "      self.CBL_7 = CBL(filters,kernel_size,strides,padding)\n",
        "      \n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #CBL_1\n",
        "      CBL_1 = self.CBL_1(inputs,train_flag)\n",
        "\n",
        "      #CBL_2\n",
        "      CBL_2 = self.CBL_2(CBL_1,train_flag)\n",
        "\n",
        "      #CBL_3\n",
        "      CBL_3 = self.CBL_3(CBL_2,train_flag)\n",
        "\n",
        "      #CBL_4\n",
        "      CBL_4 = self.CBL_4(CBL_3,train_flag)\n",
        "\n",
        "      #SPP_1\n",
        "      SPP_1 = self.SPP_1(CBL_4)\n",
        "\n",
        "      #CBL_5\n",
        "      CBL_5 = self.CBL_5(SPP_1,train_flag)\n",
        "\n",
        "      #CBL_6\n",
        "      CBL_6 = self.CBL_6(CBL_1,train_flag)\n",
        "\n",
        "      #concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[CBL_6,CBL_5],axis=-1)\n",
        "\n",
        "      #CBL_7\n",
        "      output_CBL_7 = self.CBL_7(mid_concat,train_flag)\n",
        "\n",
        "      \n",
        "      return output_CBL_7\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAG6mme5pgtf"
      },
      "source": [
        "class res_unit(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,block_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      block_info -- dictionary containing blocks' hyperparameters (filters,kernel_size,strides,padding)\n",
        "\n",
        "      Module Graph:\n",
        "\n",
        "      ------ CBM_1 ------ CBM_2 ------ Add\n",
        "         |                              |\n",
        "         |                              |\n",
        "         |                              |\n",
        "         --------------------------------\n",
        "\n",
        "      \n",
        "      \"\"\"\n",
        "\n",
        "      #initialization\n",
        "      super(res_unit,self).__init__(**kwargs)\n",
        "      \n",
        "      #1st CBM block\n",
        "      filters,kernel_size,strides,padding = block_info[\"CBM_1\"]\n",
        "      \n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #2nd CBM block\n",
        "      filters,kernel_size,strides,padding = block_info[\"CBM_2\"]\n",
        "\n",
        "      self.CBM_2 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #Add Layer\n",
        "      self.Add_layer = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      x = inputs\n",
        "\n",
        "      #1st CBM block\n",
        "      CBM_1 = self.CBM_1(inputs,train_flag)\n",
        "\n",
        "      #2nd CBM block\n",
        "      CBM_2 = self.CBM_2(CBM_1,train_flag)\n",
        "\n",
        "      #Add Layer\n",
        "      output_shortcut = self.Add_layer([CBM_2,x])\n",
        "\n",
        "      return output_shortcut\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33H-p75ep1Kf"
      },
      "source": [
        "class SPP(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(SPP,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.maxpool_5x5 = tf.keras.layers.MaxPooling2D(pool_size=5,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "      self.maxpool_9x9 = tf.keras.layers.MaxPooling2D(pool_size=9,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "      self.maxpool_13x13 = tf.keras.layers.MaxPooling2D(pool_size=13,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "\n",
        "   def call(self,inputs):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "      \n",
        "      #5x5\n",
        "      maxpool_5x5 = self.maxpool_5x5(inputs)\n",
        "\n",
        "      #9x9\n",
        "      maxpool_9x9 = self.maxpool_9x9(inputs)\n",
        "\n",
        "      #13x13\n",
        "      maxpool_13x13 = self.maxpool_13x13(inputs)\n",
        "\n",
        "      #concatenate\n",
        "      output_concat = tf.keras.layers.concatenate(inputs=[maxpool_5x5,maxpool_9x9,maxpool_13x13,inputs],axis=-1)\n",
        "\n",
        "      return output_concat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3bD4py_qElm"
      },
      "source": [
        "#TCBL Module\n",
        "class TCBL(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=1,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(TCBL,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_transpose_x = tf.keras.layers.Conv2DTranspose(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Transpose Convolution 2D layer\n",
        "      conv2D_transpose_x = self.conv2D_transpose_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_transpose_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_leaky_relu = self.output_leaky_relu(BN_x)\n",
        "\n",
        "      return output_leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woFJY6kxqHOB"
      },
      "source": [
        "#TCBM Module\n",
        "class TCBM(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(TCBM,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_transpose_x = tf.keras.layers.Conv2DTranspose(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_Mish = tf.keras.layers.Lambda(Mish)\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Transpose Convolution 2D layer\n",
        "      conv2D_transpose_x = self.conv2D_transpose_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_transpose_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_Mish = self.output_Mish(BN_x)\n",
        "\n",
        "      return output_Mish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1tqSp38U7mm"
      },
      "source": [
        "class alpha_model(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(alpha_model,self).__init__(**kwargs)\n",
        "\n",
        "      #CBM_1 in : 640 x 640 x 3 out: 640 x 640 x 32\n",
        "      filters=32\n",
        "      kernel_size=3\n",
        "      strides=1\n",
        "      padding=\"same\"\n",
        "      \n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CSP1 in : 640 x 640 x 32 out : 319 x 319 x 64\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 1\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (32,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (64,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (64,3,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (64,3,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #define CSP1\n",
        "      self.CSP1 = CSPX(CSPX_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CSP2 in : 319 x 319 x 32 out : 159 x 159 x 128\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 2\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (128,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (64,3,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (64,3,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #define CSP2\n",
        "      self.CSP2 = CSPX(CSPX_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CSP8_1 in : 159 x 159 x 128 out : 79 x 79 x 256\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 8\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "      res_unit_5 = {}\n",
        "      res_unit_5[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_5[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_5\"] = res_unit_5\n",
        "\n",
        "      res_unit_6 = {}\n",
        "      res_unit_6[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_6[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_6\"] = res_unit_6\n",
        "\n",
        "      res_unit_7 = {}\n",
        "      res_unit_7[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_7[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_7\"] = res_unit_7\n",
        "\n",
        "      res_unit_8 = {}\n",
        "      res_unit_8[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_8[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_8\"] = res_unit_8\n",
        "\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (256,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (128,3,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (128,3,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #define CSP8_1\n",
        "      self.CSP8_1 = CSPX(CSPX_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #spatialdropout_1 in : 79 x 79 x 256 out : 79 x 79 x 256 , ( branch 1 -- out : 79 x 79 x 256 )\n",
        "      self.SPA_drop_1 = tf.keras.layers.SpatialDropout2D(rate = 0,data_format=\"channels_last\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CSP8_2 in : 79 x 79 x 256 out : 39 x 39 x 512\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 8\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "      res_unit_5 = {}\n",
        "      res_unit_5[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_5[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_5\"] = res_unit_5\n",
        "\n",
        "      res_unit_6 = {}\n",
        "      res_unit_6[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_6[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_6\"] = res_unit_6\n",
        "\n",
        "      res_unit_7 = {}\n",
        "      res_unit_7[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_7[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_7\"] = res_unit_7\n",
        "\n",
        "      res_unit_8 = {}\n",
        "      res_unit_8[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_8[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_8\"] = res_unit_8\n",
        "\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (512,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (256,3,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (256,3,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #define CSP8_2\n",
        "      self.CSP8_2 = CSPX(CSPX_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #spatialdropout_2 in : 39 x 39 x 512 out : 39 x 39 x 512 , ( branch 2 -- out : 39 x 39 x 512 )\n",
        "      self.SPA_drop_2 = tf.keras.layers.SpatialDropout2D(rate = 0,data_format=\"channels_last\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CSP4 in : 39 x 39 x 512 out : 19 x 19 x 1024\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 4\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (1024,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (512,3,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (512,3,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (1024,1,1,\"same\")\n",
        "\n",
        "      #define CSP4\n",
        "      self.CSP4 = CSPX(CSPX_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #spatialdropout_3 in : 19 x 19 x 1024  out : 19 x 19 x 1024\n",
        "      self.SPA_drop_3 = tf.keras.layers.SpatialDropout2D(rate = 0,data_format=\"channels_last\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #rCSP1 in : 19 x 19 x 1024 out : 19 x 19 x 512 , ( branch 3 -- out: 19 x 19 x 512 )\n",
        "      rCSP_info = {}\n",
        "\n",
        "      rCSP_info[\"CBL_1\"] = (1024,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_2\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_3\"] = (1024,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_4\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_5\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_6\"] = (512,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_7\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #define rCSP1\n",
        "      self.rCSP1 = rCSP(rCSP_info)\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CBL_1 in : 19 x 19 x 512 out : 19 x 19 x 256\n",
        "      self.CBL_1 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #bilinear upsampling x4_1 in : 19 x 19 x 256 out : 76 x 76 x 256\n",
        "      self.upsample_bilinear_x4_1 = tf.keras.layers.UpSampling2D(size=4,interpolation=\"bilinear\",data_format = \"channels_last\")\n",
        "\n",
        "      #TCBM_1 in : 76 x 76 x 256 out : 80 x 80 x 256\n",
        "      self.TCBM_1 = TCBM(256,5,1,\"valid\")\n",
        "\n",
        "      #bilinear upsampling x2_1 in : 39 x 39 x 512 out : 78 x 78 x 512 - connect branch 2\n",
        "      self.upsample_bilinear_x2_1 = tf.keras.layers.UpSampling2D(size=2,interpolation=\"bilinear\",data_format = \"channels_last\")      \n",
        "\n",
        "      #TCBM_2 in : 78 x 78 x 512 out : 80 x 80 x 256\n",
        "      self.TCBM_2 = TCBM(256,3,1,\"valid\")\n",
        "\n",
        "      #concat TCBM_1 -- TCBM_2 , out: 80 x 80 x 512\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #rCSP2 in : 80 x 80 x 512 out : 39 x 39 x 256 ( branch 4 -- out: 39 x 39 x 256 )\n",
        "      rCSP_info = {}\n",
        "\n",
        "      rCSP_info[\"CBL_1\"] = (1024,3,2,\"valid\")\n",
        "      rCSP_info[\"CBL_2\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_3\"] = (1024,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_4\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_5\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_6\"] = (512,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_7\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #define rCSP2\n",
        "      self.rCSP2 = rCSP(rCSP_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #CBL_2 in : 39 x 39 x 256 out : 39 x 39 x 128\n",
        "      self.CBL_2 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #bilinear upsampling x2_2 in : 39 x 39 x 128 out : 78 x 78 x 128\n",
        "      self.upsample_bilinear_x2_2 = tf.keras.layers.UpSampling2D(size=2,interpolation=\"bilinear\",data_format = \"channels_last\")\n",
        "\n",
        "      #TCBM_3 in : 78 x 78 x 128 out : 80 x 80 x 128\n",
        "      self.TCBM_3 = TCBM(128,3,1,\"valid\")\n",
        "\n",
        "      #TCBM_4  in : 79 x 79 x 256 out : 80 x 80 x 128 - connect branch 1 \n",
        "      self.TCBM_4 = TCBM(128,2,1,\"valid\")\n",
        "\n",
        "      #concat TCBM_3 -- TCBM_4 , out: 80 x 80 x 256\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #rCSP3 in : 80 x 80 x 256 out : 80 x 80 x 128 ( branch 5 -- out: 80 x 80 x 128 )\n",
        "      rCSP_info = {}\n",
        "\n",
        "      rCSP_info[\"CBL_1\"] = (256,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_2\"] = (128,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_3\"] = (256,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_4\"] = (128,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_5\"] = (128,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_6\"] = (128,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_7\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #define rCSP3\n",
        "      self.rCSP3 = rCSP(rCSP_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #decouple head -- small object , in : 80 x 80 x 256  out: 80 x 80 x (1 + 2 + 2 + 80)\n",
        "\n",
        "      #reg\n",
        "      self.TCBL_reg_small  = TCBL(256,1,1,\"valid\")\n",
        "\n",
        "      self.CBL_left_small = CBL(2,3,1,\"same\")\n",
        "\n",
        "      self.CBL_center_small = CBL(2,3,1,\"same\")\n",
        "      \n",
        "      #class + prob\n",
        "      self.TCBL_clsp_small  = TCBL(256,1,1,\"valid\")\n",
        "\n",
        "      self.CBL_prob_small = CBL(1,3,1,\"same\")\n",
        "\n",
        "      self.CBL_class_small = CBL(80,3,1,\"same\")\n",
        "\n",
        "      self.CBS_prob_small = CBS(1,1,1,\"same\")\n",
        "\n",
        "      self.CBS_class_small =  CBS(80,1,1,\"same\")\n",
        "\n",
        "      #concat CBS_prob_small -- CBL_left_small -- CBL_center_small -- CBS_class_small  , out: 80 x 80 x 85\n",
        "\n",
        "      #output small\n",
        "      #self.conv2D_small = tf.keras.layers.Conv2D(85,1,1,padding=\"same\",data_format = \"channels_last\",name=\"output_small\")\n",
        "      \n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #connect_branch_5_CBL in : 80 x 80 x 128  out: 39 x 39 x 256\n",
        "      self.connect_branch_5_CBL = CBL(256,3,2,padding=\"valid\")\n",
        "      \n",
        "      #concat branch 4 -- branch 5  , out: 39 x 39 x 512\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #rCSP4 in : 39 x 39 x 512  out : 39 x 39 x 256 ( branch 6 -- out: 39 x 39 x 256 )\n",
        "      rCSP_info = {}\n",
        "\n",
        "      rCSP_info[\"CBL_1\"] = (512,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_2\"] = (256,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_3\"] = (512,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_4\"] = (256,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_5\"] = (256,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_6\"] = (256,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_7\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #define rCSP4\n",
        "      self.rCSP4 = rCSP(rCSP_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #decouple head -- medium object , in : 39 x 39 x 256  out: 40 x 40 x (1 + 2 + 2 + 2 + 2 + 80)\n",
        "\n",
        "      #reg\n",
        "      self.TCBL_reg_medium  = TCBL(256,2,1,\"valid\")\n",
        "\n",
        "      self.CBL_left_medium = CBL(2,3,1,\"same\")\n",
        "\n",
        "      self.CBL_center_medium = CBL(2,3,1,\"same\")\n",
        "\n",
        "      #class + prob\n",
        "      self.TCBL_clsp_medium  = TCBL(256,2,1,\"valid\")\n",
        "\n",
        "      self.CBL_prob_medium = CBL(1,3,1,\"same\")\n",
        "\n",
        "      self.CBL_class_medium = CBL(80,3,1,\"same\")\n",
        "\n",
        "      self.CBS_prob_medium = CBS(1,1,1,\"same\")\n",
        "\n",
        "      self.CBS_class_medium =  CBS(80,1,1,\"same\")\n",
        "      \n",
        "      #concat CBS_prob_medium  -- CBL_left_medium -- CBL_center_medium -- CBS_class_medium  , out: 40 x 40 x 85\n",
        "      \n",
        "      #output medium\n",
        "      #self.conv2D_medium = tf.keras.layers.Conv2D(85,1,1,padding=\"same\",data_format = \"channels_last\",name=\"output_medium\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #connect_branch_6_CBL in : 39 x 39 x 256  out: 19 x 19 x 512\n",
        "      self.connect_branch_6_CBL = CBL(512,3,2,padding=\"valid\")\n",
        "\n",
        "      #concat branch 3 -- branch 6  , out: 19 x 19 x 1024\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #rCSP5 in : 19 x 19 x 1024  out : 19 x 19 x 512 \n",
        "      rCSP_info = {}\n",
        "\n",
        "      rCSP_info[\"CBL_1\"] = (1024,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_2\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_3\"] = (1024,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_4\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_5\"] = (512,1,1,\"same\")\n",
        "      rCSP_info[\"CBL_6\"] = (512,3,1,\"same\")\n",
        "      rCSP_info[\"CBL_7\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #define rCSP5\n",
        "      self.rCSP5 = rCSP(rCSP_info)\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "\n",
        "      #decouple head -- large object , in : 19 x 19 x 512   out: 20 x 20 x (1 + 2 + 2 + 2 + 2 + 80)\n",
        "\n",
        "      #reg\n",
        "      self.TCBL_reg_large  = TCBL(512,2,1,\"valid\")\n",
        "\n",
        "      self.CBL_left_large = CBL(2,3,1,\"same\")\n",
        "\n",
        "      self.CBL_center_large = CBL(2,3,1,\"same\")\n",
        "\n",
        "      #class + prob\n",
        "      self.TCBL_clsp_large  = TCBL(512,2,1,\"valid\")\n",
        "\n",
        "      self.CBL_prob_large = CBL(1,3,1,\"same\")\n",
        "\n",
        "      self.CBL_class_large = CBL(80,3,1,\"same\")\n",
        "\n",
        "      self.CBS_prob_large = CBS(1,1,1,\"same\")\n",
        "\n",
        "      self.CBS_class_large =  CBS(80,1,1,\"same\")\n",
        "\n",
        "      #concat CBS_prob_large -- CBL_left_large -- CBL_center_large -- CBS_class_large  , out: 20 x 20 x 85\n",
        "      \n",
        "      #output large\n",
        "      #self.conv2D_large = tf.keras.layers.Conv2D(85,1,1,padding=\"same\",data_format = \"channels_last\",name=\"output_large\")\n",
        "\n",
        "      #----------------------------------------------------------------\n",
        "      \n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      #CBM_1\n",
        "      CBM_1 = self.CBM_1(inputs,train_flag)\n",
        "\n",
        "      #CSP1\n",
        "      CSP1 = self.CSP1(CBM_1,train_flag)\n",
        "\n",
        "      #CSP2\n",
        "      CSP2 = self.CSP2(CSP1,train_flag)\n",
        "\n",
        "      #CSP8_1\n",
        "      CSP8_1 = self.CSP8_1(CSP2,train_flag)\n",
        "\n",
        "      #spatialdropout_1 -- branch 1\n",
        "      SPA_drop_1 = self.SPA_drop_1(CSP8_1,training=train_flag)\n",
        "\n",
        "      #CSP8_2\n",
        "      CSP8_2 = self.CSP8_2(SPA_drop_1,train_flag)\n",
        "\n",
        "      #spatialdropout_2 -- branch 2\n",
        "      SPA_drop_2 = self.SPA_drop_2(CSP8_2,training=train_flag)\n",
        "\n",
        "      #CSP4\n",
        "      CSP4 = self.CSP4(SPA_drop_2,train_flag)\n",
        "\n",
        "      #spatialdropout_3 \n",
        "      SPA_drop_3 = self.SPA_drop_3(CSP4,training=train_flag)\n",
        "\n",
        "      #rCSP1 -- branch 3\n",
        "      rCSP1 = self.rCSP1(SPA_drop_3,train_flag)\n",
        "\n",
        "      #CBL_1\n",
        "      CBL_1 = self.CBL_1(rCSP1,train_flag)\n",
        "\n",
        "      #bilinear upsampling x4_1\n",
        "      upsample_bilinear_x4_1 = self.upsample_bilinear_x4_1(CBL_1)\n",
        "\n",
        "      #TCBM_1\n",
        "      TCBM_1 = self.TCBM_1(upsample_bilinear_x4_1,train_flag)\n",
        "      \n",
        "      #bilinear upsampling x2_1 - connect branch 2\n",
        "      upsample_bilinear_x2_1 = self.upsample_bilinear_x2_1(SPA_drop_2)\n",
        "\n",
        "      ##TCBM_2 \n",
        "      TCBM_2 = self.TCBM_2(upsample_bilinear_x2_1,train_flag)\n",
        "\n",
        "      #mid concat 1 -- concat TCBM_1 -- TCBM_2\n",
        "      mid_concat_1 = tf.keras.layers.concatenate(inputs=[TCBM_1,TCBM_2],axis=-1)\n",
        "\n",
        "      #rCSP2 -- branch 4\n",
        "      rCSP2 = self.rCSP2(mid_concat_1,train_flag)\n",
        "\n",
        "      #CBL_2\n",
        "      CBL_2 = self.CBL_2(rCSP2,train_flag)\n",
        "\n",
        "      #bilinear upsampling x2_2\n",
        "      upsample_bilinear_x2_2 = self.upsample_bilinear_x2_2(CBL_2)\n",
        "\n",
        "      #TCBM_3\n",
        "      TCBM_3 = self.TCBM_3(upsample_bilinear_x2_2,train_flag)\n",
        "\n",
        "      #TCBM_4 - connect branch 1\n",
        "      TCBM_4 = self.TCBM_4(SPA_drop_1,train_flag)\n",
        "\n",
        "      #mid concat 2 -- concat TCBM_3 -- TCBM_4\n",
        "      mid_concat_2 = tf.keras.layers.concatenate(inputs=[TCBM_3,TCBM_4],axis=-1)\n",
        "\n",
        "      #rCSP3 -- branch 5\n",
        "      rCSP3 = self.rCSP3(mid_concat_2,train_flag)\n",
        "\n",
        "      #decouple head -- small object\n",
        "\n",
        "      #reg -- small\n",
        "      TCBL_reg_small = self.TCBL_reg_small(rCSP3,train_flag)\n",
        "\n",
        "      CBL_left_small = self.CBL_left_small(TCBL_reg_small,train_flag)\n",
        "\n",
        "      CBL_center_small = self.CBL_center_small(TCBL_reg_small,train_flag)\n",
        "\n",
        "      #class + prob -- small\n",
        "      TCBL_clsp_small = self.TCBL_clsp_small(rCSP3,train_flag)\n",
        "\n",
        "      CBL_prob_small = self.CBL_prob_small(TCBL_clsp_small,train_flag)\n",
        "\n",
        "      CBL_class_small = self.CBL_class_small(TCBL_clsp_small,train_flag)\n",
        "\n",
        "      CBS_prob_small = self.CBS_prob_small(CBL_prob_small,train_flag)\n",
        "\n",
        "      CBS_class_small = self.CBS_class_small(CBL_class_small,train_flag)\n",
        "      \n",
        "      #concat CBS_prob_small -- CBL_left_small -- CBL_center_small -- CBS_class_small  , out: 80 x 80 x 85\n",
        "      #small_concat = tf.keras.layers.concatenate(inputs=[CBL_prob_small,CBL_left_small,CBL_center_small,CBL_class_small],axis=-1)\n",
        "\n",
        "      #**************** output small ****************\n",
        "      \n",
        "      #output_small = self.conv2D_small(small_concat)\n",
        "      output_small = tf.keras.layers.concatenate(inputs=[CBS_prob_small,CBL_left_small,CBL_center_small,CBS_class_small],axis=-1,name=\"output_small\")\n",
        "      \n",
        "      #**************** output small ****************\n",
        "\n",
        "      #connect_branch_5_CBL\n",
        "      connect_branch_5_CBL = self.connect_branch_5_CBL(rCSP3,train_flag)\n",
        "\n",
        "      #concat branch 4 -- branch 5  , out: 40 x 40 x 512\n",
        "      mid_concat_br45 = tf.keras.layers.concatenate(inputs=[rCSP2,connect_branch_5_CBL],axis=-1)\n",
        "\n",
        "      #rCSP4 -- branch 6\n",
        "      rCSP4 = self.rCSP4(mid_concat_br45,train_flag)\n",
        "\n",
        "      #decouple head -- medium object\n",
        "\n",
        "      #reg -- medium\n",
        "      TCBL_reg_medium = self.TCBL_reg_medium(rCSP4,train_flag)\n",
        "\n",
        "      CBL_left_medium = self.CBL_left_medium(TCBL_reg_medium,train_flag)\n",
        "\n",
        "      CBL_center_medium = self.CBL_center_medium(TCBL_reg_medium,train_flag)\n",
        "\n",
        "\n",
        "      #class + prob -- medium\n",
        "      TCBL_clsp_medium = self.TCBL_clsp_medium(rCSP4,train_flag)\n",
        "\n",
        "      CBL_prob_medium = self.CBL_prob_medium(TCBL_clsp_medium,train_flag)\n",
        "\n",
        "      CBL_class_medium = self.CBL_class_medium(TCBL_clsp_medium,train_flag)\n",
        "\n",
        "      CBS_prob_medium = self.CBS_prob_medium(CBL_prob_medium,train_flag)\n",
        "\n",
        "      CBS_class_medium = self.CBS_class_medium(CBL_class_medium,train_flag)\n",
        "\n",
        "\n",
        "      #concat CBS_prob_medium  -- CBL_left_medium -- CBL_center_medium -- CBS_class_medium  , out: 40 x 40 x 85\n",
        "      #medium_concat = tf.keras.layers.concatenate(inputs=[CBL_prob_medium,CBL_left_medium,CBL_center_medium,CBL_class_medium],axis=-1)\n",
        "      \n",
        "      #**************** output medium ****************\n",
        "      \n",
        "      #output_medium = self.conv2D_medium(medium_concat)\n",
        "      output_medium = tf.keras.layers.concatenate(inputs=[CBS_prob_medium,CBL_left_medium,CBL_center_medium,CBS_class_medium],axis=-1,name=\"output_medium\")\n",
        "\n",
        "      #**************** output medium ****************\n",
        "\n",
        "      #connect_branch_6_CBL\n",
        "      connect_branch_6_CBL = self.connect_branch_6_CBL(rCSP4,train_flag)\n",
        "\n",
        "      #concat branch 3 -- branch 6  , out: 19 x 19 x 1024\n",
        "      mid_concat_br36 = tf.keras.layers.concatenate(inputs=[rCSP1,connect_branch_6_CBL],axis=-1)\n",
        "      \n",
        "      #rCSP5\n",
        "      rCSP5 = self.rCSP5(mid_concat_br36,train_flag)\n",
        "\n",
        "      #decouple head -- large object \n",
        "\n",
        "      #reg -- large\n",
        "      TCBL_reg_large = self.TCBL_reg_large(rCSP5,train_flag)\n",
        "\n",
        "      CBL_left_large = self.CBL_left_large(TCBL_reg_large,train_flag)\n",
        "\n",
        "      CBL_center_large = self.CBL_center_large(TCBL_reg_large,train_flag)\n",
        "\n",
        "      #class + prob -- large\n",
        "      TCBL_clsp_large = self.TCBL_clsp_large(rCSP5,train_flag)\n",
        "\n",
        "      CBL_prob_large = self.CBL_prob_large(TCBL_clsp_large,train_flag)\n",
        "\n",
        "      CBL_class_large = self.CBL_class_large(TCBL_clsp_large,train_flag)\n",
        "\n",
        "      CBS_prob_large = self.CBS_prob_large(CBL_prob_large,train_flag)\n",
        "\n",
        "      CBS_class_large = self.CBS_class_large(CBL_class_large,train_flag)\n",
        "\n",
        "      #concat CBS_prob_large -- CBL_left_large -- CBL_center_large -- CBS_class_large  , out: 20 x 20 x 85\n",
        "      #large_concat = tf.keras.layers.concatenate(inputs=[CBL_prob_large,CBL_left_large,CBL_center_large,CBL_class_large],axis=-1)\n",
        "      \n",
        "      #**************** output large ****************\n",
        "      \n",
        "      #output_large = self.conv2D_large(large_concat)\n",
        "      output_large = tf.keras.layers.concatenate(inputs=[CBS_prob_large,CBL_left_large,CBL_center_large,CBS_class_large],axis=-1,name=\"output_large\")\n",
        "\n",
        "      #**************** output large ****************\n",
        "\n",
        "      return [output_large,output_medium,output_small]\n",
        "\n",
        "   def graph_model(self,dim):\n",
        "\n",
        "      x = tf.keras.layers.Input(shape=dim)\n",
        "      \n",
        "      return tf.keras.Model(inputs=x,outputs=self.call(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usga9mVHpXf4"
      },
      "source": [
        "initializer = tf.random_normal_initializer()\n",
        "k = tf.Variable(initial_value=initializer(shape=(10,640,640,3),dtype=\"float32\"))\n",
        "in_x = tf.keras.layers.Input(shape=tuple(k.get_shape().as_list()[1:]))\n",
        "\n",
        "y = alpha_model()\n",
        "model = y.graph_model(tuple(in_x.get_shape().as_list()[1:]))\n",
        "plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQOelddB1nz"
      },
      "source": [
        "class alpha_loss(tf.keras.losses.Loss):\n",
        "\n",
        "  def __init__(self,gamma = 2,**kwargs):\n",
        "\n",
        "    super(alpha_loss,self).__init__(**kwargs)\n",
        "\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def call(self,y_true,y_pred):\n",
        "\n",
        "    #get object mask\n",
        "    object_mask = K.cast(y_true[:,:,:,0:1],K.dtype(y_pred))\n",
        "\n",
        "    #get prob\n",
        "    prob_true = y_true[:,:,:,0:1]\n",
        "    prob_pred = y_pred[:,:,:,0:1]\n",
        "\n",
        "    prob_true = K.cast(prob_true,K.dtype(prob_pred))\n",
        "\n",
        "    #get class\n",
        "    class_true = y_true[:,:,:,5:]\n",
        "    class_pred = y_pred[:,:,:,5:]\n",
        "\n",
        "    class_true = K.cast(class_true,K.dtype(class_pred))\n",
        "\n",
        "    #****************** Focal loss ******************\n",
        "\n",
        "    #get batch size\n",
        "    m = K.cast(K.shape(y_pred)[0],K.dtype(y_pred))\n",
        "\n",
        "    #clip the prediction\n",
        "    #prob_pred = K.clip(prob_pred,min_value = 0.0, max_value = 1.0)\n",
        "    #class_pred = K.clip(class_pred,min_value = 0.0, max_value = 1.0)\n",
        "\n",
        "    #prob focal loss\n",
        "    loss_tensor =  - ( (1 - prob_pred[:,:,:,:])**self.gamma ) * prob_true[:,:,:,:] * tf.math.log( prob_pred[:,:,:,:] + 1e-18 ) - ( prob_pred[:,:,:,:] ** self.gamma ) * ( 1 - prob_true[:,:,:,:] ) * tf.math.log( 1 - prob_pred[:,:,:,:] + 1e-18 ) \n",
        "    prob_focal_loss = K.sum(loss_tensor)/ m\n",
        "\n",
        "    #class focal loss\n",
        "    loss_tensor =  - ( (1 - class_pred[:,:,:,:])**self.gamma ) * class_true[:,:,:,:] * tf.math.log( class_pred[:,:,:,:] + 1e-18 ) - ( class_pred[:,:,:,:] ** self.gamma) * ( 1 - class_true[:,:,:,:] ) * tf.math.log( 1 - class_pred[:,:,:,:] + 1e-18 )\n",
        "    class_focal_loss = K.sum(loss_tensor[:,:,:,:]*object_mask[:,:,:,:]) / m\n",
        "\n",
        "\n",
        "    #****************** Focal loss ******************\n",
        "\n",
        "    #get reg left -- (x,y)\n",
        "    reg_left_true = y_true[:,:,:,1:3] \n",
        "    reg_left_pred = y_pred[:,:,:,1:3]\n",
        "\n",
        "    reg_left_true = K.cast(reg_left_true,K.dtype(reg_left_pred))\n",
        "\n",
        "    #mask the reg (because we consider the box with object only )\n",
        "    #reg_left_pred = reg_left_pred[:,:,:,:] * object_mask[:,:,:,tf.newaxis]\n",
        "\n",
        "    #get reg center -- (x,y)\n",
        "    reg_center_true = y_true[:,:,:,3:5] \n",
        "    reg_center_pred = y_pred[:,:,:,3:5]\n",
        "\n",
        "    reg_center_true = K.cast(reg_center_true,K.dtype(reg_center_pred))\n",
        "\n",
        "    #mask the reg (because we consider the box with object only )\n",
        "    #reg_center_pred = reg_center_pred[:,:,:,:] * object_mask[:,:,:,tf.newaxis]\n",
        "\n",
        "    #calculate width x height of anchor box\n",
        "    reg_wh_true = (reg_center_true[:,:,:,:] - reg_left_true[:,:,:,:])*2\n",
        "    reg_wh_pred = (reg_center_pred[:,:,:,:] - reg_left_pred[:,:,:,:])*2 \n",
        "\n",
        "    #calculate reg right\n",
        "    reg_right_true = reg_left_true[:,:,:,:] + reg_wh_true[:,:,:,:]\n",
        "    reg_right_pred = reg_left_pred[:,:,:,:] + reg_wh_pred[:,:,:,:]\n",
        "\n",
        "    #****************** Focal IOU loss ******************\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "    #calculate IOU  \n",
        "\n",
        "    #calculate intersection left\n",
        "    reg_left_intersection = tf.math.maximum(reg_left_pred,reg_left_true)\n",
        "\n",
        "    #calculate intersection right\n",
        "    reg_right_intersection = tf.math.minimum(reg_right_pred,reg_right_true)\n",
        "\n",
        "    #calibrate\n",
        "    #reg_right_intersection = tf.where((reg_left_intersection>reg_right_intersection),reg_left_intersection,reg_right_intersection) #-- same meaning\n",
        "    reg_right_intersection = tf.math.maximum(reg_left_intersection,reg_right_intersection) #-- same meaning\n",
        "\n",
        "    #intersection width height\n",
        "    intersection_wh = reg_right_intersection[:,:,:,:] - reg_left_intersection[:,:,:,:]\n",
        "\n",
        "    #intersection area\n",
        "    intersection_area = intersection_wh[:,:,:,0:1] * intersection_wh[:,:,:,1:2]\n",
        "\n",
        "    #union area\n",
        "    true_area = reg_wh_true[:,:,:,0:1] * reg_wh_true[:,:,:,1:2]\n",
        "    pred_area = reg_wh_pred[:,:,:,0:1] * reg_wh_pred[:,:,:,1:2]\n",
        "\n",
        "    union_area = true_area[:,:,:,:] + pred_area[:,:,:,:] - intersection_area[:,:,:,:]\n",
        "\n",
        "    #calculate iou \n",
        "    iou_val = intersection_area[:,:,:,:] / ( union_area[:,:,:,:] +  1e-10 )\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "\n",
        "    #calculate reg loss\n",
        "    loss_tensor = - ( (1 - iou_val[:,:,:,:])**self.gamma ) * tf.math.log( iou_val[:,:,:,:] + 1e-18) * object_mask[:,:,:,:]\n",
        "    reg_loss = K.sum(loss_tensor) / m\n",
        "\n",
        "    #****************** Focal IOU loss ******************\n",
        " \n",
        "    #calculate loss\n",
        "    loss = prob_focal_loss + class_focal_loss + 5 * reg_loss\n",
        " \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyZsdFRpkz4E"
      },
      "source": [
        "def preprocess_class(input_path,save_path ,name=\"class_map.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   MS COCO 2017 Dataset\n",
        "\n",
        "   return dict\n",
        "   \"\"\"\n",
        "   dataset = pd.read_csv(input_path)\n",
        "\n",
        "   #get class map\n",
        "   class_array = dataset.iloc[:,3]\n",
        "   class_map = {}\n",
        "   idx = 0\n",
        "   \n",
        "   for i in range(class_array.shape[0]):\n",
        "\n",
        "      if not (class_array[i] in class_map.keys()):\n",
        "\n",
        "         class_map[class_array[i]] = idx\n",
        "         idx = idx + 1\n",
        "\n",
        "   #save class map \n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "      \n",
        "     file.write(json.dumps(class_map))\n",
        "\n",
        "   return class_map\n",
        "\n",
        "def preprocess_bbox_info(path,path_pos,path_hw,name_pos=\"bbox_pos.txt\",name_hw=\"bbox_hw.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   MS COCO 2017 Dataset\n",
        "\n",
        "   return numpy.ndarray,bbox_pos,bbox_hw\n",
        "   \"\"\"\n",
        "\n",
        "   dataset = pd.read_csv(path)\n",
        "\n",
        "   \"\"\"\n",
        "   #find center\n",
        "   dataset[\"center_x\"] = (dataset[\"xmax\"] + dataset[\"xmin\"])/2\n",
        "   dataset[\"center_y\"] = (dataset[\"ymax\"] + dataset[\"ymin\"])/2\n",
        "   \"\"\"\n",
        "\n",
        "   #get positional data\n",
        "   bbox_pos = dataset.iloc[:,4:].to_numpy()\n",
        "\n",
        "   #save bbox_pos\n",
        "   with open(f\"{path_pos}/{name_pos}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(bbox_pos.tolist()))\n",
        "\n",
        "   #construct bbox_hw (m,2) : h -> 0 , w -> 1\n",
        "   m = bbox_pos.shape[0]\n",
        "   bbox_hw = np.zeros((m,2))\n",
        "\n",
        "   for i in range(m):\n",
        "\n",
        "      bbox_hw[i,0] = bbox_pos[i,3] - bbox_pos[i,1]\n",
        "      bbox_hw[i,1] = bbox_pos[i,2] - bbox_pos[i,0]\n",
        "\n",
        "\n",
        "   #save bbox_hw \n",
        "   with open(f\"{path_hw}/{name_hw}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(bbox_hw.tolist()))\n",
        "\n",
        "\n",
        "   return bbox_pos,bbox_hw\n",
        "\n",
        "\n",
        "def preprocess_pre_define_anchor_box(bbox_hw,save_path,K=9,name=\"anchors.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   bbox_hw -- numpy.ndarray (m,2)\n",
        "   \"\"\"\n",
        "\n",
        "   anchors = Kmean(bbox_hw,K)\n",
        "\n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(anchors.tolist()))\n",
        "\n",
        "   \n",
        "   return anchors\n",
        "\n",
        "def preprocessing_label(input_path,save_path,name=\"gt_dataset.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   save dict as {obj1:[[class,xmin,ymin,xcenter,ycenter],[class,xmin,ymin,xcenter,ycenter],...],obj2:...} (for each key)\n",
        "   \"\"\"\n",
        "\n",
        "   #read csv file\n",
        "   dataset = pd.read_csv(input_path)\n",
        "   m = dataset.shape[0]\n",
        "\n",
        "   #calibrate bbox pos\n",
        "   dataset[\"xmin\"] = dataset[\"xmin\"] + (640 - dataset[\"width\"] )//2\n",
        "   dataset[\"xmax\"] = dataset[\"xmax\"] + (640 - dataset[\"width\"] )//2\n",
        "\n",
        "   dataset[\"ymin\"] = dataset[\"ymin\"] + (640 - dataset[\"height\"] )//2\n",
        "   dataset[\"ymax\"] = dataset[\"ymax\"] + (640 - dataset[\"height\"] )//2\n",
        "\n",
        "   #calculate center\n",
        "   dataset[\"xcenter\"] = (dataset[\"xmin\"] + dataset[\"xmax\"])/2\n",
        "   dataset[\"ycenter\"] = (dataset[\"ymin\"] + dataset[\"ymax\"])/2\n",
        "\n",
        "   #Group the label\n",
        "   gt_dataset = {}\n",
        "   \n",
        "   for i in range(m):\n",
        "\n",
        "      filename = dataset.iloc[i,0]\n",
        "      tmp = []\n",
        "\n",
        "      #class\n",
        "      tmp.append(dataset.iloc[i,3])\n",
        "\n",
        "      #xmin xmax ymin ymax\n",
        "      tmp.append(dataset.iloc[i,4].item())\n",
        "      tmp.append(dataset.iloc[i,5].item())\n",
        "      tmp.append(dataset.iloc[i,8].item())\n",
        "      tmp.append(dataset.iloc[i,9].item())\n",
        "\n",
        "      if not ( filename in gt_dataset.keys()  ):\n",
        "\n",
        "         gt_dataset[filename] = []\n",
        "         \n",
        "      gt_dataset[filename].append(tmp)\n",
        "\n",
        "\n",
        "   #save dataset\n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(gt_dataset))\n",
        "\n",
        "      file.close()\n",
        "\n",
        "   return gt_dataset\n",
        "   \n",
        "def preprocess_image(img,standard_shape=(640,640)):\n",
        "\n",
        "   \"\"\"\n",
        "   img -- numpy.ndarray\n",
        "   standard_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   #get h,w\n",
        "   height,width = standard_shape\n",
        "\n",
        "   #get pad size\n",
        "   padH = (height - img.shape[0])//2\n",
        "   padW = (width - img.shape[1])//2\n",
        "\n",
        "   #pad img\n",
        "   diff_H = height - img.shape[0]\n",
        "   diff_W = width - img.shape[1]\n",
        "   \n",
        "   if (diff_H % 2 ) == 0 and (diff_W % 2) == 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) != 0 and (diff_W % 2) == 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH+1),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) == 0 and (diff_W % 2) != 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH),(padW,padW+1),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) != 0 and (diff_W % 2) != 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH+1),(padW,padW+1),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   return img_pad\n",
        "   \n",
        "\n",
        "def preprocess_y_true(input_path,save_path,anchors,class_map,input_shape = (640,640),pos_info_format = [(76,76,255),(38,38,255),(19,19,255)],bbox_type=3):\n",
        "\n",
        "   \"\"\"\n",
        "   anchors -- numpy.ndarray (K,2)\n",
        "   input_shape -- (x,y)\n",
        "   \"\"\"\n",
        "\n",
        "   #read csv file\n",
        "   dataset = pd.read_csv(input_path)\n",
        "   m = dataset.shape[0]\n",
        "\n",
        "   #get center info\n",
        "   dataset[\"x_center\"] = (dataset[\"xmin\"] + dataset[\"xmax\"])/2\n",
        "   dataset[\"y_center\"] = (dataset[\"ymin\"] + dataset[\"ymax\"])/2\n",
        "\n",
        "   #calibrate bbox pos\n",
        "   dataset[\"xmin\"] = dataset[\"xmin\"] + (640 - dataset[\"width\"] )//2\n",
        "   dataset[\"xmax\"] = dataset[\"xmax\"] + (640 - dataset[\"width\"] )//2\n",
        "\n",
        "   dataset[\"ymin\"] = dataset[\"ymin\"] + (640 - dataset[\"height\"] )//2\n",
        "   dataset[\"ymax\"] = dataset[\"ymax\"] + (640 - dataset[\"height\"] )//2\n",
        "\n",
        "   #set up\n",
        "   prev_name = \"#\"\n",
        "   pos_format_size = len(pos_info_format)\n",
        "\n",
        "   #loop through dataset\n",
        "   for i in range(m):\n",
        "\n",
        "      curr_name = dataset.iloc[i,0]\n",
        "      class_name = dataset.iloc[i,3]\n",
        "\n",
        "      if curr_name == prev_name:\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "      \n",
        "      elif curr_name != prev_name and prev_name != \"#\":\n",
        "\n",
        "         #save prev pos info\n",
        "         for q in range(pos_format_size):\n",
        "            \n",
        "            h_size = pos_info_format[q][0]\n",
        "            w_size = pos_info_format[q][1]\n",
        "            \n",
        "            with open(f\"{save_path}/{curr_name}_{h_size}x{w_size}.txt\",\"w\") as file:\n",
        "\n",
        "               file.write(json.dumps((pos_info[q]).tolist()))\n",
        "\n",
        "               file.close()\n",
        "\n",
        "         #crreate new pos info\n",
        "         pos_info = [np.zeros(pos_info_format[0]),np.zeros(pos_info_format[1]),np.zeros(pos_info_format[2])]\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "\n",
        "      elif prev_name == \"#\":\n",
        "\n",
        "         #crreate new pos info\n",
        "         pos_info = [np.zeros(pos_info_format[0]),np.zeros(pos_info_format[1]),np.zeros(pos_info_format[2])]\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "\n",
        "      #update prev_name\n",
        "      prev_name = curr_name\n",
        "\n",
        "   #save last obj\n",
        "   for q in range(pos_format_size):\n",
        "   \n",
        "      h_size = pos_info_format[q][0]\n",
        "      w_size = pos_info_format[q][1]\n",
        "      \n",
        "      with open(f\"{save_path}/{curr_name}_{h_size}x{w_size}.txt\",\"w\") as file:\n",
        "\n",
        "         file.write(json.dumps((pos_info[q]).tolist()))\n",
        "\n",
        "         file.close()\n",
        "\n",
        "\n",
        "#@jit(nopython=True)\n",
        "def update_pos_info(pos_info,center_x,center_y,xmin,ymin,xmax,ymax,class_index,anchors,image_shape = (640,640),bbox_type = 3,feature_size = 85):\n",
        "\n",
        "   \"\"\"\n",
        "   center_info -- (x,y)\n",
        "   pos_info -- list containing numpy.ndarray\n",
        "   obj_pos -- numpy.ndarray (1,4)\n",
        "   anchors -- numpy.ndarray (K,2)\n",
        "   image_shape -- (x,y)\n",
        "   \"\"\"\n",
        "   \n",
        "   #object bbox h w\n",
        "   bbox_h = xmax - xmin\n",
        "   bbox_w = ymax - ymin\n",
        "   \n",
        "   #find bext anchor index\n",
        "   max_index = 0\n",
        "   max_iou = 0\n",
        "   \n",
        "   for i in range(anchors.shape[0]):\n",
        "\n",
        "      min_h = np.minimum(anchors[i,0],bbox_h).item()\n",
        "      min_w = np.minimum(anchors[i,1],bbox_w).item()\n",
        "\n",
        "      #intersection\n",
        "      intersection_area = min_w * min_h\n",
        "\n",
        "      #union\n",
        "      union_area = bbox_h * bbox_w  + anchors[i,0] * anchors[i,1] - intersection_area\n",
        "\n",
        "      #iou\n",
        "      cur_iou = intersection_area / union_area\n",
        "\n",
        "      if cur_iou > max_iou:\n",
        "\n",
        "         max_iou = cur_iou\n",
        "\n",
        "         max_index = i\n",
        "\n",
        "   #size of particular type\n",
        "   type_size = np.int64(anchors.shape[0]/bbox_type).item()\n",
        "   \n",
        "   #best box index -- dim 1 (determine which type of box)\n",
        "   best_anchor_index = np.int64(max_index/type_size).item()\n",
        "      \n",
        "   #best box index -- dim 2 (in a particular type of box , determine sub class of particular type )\n",
        "   sub_class_index = max_index % type_size\n",
        "\n",
        "   #update info (prob,xmin,ymin,xmax,ymax,class)\n",
        "   #feature mapping\n",
        "   feature_per_image_pixel_x = pos_info[best_anchor_index].shape[0] / image_shape[0]\n",
        "   feature_per_image_pixel_y = pos_info[best_anchor_index].shape[1] / image_shape[1]\n",
        "\n",
        "   target_x = np.int64(center_x * feature_per_image_pixel_x).item()\n",
        "   target_y = np.int64(center_y * feature_per_image_pixel_y).item()\n",
        "\n",
        "   #prob\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,0] = 1\n",
        "\n",
        "   #xmin,ymin,xmax,ymax\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,1] = xmin\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,2] = ymin\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,3] = xmax\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,4] = ymax\n",
        "\n",
        "   #class\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,(class_index+5)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_oXa58fUcG"
      },
      "source": [
        "#data augmentation\n",
        "def data_aug(img):\n",
        "\n",
        "   idx = np.random.randint(0,4)\n",
        "\n",
        "   if idx == 0:\n",
        "\n",
        "      h  = tune_odd(np.random.randint(1,17))\n",
        "      w  = tune_odd(np.random.randint(1,17))\n",
        "\n",
        "      kernel_shape = (h,w)\n",
        "\n",
        "      img = GaussianBlur(img,kernel_shape)\n",
        "\n",
        "   elif idx == 1:\n",
        "\n",
        "      img = GaussianNoise(img)\n",
        "\n",
        "   elif idx == 2:\n",
        "\n",
        "      color_aug_seq = [ i for i in range(4)]\n",
        "      random.shuffle(color_aug_seq)\n",
        "\n",
        "      for i in color_aug_seq:\n",
        "\n",
        "         if i == 0:\n",
        "\n",
        "            img = random_brightness(img)\n",
        "\n",
        "         elif i == 1:\n",
        "\n",
        "            img = random_saturation(img)\n",
        "\n",
        "         elif i == 2:\n",
        "\n",
        "            img = random_contrast(img)\n",
        "\n",
        "         elif i == 3:\n",
        "\n",
        "            img = random_hue(img)\n",
        "\n",
        "   elif idx == 3:\n",
        "\n",
        "      for i in range(16):\n",
        "\n",
        "         h = np.random.randint(8,81)\n",
        "         w = np.random.randint(8,81)\n",
        "\n",
        "         kernel_size = (h,w)\n",
        "\n",
        "         img = random_erase(img,kernel_size)\n",
        "\n",
        "      \n",
        "   return img\n",
        "\n",
        "@jit(nopython=True)\n",
        "def tune_odd(val):\n",
        "\n",
        "   if (val % 2) == 0:\n",
        "\n",
        "      val = val + 1\n",
        "\n",
        "   return val\n",
        "\n",
        "#Gaussian Blur\n",
        "def GaussianBlur(img,kernel_shape,sigma=0):\n",
        "\n",
        "   \"\"\"\n",
        "   img -- numpy array\n",
        "   kernel_shape -- (int,int)\n",
        "   sigma -- real number\n",
        "   \"\"\"\n",
        "\n",
        "   return cv2.GaussianBlur(img,kernel_shape,sigma)\n",
        "\n",
        "\n",
        "##Gaussian Noise\n",
        "@jit(nopython=True)\n",
        "def ext_operator(val):\n",
        "\n",
        "  if val > 255 :\n",
        "\n",
        "    val = 255\n",
        "\n",
        "  elif val < 0:\n",
        "\n",
        "    val = 0\n",
        "\n",
        "  return val\n",
        "\n",
        "@jit(nopython=True)\n",
        "def GaussianNoise(img,low=8,high=64):\n",
        "\n",
        "  nH,nW,nC = img.shape\n",
        "\n",
        "  for h in range(nH):\n",
        "\n",
        "    for w in range(nW):\n",
        "\n",
        "      for c in range(nC):\n",
        "\n",
        "        factor = np.random.randint(low,high)\n",
        "\n",
        "        loc = factor * np.random.random()\n",
        "\n",
        "        scale = factor * np.random.random()\n",
        "\n",
        "        noise = np.random.normal(loc,scale)\n",
        "\n",
        "        img[h,w,c] = ext_operator(img[h,w,c]+noise)\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "#brightness\n",
        "def random_brightness(img):\n",
        "\n",
        "   return tf.image.random_brightness(img,0.4).numpy()\n",
        "\n",
        "\n",
        "#saturation\n",
        "def random_saturation(img):\n",
        "\n",
        "   return tf.image.random_saturation(img,1.5,8.0).numpy()\n",
        "\n",
        "#contrast\n",
        "def random_contrast(img):\n",
        "\n",
        "   return tf.image.random_contrast(img,1.5,8.0).numpy()\n",
        "\n",
        "#hue\n",
        "def random_hue(img):\n",
        "\n",
        "   return tf.image.random_hue(img,0.4).numpy()\n",
        "\n",
        "#erase\n",
        "@jit(nopython=True)\n",
        "def random_erase(img,kernel_size=(64,64)):\n",
        "\n",
        "   nH,nW,_ = img.shape\n",
        "   fH,fW = kernel_size\n",
        "\n",
        "   new_nH = nH - fH + 1\n",
        "   new_nW = nW - fW + 1\n",
        "\n",
        "   nH_start = np.random.randint(0,new_nH)\n",
        "   nW_start = np.random.randint(0,new_nW)\n",
        "\n",
        "   nH_end = nH_start + fH\n",
        "   nW_end = nW_start + fW\n",
        "\n",
        "   erase_region = img[nH_start:nH_end,nW_start:nW_end,:].copy()\n",
        "\n",
        "   for h in range(nH_start,nH_end):\n",
        "\n",
        "      for w in range(nW_start,nW_end):\n",
        "         \n",
        "         random_h = np.random.randint(0,fH)\n",
        "         random_w = np.random.randint(0,fW)\n",
        "\n",
        "         img[h,w,:] = erase_region[random_h,random_w,:].copy()\n",
        "\n",
        "\n",
        "   return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSwsLaF0feA7"
      },
      "source": [
        "#generator\n",
        "def get_gt_data(batch_size,img_info,class_info,img_path,img_shape = (640,640),standard_scale=(19360,66930),aug_flag=False):\n",
        "\n",
        "   \"\"\"\n",
        "   #img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   \n",
        "   img_list_shuffled = list(img_info.keys())\n",
        "   \n",
        "   random.shuffle(img_list_shuffled)\n",
        "\n",
        "   m = len(img_list_shuffled)\n",
        "\n",
        "   idx = 0\n",
        "\n",
        "   while m >= batch_size:\n",
        "\n",
        "      \"\"\"\n",
        "      #check remaining sample\n",
        "      #if m < batch_size:\n",
        "\n",
        "         #break\n",
        "      \"\"\"\n",
        "      \n",
        "      #get name list\n",
        "      name_list = []\n",
        "\n",
        "      for i in range(idx*batch_size,(idx+1)*batch_size):\n",
        "\n",
        "         name_list.append(img_list_shuffled[i])\n",
        "\n",
        "\n",
        "      #get image data -- np.array\n",
        "      img_data = get_image_data(name_list,img_path,img_shape,aug_flag)\n",
        "\n",
        "      #get y_true data -- tuple (np.array,np.array,np.array)\n",
        "      label = get_y_true(name_list,img_info,class_info,img_shape,standard_scale)\n",
        "\n",
        "      #update remaining sample\n",
        "      m = m - batch_size\n",
        "      idx = idx + 1\n",
        "\n",
        "      yield img_data,label\n",
        "\n",
        "\"\"\"\n",
        "def get_gt_data(batch_size,img_list_shuffled,img_info,class_info,img_path,img_shape = (640,640),standard_scale=(8000,50000)):\n",
        "   \n",
        "   random.shuffle(img_list_shuffled)\n",
        "      \n",
        "   #get name list\n",
        "   name_list = []\n",
        "\n",
        "   for i in range(batch_size):\n",
        "\n",
        "      name_list.append(img_list_shuffled[i])\n",
        "\n",
        "      img_list_shuffled.remove(img_list_shuffled[i])\n",
        "\n",
        "\n",
        "   #get image data -- np.array\n",
        "   img_data = get_image_data(name_list,img_path,img_shape)\n",
        "\n",
        "   #get y_true data -- tuple (np.array,np.array,np.array)\n",
        "   label = get_y_true(name_list,img_info,class_info)\n",
        "\n",
        "   return img_data,label\n",
        "\"\"\"\n",
        "      \n",
        "def get_image_data(name_list,img_path,img_shape=(640,640),aug_flag=False):\n",
        "\n",
        "   \"\"\"\n",
        "   return numpy.ndarray\n",
        "   \"\"\"\n",
        "\n",
        "   img_data = []\n",
        "\n",
        "   for name in name_list:\n",
        "\n",
        "      #img -- numpy.ndarray\n",
        "      img = cv2.imread(f\"{img_path}/{name}\")\n",
        "\n",
        "      #calibrate image\n",
        "      img = preprocess_image(img,img_shape)\n",
        "\n",
        "      #img = cv2.resize(img,(128,128))\n",
        "      #data augmentation\n",
        "      if aug_flag:\n",
        "\n",
        "         img = data_aug(img)\n",
        "\n",
        "      #save img\n",
        "      img_data.append(img)\n",
        "\n",
        "   img_data = np.array(img_data)\n",
        "\n",
        "   return img_data\n",
        "\n",
        "\n",
        "def get_y_true(name_list,img_info,class_info,img_shape = (640,640),standard_scale=(19360,66930)):\n",
        "\n",
        "   \"\"\"\n",
        "   name_list -- list\n",
        "   img_info -- dict -- {obj1:[[class,xmin,ymin,xcenter,ycenter],[class,xmin,ymin,xcenter,ycenter],...],obj2:...} (for each key)\n",
        "   class_info -- dict\n",
        "   standard_scale -- dict (small , medium , large)\n",
        "   img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   #initialize y_true\n",
        "   small_true = []\n",
        "   medium_true = []\n",
        "   large_true = []\n",
        "   \n",
        "   \n",
        "   for name in name_list:\n",
        "\n",
        "\n",
        "      #initialize y_true extra dim will be removed when it is saved (it is used for overlap region checking)\n",
        "      obj_small_true = np.zeros((80,80,87))\n",
        "      obj_medium_true = np.zeros((40,40,87))\n",
        "      obj_large_true = np.zeros((20,20,87))\n",
        "\n",
        "      #obj_small_true = np.zeros((16,16,91))\n",
        "      #obj_medium_true = np.zeros((8,8,91))\n",
        "      #obj_large_true = np.zeros((4,4,91))\n",
        "\n",
        "      #get (obj_info -- list)\n",
        "      obj_info = img_info[name]\n",
        "\n",
        "      #loop via all object in the image (obj -- list)\n",
        "      for obj in obj_info:\n",
        "\n",
        "         #update y_true\n",
        "         obj_small_true,obj_medium_true,obj_large_true = update_y_true(obj,class_info[obj[0]],obj_small_true,obj_medium_true,obj_large_true,img_shape,standard_scale)\n",
        "         \n",
        "      #save image info\n",
        "      small_true.append(obj_small_true[:,:,:-2])\n",
        "      medium_true.append(obj_medium_true[:,:,:-2])\n",
        "      large_true.append(obj_large_true[:,:,:-2])\n",
        "\n",
        "   #convert y_true to numpy array\n",
        "   small_true = np.array(small_true)\n",
        "   medium_true = np.array(medium_true)\n",
        "   large_true = np.array(large_true)\n",
        "\n",
        "   return (large_true,medium_true,small_true)\n",
        "\n",
        "   \n",
        "def update_y_true(obj,class_id,obj_small_true,obj_medium_true,obj_large_true,img_shape = (640,640),standard_scale=(19360,66930)):\n",
        "\n",
        "   \"\"\"\n",
        "   obj -- list [class,xmin,ymin,xcenter,ycenter]\n",
        "   img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "\n",
        "   _,xmin,ymin,xcenter,ycenter = obj\n",
        "\n",
        "   #avoid x,y division by zeros for ratio calculation\n",
        "   xmin = xmin + 1e-18\n",
        "   ymin = ymin + 1e-18\n",
        "   xcenter = xcenter + 1e-18\n",
        "   ycenter = ycenter + 1e-18\n",
        "\n",
        "   width = (xcenter - xmin) * 2\n",
        "   height = (ycenter - ymin) * 2\n",
        "\n",
        "   xmax = xmin + width\n",
        "   ymax = ymin + height\n",
        "\n",
        "   area  = width * height\n",
        "\n",
        "   if (area < standard_scale[0]):\n",
        "\n",
        "      step_h = obj_small_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_small_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter)\n",
        "\n",
        "      if (obj_small_true[h_pos,w_pos,-2] == 0) :\n",
        "      \n",
        "         #prob\n",
        "         obj_small_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "         #xmin,ymin\n",
        "         obj_small_true[h_pos,w_pos,1] = xmin \n",
        "         obj_small_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "         #xcenter,ycenter\n",
        "         obj_small_true[h_pos,w_pos,3] = xcenter \n",
        "         obj_small_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "         #class\n",
        "         obj_small_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "         #label center\n",
        "         obj_small_true[h_pos,w_pos,-2] =  1 \n",
        "\n",
        "         #label occupied cells\n",
        "         obj_small_true[h_pos,w_pos,-1] =  1\n",
        "\n",
        "         #multiple positive\n",
        "         obj_small_true = multiple_positive_labeling(obj_small_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "         \n",
        "\n",
        "   elif (area > standard_scale[0]) and (area < standard_scale[1]):\n",
        "\n",
        "      step_h = obj_medium_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_medium_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter) \n",
        "\n",
        "      if (obj_medium_true[h_pos,w_pos,-2] == 0):\n",
        "         \n",
        "         #prob\n",
        "         obj_medium_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "         #xmin,ymin\n",
        "         obj_medium_true[h_pos,w_pos,1] = xmin \n",
        "         obj_medium_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "         #xcenter,ycenter\n",
        "         obj_medium_true[h_pos,w_pos,3] = xcenter \n",
        "         obj_medium_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "         #class\n",
        "         obj_medium_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "         #label center\n",
        "         obj_medium_true[h_pos,w_pos,-2] = 1 \n",
        "\n",
        "         #label occupied cells\n",
        "         obj_medium_true[h_pos,w_pos,-1] = 1\n",
        "\n",
        "         #multiple positive\n",
        "         obj_medium_true = multiple_positive_labeling(obj_medium_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "         \n",
        "\n",
        "   elif (area > standard_scale[1]) :\n",
        "\n",
        "      step_h = obj_large_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_large_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter)\n",
        "\n",
        "      if (obj_large_true[h_pos,w_pos,-2] == 0) :\n",
        "      \n",
        "         #prob\n",
        "         obj_large_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "         #xmin,ymin\n",
        "         obj_large_true[h_pos,w_pos,1] = xmin \n",
        "         obj_large_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "         #xcenter,ycenter\n",
        "         obj_large_true[h_pos,w_pos,3] = xcenter \n",
        "         obj_large_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "         #class\n",
        "         obj_large_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "         #label center\n",
        "         obj_large_true[h_pos,w_pos,-2] =  1\n",
        "\n",
        "         #label occupied cells\n",
        "         obj_large_true[h_pos,w_pos,-1] =  1\n",
        "\n",
        "         #multiple positive\n",
        "         obj_large_true = multiple_positive_labeling(obj_large_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "         \n",
        "\n",
        "   return obj_small_true,obj_medium_true,obj_large_true\n",
        "\n",
        "            \n",
        "@jit(nopython=True)  \n",
        "def multiple_positive_labeling(y_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h):\n",
        "\n",
        "   \"\"\"\n",
        "   y_true -- numpy array\n",
        "   \"\"\"\n",
        "\n",
        "   w_pos_init = int(xmin*step_w)\n",
        "   h_pos_init = int(ymin*step_h)\n",
        "\n",
        "   w_max = int(xmax*step_w)\n",
        "   h_max = int(ymax*step_h)\n",
        "\n",
        "   for w_pos in range(w_pos_init,w_max):\n",
        "\n",
        "      for h_pos in range(h_pos_init,h_max):\n",
        "\n",
        "         if (y_true[h_pos,w_pos,-2] == 0) and (y_true[h_pos,w_pos,-1] == 0):\n",
        "\n",
        "            #prob\n",
        "            y_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "            #xmin,ymin\n",
        "            y_true[h_pos,w_pos,1] = xmin + y_true[h_pos,w_pos,1] \n",
        "            y_true[h_pos,w_pos,2] = ymin + y_true[h_pos,w_pos,2]\n",
        "\n",
        "            #xcenter,ycenter\n",
        "            y_true[h_pos,w_pos,3] = xcenter + y_true[h_pos,w_pos,3]\n",
        "            y_true[h_pos,w_pos,4] = ycenter + y_true[h_pos,w_pos,4]\n",
        "\n",
        "            #class\n",
        "            y_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "            #label occupied cells\n",
        "            y_true[h_pos,w_pos,-1] =  1\n",
        "\n",
        "\n",
        "   return y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QILjQw-aXg9v"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiRB-rLim8u"
      },
      "source": [
        "#step function\n",
        "@tf.function\n",
        "def distributed_train_step(data_inputs):\n",
        "\n",
        "   per_replica_losses = strategy.run(train_step,args=(data_inputs,))\n",
        "\n",
        "   return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_losses,axis=None)\n",
        "\n",
        "\n",
        "def train_step(inputs):\n",
        "\n",
        "   images,labels = inputs\n",
        "\n",
        "   with tf.GradientTape() as tape:\n",
        "\n",
        "      predictions = model(images,train_flag=True)\n",
        "      \n",
        "      loss = compute_loss(labels,predictions)\n",
        "\n",
        "   gradients = tape.gradient(loss,model.trainable_weights)\n",
        "\n",
        "   optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n",
        "\n",
        "   return loss\n",
        "\n",
        "#find current path\n",
        "cur_path = os.getcwd()\n",
        "\n",
        "#define strategy\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "\n",
        "#define model,loss,optimizer\n",
        "with strategy.scope():\n",
        "\n",
        "   #define loss object\n",
        "   loss_object = alpha_loss(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "   #define compute loss\n",
        "   def compute_loss(labels,predictions):\n",
        "\n",
        "      #large\n",
        "      large_obj_loss = loss_object(labels[0],predictions[0])\n",
        "\n",
        "      #medium\n",
        "      medium_obj_loss = loss_object(labels[1],predictions[1])\n",
        "\n",
        "      #small\n",
        "      small_obj_loss = loss_object(labels[2],predictions[2])\n",
        "\n",
        "      #total loss\n",
        "      total_loss = large_obj_loss + medium_obj_loss + small_obj_loss\n",
        "\n",
        "      return total_loss \n",
        "\n",
        "   #define learning rate scheduler\n",
        "   lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001,decay_steps=5,alpha=1e-5)\n",
        "\n",
        "   #define optimizer\n",
        "   optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "   #define model\n",
        "   #model = alpha_model()\n",
        "   model = tf.keras.models.load_model(f\"{cur_path}/gdrive/MyDrive/model\")\n",
        "   \n",
        "\n",
        "batch_size_per_replica = 5\n",
        "\n",
        "EPOCHS = 300\n",
        "\n",
        "#preprocessing class\n",
        "input_path = f\"{cur_path}/gdrive/MyDrive/annotations/train_annotations.csv\"\n",
        "save_path = f\"{cur_path}/gdrive/MyDrive/data\"\n",
        "\n",
        "class_train_info = preprocess_class(input_path,save_path)\n",
        "\n",
        "#preprocessing label\n",
        "input_path = f\"{cur_path}/gdrive/MyDrive/annotations/test_annotations.csv\"\n",
        "save_path = f\"{cur_path}/gdrive/MyDrive/data\"\n",
        "\n",
        "img_train_info = preprocessing_label(input_path,save_path)\n",
        "\n",
        "#train image path\n",
        "img_train_path = f\"{cur_path}/gdrive/MyDrive/img\"\n",
        "\n",
        "img_shape = (640,640)\n",
        "standard_scale=(19360,66930)\n",
        "\n",
        "#get number of sample m\n",
        "m = len(list(img_train_info.keys()))\n",
        "\n",
        "#dataset size\n",
        "global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "buffer_size = global_batch_size * 2\n",
        "\n",
        "#total step per epochs\n",
        "total_step_per_epoch = int(m/buffer_size)\n",
        "\n",
        "#aug flag\n",
        "aug_flag = False\n",
        "\n",
        "#train\n",
        "for i  in range(EPOCHS):\n",
        "\n",
        "   total_loss = 0.0\n",
        "\n",
        "   if (i % 2) == 1:\n",
        "\n",
        "     aug_flag = True\n",
        "\n",
        "   else:\n",
        "\n",
        "     aug_flag = False\n",
        "\n",
        "   #get data \n",
        "   for train_images, train_labels in get_gt_data(buffer_size,img_train_info,class_train_info,img_train_path,img_shape,standard_scale,aug_flag):\n",
        "\n",
        "      #normalize the image to 0 to 1\n",
        "      train_images = train_images/ np.float64(255)\n",
        "\n",
        "      # Create Datasets from the batches\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size).batch(global_batch_size)\n",
        "\n",
        "      #create distributed dataset\n",
        "      train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "\n",
        "      #Do training\n",
        "      for batch in train_dist_dataset:\n",
        "\n",
        "         total_loss = total_loss + distributed_train_step(batch) \n",
        "   \n",
        "   total_loss = total_loss \n",
        "   \n",
        "   print(f\"Epoch {i+1} , Loss: {total_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWqOutS8rbr"
      },
      "source": [
        "#tf.keras.models.save_model(model,f\"{cur_path}/gdrive/MyDrive/model\")\n",
        "model.save(f\"{cur_path}/gdrive/MyDrive/model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}