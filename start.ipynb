{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "start.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMyKynIp/Tz13LyXz2aZ2ve",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amir-D-Shadow/Google-Colab/blob/main/start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRE50iEku0BT"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUe9YXzPHbT1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from numba import jit\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import cv2\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnvmYrenHnhU"
      },
      "source": [
        "def foo(a):\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if a > 8:\n",
        "\n",
        "      break\n",
        "\n",
        "    a = a + 1\n",
        "\n",
        "    yield a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12q0tZuBpIR1"
      },
      "source": [
        "for i in foo(3):\n",
        "\n",
        "  print((i is None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDYJFWHR2bl"
      },
      "source": [
        "class CBS(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding='valid',**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBS,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #convolution layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by sigmoid\n",
        "      output_sigmoid = tf.keras.activations.sigmoid(BN_x)\n",
        "\n",
        "      return output_sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63_vysbRo8s7"
      },
      "source": [
        "class CBL(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding='valid',**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBL,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #convolution layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by Leaky relu\n",
        "      output_leaky_relu = self.output_leaky_relu(BN_x)\n",
        "\n",
        "      return output_leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faQvhqe7plsl"
      },
      "source": [
        "#activation Mish\n",
        "def Mish(x):\n",
        "\n",
        "   softplus = tf.math.softplus(x)\n",
        "   tanh_s = tf.math.tanh(softplus)\n",
        "\n",
        "   return (x * tanh_s)\n",
        "\n",
        "\n",
        "#CBM Module\n",
        "class CBM(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(CBM,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_x = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_Mish = tf.keras.layers.Lambda(Mish)\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Convolution 2D layer\n",
        "      conv2D_x = self.conv2D_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_Mish = self.output_Mish(BN_x)\n",
        "\n",
        "      return output_Mish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhMdg-bNpY7y"
      },
      "source": [
        "#Backbone CSPX\n",
        "class CSPX(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,CSPX_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      CSPX_info -- dictionary containing information: num_of_res_unit , res_unit block info , CBM block info , CBL_info\n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "      ------ CBL_1 ------ CBL_2 ------ res_unit * X ------ CBL_3 -----\n",
        "                     |                                               |\n",
        "                     |                                               |______\n",
        "                     |                                                ______  Concat --- BN --- leaky relu --- CBM_1 \n",
        "                     |                                               |\n",
        "                     |                                               |\n",
        "                     -------------------------------- CBL_4 ----------\n",
        "      \"\"\"\n",
        "\n",
        "      #initialization\n",
        "      super(CSPX,self).__init__(**kwargs)\n",
        "\n",
        "      #extract num_of_res_unit\n",
        "      self.num_of_res_unit = CSPX_info[\"num_of_res_unit\"]\n",
        "\n",
        "      #define layers\n",
        "\n",
        "      #res_unit\n",
        "      self.res_unit_seq = {}\n",
        "\n",
        "      #Important: When defining the CSPX layer, remember to define res unit info (dictionary key) in the form of res_unit_i : i start from 1\n",
        "      for i in range(1,self.num_of_res_unit+1):\n",
        "\n",
        "         #Extract res_unit_i info\n",
        "         res_unit_info = CSPX_info[f\"res_unit_{i}\"]\n",
        "\n",
        "         #define resunit layer\n",
        "         self.res_unit_seq[f\"res_unit_{i}\"] = res_unit(res_unit_info)\n",
        "         \n",
        "\n",
        "      #CBL_1\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_1\"]\n",
        "      \n",
        "      self.CBL_1 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_2\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_2\"]\n",
        "      \n",
        "      self.CBL_2 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_3\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_3\"]\n",
        "      \n",
        "      self.CBL_3 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_4\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBL_4\"]\n",
        "      \n",
        "      self.CBL_4 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #BN\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      #leaky relu\n",
        "      self.leaky_relu_x = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      #CBM_1\n",
        "      filters,kernel_size,strides,padding = CSPX_info[\"CBM_1\"]\n",
        "\n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      x = inputs\n",
        "\n",
        "      #CBL_1\n",
        "      CBL_1 = self.CBL_1(x,train_flag)\n",
        "\n",
        "      #CBL_2\n",
        "      CBL_2 = self.CBL_2(CBL_1,train_flag)\n",
        "\n",
        "      #res_unit block\n",
        "      res_unit_block = CBL_2\n",
        "      \n",
        "      for i in range(1,self.num_of_res_unit+1):\n",
        "\n",
        "         res_unit_block =  (self.res_unit_seq[f\"res_unit_{i}\"])(res_unit_block,train_flag) \n",
        "\n",
        "      #CBL3\n",
        "      CBL_3 = self.CBL_3(res_unit_block,train_flag)\n",
        "      \n",
        "      #CBL_4\n",
        "      CBL_4 = self.CBL_4(CBL_1,train_flag)\n",
        "\n",
        "      #Concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[CBL_3,CBL_4],axis=-1)\n",
        "\n",
        "      #Batch Normalization\n",
        "      BN_x = self.BN_x(mid_concat,training=train_flag)\n",
        "\n",
        "      #leaky_relu_x\n",
        "      leaky_relu_x = self.leaky_relu_x(BN_x)\n",
        "\n",
        "      #output_CBM\n",
        "      output_CBM = self.CBM_1(leaky_relu_x,train_flag)\n",
        "\n",
        "      return output_CBM\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "#Neck CSPX\n",
        "class CSPX_Neck(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,NECK_info,**kwargs):\n",
        "      \n",
        "      \"\"\"\n",
        "      NECK_info -- dictionary containing information: num_of_CBL, CBM block info , CBL block info , conv2D info\n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "      ----------- CBL * X ------ conv2D_1 ----------------\n",
        "         |                                               |\n",
        "         |                                               |______\n",
        "         |                                                ______  Concat --- BN --- leaky relu --- CBM_1 \n",
        "         |                                               |\n",
        "         |                                               |\n",
        "         -------------------------------- conv2D_2 -------\n",
        "         \n",
        "      \"\"\"\n",
        "      \n",
        "      #initialization\n",
        "      super(CSPX_Neck,self).__init__(**kwargs)\n",
        "\n",
        "      #Get num_of_CBL\n",
        "      self.num_of_CBL = NECK_info[\"num_of_CBL\"]\n",
        "\n",
        "      #define layers\n",
        "\n",
        "      #CBL_X\n",
        "      self.CBL_seq = {}\n",
        "\n",
        "      for i in range(1,self.num_of_CBL+1):\n",
        "\n",
        "         filters,kernel_size,strides,padding = NECK_info[f\"CBL_{i}\"]\n",
        "\n",
        "         self.CBL_seq[f\"CBL_{i}\"] = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "\n",
        "      #Conv2D_1\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"conv2D_1\"]\n",
        "      \n",
        "      self.conv2D_1 = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      #conv2D_2\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"conv2D_2\"]\n",
        "\n",
        "      self.conv2D_2 = tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      #Batch Normalization\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "      \n",
        "      #leaky relu\n",
        "      self.leaky_relu_x = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      #CBM_1\n",
        "      filters,kernel_size,strides,padding = NECK_info[\"CBM_1\"]\n",
        "\n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #CBL_X\n",
        "      CBL_block = inputs\n",
        "      \n",
        "      for i in range(1,self.num_of_CBL+1):\n",
        "\n",
        "         CBL_block = (self.CBL_seq[f\"CBL_{i}\"])(CBL_block,train_flag)\n",
        "\n",
        "\n",
        "      #conv2D_1\n",
        "      conv2D_1 = self.conv2D_1(CBL_block)\n",
        "\n",
        "      #conv2D_2\n",
        "      conv2D_2 = self.conv2D_2(inputs)\n",
        "\n",
        "      #concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[conv2D_1,conv2D_2],axis=-1)\n",
        "\n",
        "      #BN_x\n",
        "      BN_x = self.BN_x(mid_concat,training=train_flag)\n",
        "\n",
        "      #leaky relu\n",
        "      leaky_relu_x = self.leaky_relu_x(BN_x)\n",
        "\n",
        "      #CBM_1\n",
        "      output_CBM = self.CBM_1(leaky_relu_x,train_flag)\n",
        "\n",
        "      return output_CBM\n",
        "\n",
        "\n",
        "#revised CSP\n",
        "class rCSP(tf.keras.Model):\n",
        "\n",
        "\n",
        "   def __init__(self,rCSP_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      rCSP_info -- dictionary containing information:  CBL info \n",
        "\n",
        "                     - hpara: (filters,kernel_size,strides,padding)\n",
        "\n",
        "                     \n",
        "      Module Graph:\n",
        "      \n",
        "                   ------- CBL_2 --- CBL_3 --- CBL_4 --- SPP_1 --- CBL_5 -----\n",
        "                   |                                                         |\n",
        "                   |                                                         |______\n",
        "         CBL_1  ---|                                                          ______  Concat --- CBL_7\n",
        "                   |                                                         |\n",
        "                   |                                                         |\n",
        "                   --------------------------- CBL_6 -------------------------\n",
        "         \n",
        "      \"\"\"\n",
        "      #initialization\n",
        "      super(rCSP,self).__init__(**kwargs)\n",
        "\n",
        "      #CBL_1\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_1\"]\n",
        "      \n",
        "      self.CBL_1 = CBL(filters,kernel_size,strides,padding)\n",
        "      \n",
        "      #CBL_2\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_2\"]\n",
        "\n",
        "      self.CBL_2 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_3\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_3\"]\n",
        "\n",
        "      self.CBL_3 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_4\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_4\"]\n",
        "\n",
        "      self.CBL_4 = CBL(filters,kernel_size,strides,padding)     \n",
        "\n",
        "      #SPP\n",
        "      self.SPP_1 = SPP()\n",
        "\n",
        "      #CBL_5\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_5\"]\n",
        "\n",
        "      self.CBL_5 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_6\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_6\"]\n",
        "\n",
        "      self.CBL_6 = CBL(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #CBL_7\n",
        "      filters,kernel_size,strides,padding = rCSP_info[\"CBL_7\"]\n",
        "\n",
        "      self.CBL_7 = CBL(filters,kernel_size,strides,padding)\n",
        "      \n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #CBL_1\n",
        "      CBL_1 = self.CBL_1(inputs,train_flag)\n",
        "\n",
        "      #CBL_2\n",
        "      CBL_2 = self.CBL_2(CBL_1,train_flag)\n",
        "\n",
        "      #CBL_3\n",
        "      CBL_3 = self.CBL_3(CBL_2,train_flag)\n",
        "\n",
        "      #CBL_4\n",
        "      CBL_4 = self.CBL_4(CBL_3,train_flag)\n",
        "\n",
        "      #SPP_1\n",
        "      SPP_1 = self.SPP_1(CBL_4)\n",
        "\n",
        "      #CBL_5\n",
        "      CBL_5 = self.CBL_5(SPP_1,train_flag)\n",
        "\n",
        "      #CBL_6\n",
        "      CBL_6 = self.CBL_6(CBL_1,train_flag)\n",
        "\n",
        "      #concat\n",
        "      mid_concat = tf.keras.layers.concatenate(inputs=[CBL_6,CBL_5],axis=-1)\n",
        "\n",
        "      #CBL_7\n",
        "      output_CBL_7 = self.CBL_7(mid_concat,train_flag)\n",
        "\n",
        "      \n",
        "      return output_CBL_7\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAG6mme5pgtf"
      },
      "source": [
        "class res_unit(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,block_info,**kwargs):\n",
        "\n",
        "      \"\"\"\n",
        "      block_info -- dictionary containing blocks' hyperparameters (filters,kernel_size,strides,padding)\n",
        "\n",
        "      Module Graph:\n",
        "\n",
        "      ------ CBM_1 ------ CBM_2 ------ Add\n",
        "         |                              |\n",
        "         |                              |\n",
        "         |                              |\n",
        "         --------------------------------\n",
        "\n",
        "      \n",
        "      \"\"\"\n",
        "\n",
        "      #initialization\n",
        "      super(res_unit,self).__init__(**kwargs)\n",
        "      \n",
        "      #1st CBM block\n",
        "      filters,kernel_size,strides,padding = block_info[\"CBM_1\"]\n",
        "      \n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #2nd CBM block\n",
        "      filters,kernel_size,strides,padding = block_info[\"CBM_2\"]\n",
        "\n",
        "      self.CBM_2 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #Add Layer\n",
        "      self.Add_layer = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      x = inputs\n",
        "\n",
        "      #1st CBM block\n",
        "      CBM_1 = self.CBM_1(inputs,train_flag)\n",
        "\n",
        "      #2nd CBM block\n",
        "      CBM_2 = self.CBM_2(CBM_1,train_flag)\n",
        "\n",
        "      #Add Layer\n",
        "      output_shortcut = self.Add_layer([CBM_2,x])\n",
        "\n",
        "      return output_shortcut\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33H-p75ep1Kf"
      },
      "source": [
        "class SPP(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(SPP,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.maxpool_5x5 = tf.keras.layers.MaxPooling2D(pool_size=5,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "      self.maxpool_9x9 = tf.keras.layers.MaxPooling2D(pool_size=9,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "      self.maxpool_13x13 = tf.keras.layers.MaxPooling2D(pool_size=13,strides=1,padding=\"same\",data_format=\"channels_last\")\n",
        "\n",
        "\n",
        "   def call(self,inputs):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "      \n",
        "      #5x5\n",
        "      maxpool_5x5 = self.maxpool_5x5(inputs)\n",
        "\n",
        "      #9x9\n",
        "      maxpool_9x9 = self.maxpool_9x9(inputs)\n",
        "\n",
        "      #13x13\n",
        "      maxpool_13x13 = self.maxpool_13x13(inputs)\n",
        "\n",
        "      #concatenate\n",
        "      output_concat = tf.keras.layers.concatenate(inputs=[maxpool_5x5,maxpool_9x9,maxpool_13x13,inputs],axis=-1)\n",
        "\n",
        "      return output_concat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3bD4py_qElm"
      },
      "source": [
        "#TCBL Module\n",
        "class TCBL(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=1,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(TCBL,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_transpose_x = tf.keras.layers.Conv2DTranspose(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Transpose Convolution 2D layer\n",
        "      conv2D_transpose_x = self.conv2D_transpose_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_transpose_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_leaky_relu = self.output_leaky_relu(BN_x)\n",
        "\n",
        "      return output_leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woFJY6kxqHOB"
      },
      "source": [
        "#TCBM Module\n",
        "class TCBM(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,filters=32,kernel_size=3,strides=2,padding=\"valid\",**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(TCBM,self).__init__(**kwargs)\n",
        "\n",
        "      #define layers\n",
        "      self.conv2D_transpose_x = tf.keras.layers.Conv2DTranspose(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=\"channels_last\")\n",
        "\n",
        "      self.BN_x = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "      self.output_Mish = tf.keras.layers.Lambda(Mish)\n",
        "\n",
        "      \n",
        "\n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      \"\"\"\n",
        "      input -- tensorflow layer with shape (m,n_H,n_W,n_C)\n",
        "      \"\"\"\n",
        "\n",
        "      #Transpose Convolution 2D layer\n",
        "      conv2D_transpose_x = self.conv2D_transpose_x(inputs)\n",
        "\n",
        "      #Batch Normalization layer\n",
        "      BN_x = self.BN_x(conv2D_transpose_x,training=train_flag)\n",
        "\n",
        "      #activate by Mish\n",
        "      output_Mish = self.output_Mish(BN_x)\n",
        "\n",
        "      return output_Mish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VHYCVfhvWEG"
      },
      "source": [
        "class alpha_model(tf.keras.Model):\n",
        "\n",
        "   def __init__(self,**kwargs):\n",
        "\n",
        "      #initialization\n",
        "      super(alpha_model,self).__init__(**kwargs)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #CBM_1 in : 640 x 640 x 3 out: 640 x 640 x 32\n",
        "      filters=32\n",
        "      kernel_size=3\n",
        "      strides=1\n",
        "      padding=\"same\"\n",
        "\n",
        "      self.CBM_1 = CBM(filters,kernel_size,strides,padding)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CSP_1 in : 640 x 640 x 32 out: 319 x 319 x 64\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 1\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (32,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (64,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #define CSP1\n",
        "      self.CSP1 = CSPX(CSPX_info)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CSP2 in : 319 x 319 x 64 out : 159 x 159 x 128\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 2\n",
        "\n",
        "      #res_unit_info\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (64,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (64,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (128,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (64,1,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #define CSP2\n",
        "      self.CSP2 = CSPX(CSPX_info)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CSP8_1 in : 159 x 159 x 128  out : 79 x 79 x 256 -- branch 1\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 8\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "      res_unit_5 = {}\n",
        "      res_unit_5[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_5[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_5\"] = res_unit_5\n",
        "\n",
        "      res_unit_6 = {}\n",
        "      res_unit_6[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_6[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_6\"] = res_unit_6\n",
        "\n",
        "      res_unit_7 = {}\n",
        "      res_unit_7[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_7[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_7\"] = res_unit_7\n",
        "\n",
        "      res_unit_8 = {}\n",
        "      res_unit_8[\"CBM_1\"] = (128,1,1,\"same\")\n",
        "      res_unit_8[\"CBM_2\"] = (128,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_8\"] = res_unit_8\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (256,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (128,1,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #define CSP8_1 -- branch_1\n",
        "      self.CSP8_branch_1 = CSPX(CSPX_info)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CSP8_2 in : 79 x 79 x 256 out : 39 x 39 x 512 --- branch_2\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 8\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "      res_unit_5 = {}\n",
        "      res_unit_5[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_5[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_5\"] = res_unit_5\n",
        "\n",
        "      res_unit_6 = {}\n",
        "      res_unit_6[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_6[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_6\"] = res_unit_6\n",
        "\n",
        "      res_unit_7 = {}\n",
        "      res_unit_7[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_7[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_7\"] = res_unit_7\n",
        "\n",
        "      res_unit_8 = {}\n",
        "      res_unit_8[\"CBM_1\"] = (256,1,1,\"same\")\n",
        "      res_unit_8[\"CBM_2\"] = (256,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_8\"] = res_unit_8\n",
        "\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (512,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (256,1,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #define CSP8_2 --- branch_2\n",
        "      self.CSP8_branch_2 = CSPX(CSPX_info)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CSP4 in : 39 x 39 x 512 out : 19 x 19 x 1024\n",
        "      CSPX_info = {}\n",
        "\n",
        "      #num_of_res_unit\n",
        "      CSPX_info[\"num_of_res_unit\"] = 4\n",
        "\n",
        "      #res_unit_info\n",
        "\n",
        "      res_unit_1 = {}\n",
        "      res_unit_1[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_1[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_1\"] = res_unit_1\n",
        "\n",
        "      res_unit_2 = {}\n",
        "      res_unit_2[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_2[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_2\"] = res_unit_2\n",
        "\n",
        "      res_unit_3 = {}\n",
        "      res_unit_3[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_3[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_3\"] = res_unit_3\n",
        "\n",
        "      res_unit_4 = {}\n",
        "      res_unit_4[\"CBM_1\"] = (512,1,1,\"same\")\n",
        "      res_unit_4[\"CBM_2\"] = (512,3,1,\"same\")\n",
        "      CSPX_info[\"res_unit_4\"] = res_unit_4\n",
        "\n",
        "\n",
        "      #CBL_1\n",
        "      CSPX_info[\"CBL_1\"] = (1024,3,2,\"valid\")\n",
        "\n",
        "      #CBL_2\n",
        "      CSPX_info[\"CBL_2\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #CBL_3\n",
        "      CSPX_info[\"CBL_3\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #CBL_4\n",
        "      CSPX_info[\"CBL_4\"] = (512,1,1,\"same\")\n",
        "\n",
        "      #CBM_1\n",
        "      CSPX_info[\"CBM_1\"] = (1024,1,1,\"same\")\n",
        "\n",
        "      #define CSP4\n",
        "      self.CSP4 = CSPX(CSPX_info)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_1 neck in : 19 x 19 x 1024 out : 19 x 19 x 512\n",
        "      self.CBL_1_neck = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #CBL_2 neck in : 19 x 19 x 512 out : 19 x 19 x 1024\n",
        "      self.CBL_2_neck = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_3 neck in : 19 x 19 x 1024 out : 19 x 19 x 512\n",
        "      self.CBL_3_neck = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #SPP_neck in:19 x 19 x 512 out: 19 x 19 x 2048\n",
        "      self.SPP_neck = SPP()\n",
        "\n",
        "      #CBL_4 neck in : 19 x 19 x 2048 out : 19 x 19 x 512\n",
        "      self.CBL_4_neck = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #CBL_5 neck in : 19 x 19 x 512 out : 19 x 19 x 1024\n",
        "      self.CBL_5_neck = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_6 branch 3 in : 19 x 19 x 1024 out : 19 x 19 x 512 -- branch 3\n",
        "      self.CBL_6_branch_3 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #CBL 7 neck in : 19 x 19 x 512 out : 19 x 19 x 256\n",
        "      self.CBL_7_neck = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #TCBL1 in : 19 x 19 x 256 out: 39 x 39 x 256\n",
        "      self.TCBL1 = TCBL(256,21,1,\"valid\")\n",
        "\n",
        "      #CBL_connect_branch_2 in : 39 x 39 x 512 out: 39 x 39 x 256\n",
        "      self.CBL_connect_branch_2 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #concat TCBL1 -- CBL_connect_branch_2 , out: 39 x 39 x 512\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_1 phase1 in : 39 x 39 x 512 out : 39 x 39 x 256\n",
        "      self.CBL_1_phase1 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #CBL_2 phase1 in : 39 x 39 x 256 out : 39 x 39 x 512\n",
        "      self.CBL_2_phase1 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_3 phase1 in : 39 x 39 x 512 out : 39 x 39 x 256\n",
        "      self.CBL_3_phase1 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #CBL_4 phase1 in : 39 x 39 x 256 out : 39 x 39 x 512\n",
        "      self.CBL_4_phase1 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_5 phase1_branch_4 in : 39 x 39 x 512 out : 39 x 39 x 256 -- branch 4\n",
        "      self.CBL_5_phase1_branch_4 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #CBL_6 phase1 in : 39 x 39 x 256 out : 39 x 39 x 128\n",
        "      self.CBL_6_phase1 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #TCBL2 in : 39 x 39 x 128 out: 79 x 79 x 128\n",
        "      self.TCBL2 = TCBL(128,41,1,\"valid\")\n",
        "\n",
        "      #CBL_connect_branch_1 in : 79 x 79 x 256 out: 79 x 79 x 128\n",
        "      self.CBL_connect_branch_1 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #concat TCBL2 -- CBL_connect_branch_1 , out: 79 x 79 x 256\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_1 phase2 in : 79 x 79 x 256 out : 79 x 79 x 128\n",
        "      self.CBL_1_phase2 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #CBL_2 phase2 in : 79 x 79 x 128 out : 79 x 79 x 256\n",
        "      self.CBL_2_phase2 = CBL(256,3,1,\"same\")\n",
        "\n",
        "      #CBL_3 phase2 in : 79 x 79 x 256 out : 79 x 79 x 128\n",
        "      self.CBL_3_phase2 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #CBL_4 phase2 in : 79 x 79 x 128 out : 79 x 79 x 256\n",
        "      self.CBL_4_phase2 = CBL(256,3,1,\"same\")\n",
        "\n",
        "      #CBL_5 phase2_branch_5 in : 79 x 79 x 256 out : 79 x 79 x 128 -- branch 5\n",
        "      self.CBL_5_phase2_branch_5 = CBL(128,1,1,\"same\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# small output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #TCBL_small in : 79 x 79 x 128 out : 80 x 80 x 128\n",
        "      self.TCBL_small = TCBL(128,2,1,\"valid\")\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_small_1 in : 80 x 80 x 128 out : 80 x 80 x 256\n",
        "      self.CBL_prob_class_small_1 = CBL(256,3,1,\"same\")\n",
        "\n",
        "      #CBL_prob_class_small_2 in : 80 x 80 x 256 out : 80 x 80 x 256\n",
        "      self.CBL_prob_class_small_2 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #conv_prob_small in : 80 x 80 x 256 out : 80 x 80 x 1\n",
        "      self.conv_prob_small = tf.keras.layers.Conv2D(filters=1,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "\n",
        "      #conv_class_small in : 80 x 80 x 256 out : 80 x 80 x 80\n",
        "      self.conv_class_small = tf.keras.layers.Conv2D(filters=80,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "      \n",
        "      #Reg info\n",
        "      \n",
        "      #CBL_left_center_small_1 in : 80 x 80 x 128 out : 80 x 80 x 256\n",
        "      self.CBL_left_center_small_1 = CBL(256,3,1,\"same\")\n",
        "\n",
        "      #CBL_left_center_small_2 in : 80 x 80 x 256 out : 80 x 80 x 256\n",
        "      self.CBL_left_center_small_2 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #conv_pos_info_small in : 80 x 80 x 256 out : 80 x 80 x 4\n",
        "      self.conv_pos_info_small = tf.keras.layers.Conv2D(filters=4,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=tf.keras.layers.LeakyReLU())\n",
        "\n",
        "      #concat conv_prob_small -- conv_pos_info_small -- conv_class_small , out: 80 x 80 x 85\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# small output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_connect_branch_5 in : 79 x 79 x 128 out : 39 x 39 x 256\n",
        "      self.CBL_connect_branch_5 = CBL(256,3,2,\"valid\")\n",
        "\n",
        "      #concat CBL_connect_branch_5 -- CBL_5_phase1_branch_4 , out: 39 x 39 x 512\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_1 phase3 in : 39 x 39 x 512 out : 39 x 39 x 256\n",
        "      self.CBL_1_phase3 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #CBL_2 phase3 in : 39 x 39 x 256 out : 39 x 39 x 512\n",
        "      self.CBL_2_phase3 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_3 phase3 in : 39 x 39 x 512 out : 39 x 39 x 256\n",
        "      self.CBL_3_phase3 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #CBL_4 phase3 in : 39 x 39 x 256 out : 39 x 39 x 512\n",
        "      self.CBL_4_phase3 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_5 phase3_branch_6 in : 39 x 39 x 512 out : 39 x 39 x 256 -- branch 6\n",
        "      self.CBL_5_phase3_branch_6 = CBL(256,1,1,\"same\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# medium output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #TCBL_medium in : 39 x 39 x 256 out : 40 x 40 x 256\n",
        "      self.TCBL_medium = TCBL(128,2,1,\"valid\")\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_medium_1 in : 40 x 40 x 256 out : 40 x 40 x 512\n",
        "      self.CBL_prob_class_medium_1 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_prob_class_medium_2 in : 40 x 40 x 512 out : 40 x 40 x 512\n",
        "      self.CBL_prob_class_medium_2 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #conv_prob_medium in : 40 x 40 x 512 out : 40 x 40 x 1\n",
        "      self.conv_prob_medium = tf.keras.layers.Conv2D(filters=1,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "\n",
        "      #conv_class_medium in : 40 x 40 x 512 out : 40 x 40 x 80\n",
        "      self.conv_class_medium = tf.keras.layers.Conv2D(filters=80,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "\n",
        "\n",
        "      #Reg info\n",
        "      \n",
        "      #CBL_left_center_medium_1 in : 40 x 40 x 256 out : 40 x 40 x 512\n",
        "      self.CBL_left_center_medium_1 = CBL(512,3,1,\"same\")\n",
        "\n",
        "      #CBL_left_center_medium_2 in :  40 x 40 x 512 out :  40 x 40 x 512\n",
        "      self.CBL_left_center_medium_2 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #conv_pos_info_medium in : 40 x 40 x 512 out : 40 x 40 x 4\n",
        "      self.conv_pos_info_medium = tf.keras.layers.Conv2D(filters=4,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=tf.keras.layers.LeakyReLU())\n",
        "\n",
        "      #concat conv_prob_medium -- conv_pos_info_medium -- conv_class_medium , out: 40 x 40 x 85\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# medium output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_connect_branch_6 in : 39 x 39 x 256 out : 19 x 19 x 512\n",
        "      self.CBL_connect_branch_6 = CBL(512,3,2,\"valid\")\n",
        "\n",
        "      #concat CBL_connect_branch_6 -- CBL_6_branch_3 , out: 19 x 19 x 1024\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_1 phase4 in : 19 x 19 x 1024 out : 19 x 19 x 512\n",
        "      self.CBL_1_phase4 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #CBL_2 phase4 in : 19 x 19 x 512 out : 19 x 19 x 1024\n",
        "      self.CBL_2_phase4 = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_3 phase4 in : 19 x 19 x 1024 out : 19 x 19 x 512\n",
        "      self.CBL_3_phase4 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      #CBL_4 phase4 in : 19 x 19 x 512 out : 19 x 19 x 1024\n",
        "      self.CBL_4_phase4 = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_5 phase4 in : 19 x 19 x 1024 out : 19 x 19 x 512 \n",
        "      self.CBL_5_phase4 = CBL(512,1,1,\"same\")\n",
        "\n",
        "      \n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# large output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #TCBL_large in : 19 x 19 x 512 out : 20 x 20 x 512\n",
        "      self.TCBL_large = TCBL(512,2,1,\"valid\")\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_large_1 in : 20 x 20 x 512 out : 20 x 20 x 1024\n",
        "      self.CBL_prob_class_large_1 = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_prob_class_large_2 in : 20 x 20 x 1024 out : 20 x 20 x 1024\n",
        "      self.CBL_prob_class_large_2 = CBL(1024,1,1,\"same\")\n",
        "\n",
        "      #conv_prob_large in : 20 x 20 x 512 out : 20 x 20 x 1\n",
        "      self.conv_prob_large = tf.keras.layers.Conv2D(filters=1,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "\n",
        "      #conv_class_large in : 20 x 20 x 512 out : 20 x 20 x 80\n",
        "      self.conv_class_large = tf.keras.layers.Conv2D(filters=80,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=\"sigmoid\")\n",
        "\n",
        "\n",
        "      #Reg info\n",
        "      \n",
        "      #CBL_left_center_large_1 in : 20 x 20 x 512 out : 20 x 20 x 1024\n",
        "      self.CBL_left_center_large_1 = CBL(1024,3,1,\"same\")\n",
        "\n",
        "      #CBL_left_center_large_2 in :  20 x 20 x 1024 out :  20 x 20 x 1024\n",
        "      self.CBL_left_center_large_2 = CBL(1024,1,1,\"same\")\n",
        "\n",
        "      #conv_pos_info_large in : 20 x 20 x 1024 out : 20 x 20 x 4\n",
        "      self.conv_pos_info_large = tf.keras.layers.Conv2D(filters=4,kernel_size=1,strides=1,padding=\"same\",data_format=\"channels_last\",activation=tf.keras.layers.LeakyReLU())\n",
        "\n",
        "      #concat conv_prob_large -- conv_pos_info_large -- conv_class_large , out: 20 x 20 x 85\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# large output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      \n",
        "   def call(self,inputs,train_flag=True):\n",
        "\n",
        "      #CBM1\n",
        "      CBM_1 = self.CBM_1(inputs,train_flag)\n",
        "\n",
        "      #CSP1\n",
        "      CSP1 = self.CSP1(CBM_1,train_flag)\n",
        "\n",
        "      #CSP2\n",
        "      CSP2 = self.CSP2(CSP1,train_flag)\n",
        "\n",
        "      #CSP8_branch_1 ----------------------------------------- branch 1\n",
        "      CSP8_branch_1 = self.CSP8_branch_1(CSP2,train_flag)\n",
        "\n",
        "      #CSP8_branch_2 ----------------------------------------- branch 2\n",
        "      CSP8_branch_2 = self.CSP8_branch_2(CSP8_branch_1,train_flag)\n",
        "\n",
        "      #CSP4\n",
        "      CSP4 = self.CSP4(CSP8_branch_2,train_flag)\n",
        "\n",
        "      #CBL_1_neck\n",
        "      CBL_1_neck = self.CBL_1_neck(CSP4,train_flag)\n",
        "\n",
        "      #CBL_2_neck\n",
        "      CBL_2_neck = self.CBL_2_neck(CBL_1_neck,train_flag)\n",
        "\n",
        "      #CBL_3_neck\n",
        "      CBL_3_neck = self.CBL_3_neck(CBL_2_neck,train_flag)\n",
        "\n",
        "      #SPP_neck\n",
        "      SPP_neck = self.SPP_neck(CBL_3_neck)\n",
        "\n",
        "      #CBL_4_neck\n",
        "      CBL_4_neck = self.CBL_4_neck(SPP_neck,train_flag)\n",
        "\n",
        "      #CBL_5_neck\n",
        "      CBL_5_neck = self.CBL_5_neck(CBL_4_neck,train_flag)\n",
        "\n",
        "      #CBL_6_branch_3 ----------------------------------------- branch 3\n",
        "      CBL_6_branch_3 = self.CBL_6_branch_3(CBL_5_neck,train_flag)\n",
        "\n",
        "      #CBL_7_neck\n",
        "      CBL_7_neck = self.CBL_7_neck(CBL_6_branch_3,train_flag)\n",
        "\n",
        "      #TCBL1\n",
        "      TCBL1 = self.TCBL1(CBL_7_neck,train_flag)\n",
        "\n",
        "      #CBL_connect_branch_2\n",
        "      CBL_connect_branch_2 = self.CBL_connect_branch_2(CSP8_branch_2,train_flag)\n",
        "\n",
        "      #concat TCBL1 -- CBL_connect_branch_2\n",
        "      concat_TCBL1_CBL_connect_branch_2 = tf.keras.layers.concatenate(inputs=[TCBL1,CBL_connect_branch_2],axis=-1)\n",
        "\n",
        "      #CBL_1 phase1\n",
        "      CBL_1_phase1 = self.CBL_1_phase1(concat_TCBL1_CBL_connect_branch_2,train_flag)\n",
        "\n",
        "      #CBL_2_phase1\n",
        "      CBL_2_phase1 = self.CBL_2_phase1(CBL_1_phase1,train_flag)\n",
        "\n",
        "      #CBL_3_phase1\n",
        "      CBL_3_phase1 = self.CBL_3_phase1(CBL_2_phase1,train_flag)\n",
        "\n",
        "      #CBL_4_phase1\n",
        "      CBL_4_phase1 = self.CBL_4_phase1(CBL_3_phase1,train_flag)\n",
        "\n",
        "      #CBL_5_phase1_branch_4 ----------------------------------------- branch 4\n",
        "      CBL_5_phase1_branch_4 = self.CBL_5_phase1_branch_4(CBL_4_phase1,train_flag)\n",
        "\n",
        "      #CBL_6_phase1\n",
        "      CBL_6_phase1 = self.CBL_6_phase1(CBL_5_phase1_branch_4,train_flag)\n",
        "\n",
        "      #TCBL2\n",
        "      TCBL2 = self.TCBL2(CBL_6_phase1,train_flag)\n",
        "\n",
        "      #CBL_connect_branch_1\n",
        "      CBL_connect_branch_1 = self.CBL_connect_branch_1(CSP8_branch_1,train_flag)\n",
        "\n",
        "      #concat TCBL2 -- CBL_connect_branch_1 , out: 79 x 79 x 256\n",
        "      concat_TCBL2_CBL_connect_branch_1 = tf.keras.layers.concatenate(inputs=[TCBL2,CBL_connect_branch_1],axis=-1)\n",
        "\n",
        "      #CBL_1_phase2\n",
        "      CBL_1_phase2 = self.CBL_1_phase2(concat_TCBL2_CBL_connect_branch_1,train_flag)\n",
        "\n",
        "      #CBL_2_phase2\n",
        "      CBL_2_phase2 = self.CBL_2_phase2(CBL_1_phase2,train_flag)\n",
        "\n",
        "      #CBL_3_phase2\n",
        "      CBL_3_phase2 = self.CBL_3_phase2(CBL_2_phase2,train_flag)\n",
        "\n",
        "      #CBL_4_phase2\n",
        "      CBL_4_phase2 = self.CBL_4_phase2(CBL_3_phase2,train_flag)\n",
        "\n",
        "      #CBL_5_phase2_branch_5 ----------------------------------------- branch 5\n",
        "      CBL_5_phase2_branch_5 = self.CBL_5_phase2_branch_5(CBL_4_phase2,train_flag)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# small output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #TCBL_small\n",
        "      TCBL_small = self.TCBL_small(CBL_5_phase2_branch_5,train_flag)\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_1 \n",
        "      CBL_prob_class_small_1 = self.CBL_prob_class_small_1(TCBL_small,train_flag)\n",
        "\n",
        "      #CBL_prob_class_2 in : 80 x 80 x 256 out : 80 x 80 x 256\n",
        "      CBL_prob_class_small_2 = self.CBL_prob_class_small_2(CBL_prob_class_small_1,train_flag)\n",
        "\n",
        "      #CBL_prob_small \n",
        "      conv_prob_small = self.conv_prob_small(CBL_prob_class_small_2)\n",
        "\n",
        "      #CBL_class_small \n",
        "      conv_class_small = self.conv_class_small(CBL_prob_class_small_2)\n",
        "\n",
        "      #reg info\n",
        "      \n",
        "      #CBL_left_center_small_1\n",
        "      CBL_left_center_small_1 = self.CBL_left_center_small_1(TCBL_small,train_flag)\n",
        "\n",
        "      #CBL_left_center_small_2\n",
        "      CBL_left_center_small_2 = self.CBL_left_center_small_2(CBL_left_center_small_1,train_flag)\n",
        "\n",
        "      #conv_pos_info_small\n",
        "      conv_pos_info_small = self.conv_pos_info_small(CBL_left_center_small_2)\n",
        "\n",
        "      #concat conv_prob_small -- conv_pos_info_small -- conv_class_small , out: 80 x 80 x 85\n",
        "      output_small = tf.keras.layers.concatenate(inputs=[conv_prob_small,conv_pos_info_small,conv_class_small],axis=-1,name=\"output_small\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# small output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_connect_branch_5\n",
        "      CBL_connect_branch_5 = self.CBL_connect_branch_5(CBL_5_phase2_branch_5,train_flag)\n",
        "\n",
        "      #concat CBL_connect_branch_5 -- CBL_5_phase1_branch_4 \n",
        "      concat_CBL_connect_branch_5_CBL_5_phase1_branch_4 = tf.keras.layers.concatenate(inputs=[CBL_connect_branch_5,CBL_5_phase1_branch_4],axis=-1)\n",
        "      \n",
        "\n",
        "      #CBL_1 phase3 \n",
        "      CBL_1_phase3 = self.CBL_1_phase3(concat_CBL_connect_branch_5_CBL_5_phase1_branch_4,train_flag)\n",
        "\n",
        "      #CBL_2 phase3 \n",
        "      CBL_2_phase3 = self.CBL_2_phase3(CBL_1_phase3,train_flag)\n",
        "\n",
        "      #CBL_3 phase3 \n",
        "      CBL_3_phase3 = self.CBL_3_phase3(CBL_2_phase3,train_flag)\n",
        "\n",
        "      #CBL_4 phase3 \n",
        "      CBL_4_phase3 = self.CBL_4_phase3(CBL_3_phase3,train_flag)\n",
        "\n",
        "      #CBL_5 phase3_branch_6 ----------------------------------------- branch 6\n",
        "      CBL_5_phase3_branch_6  = self.CBL_5_phase3_branch_6(CBL_4_phase3,train_flag)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# medium output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #TCBL_medium\n",
        "      TCBL_medium = self.TCBL_medium(CBL_5_phase3_branch_6,train_flag)\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_medium_1 \n",
        "      CBL_prob_class_medium_1 = self.CBL_prob_class_medium_1(TCBL_medium,train_flag)\n",
        "\n",
        "      #CBL_prob_class_medium_2\n",
        "      CBL_prob_class_medium_2 = self.CBL_prob_class_medium_2(CBL_prob_class_medium_1,train_flag)\n",
        "\n",
        "      #conv_prob_medium \n",
        "      conv_prob_medium = self.conv_prob_medium(CBL_prob_class_medium_2)\n",
        "\n",
        "      #conv_class_medium \n",
        "      conv_class_medium = self.conv_class_medium(CBL_prob_class_medium_2)\n",
        "\n",
        "\n",
        "      #Reg info\n",
        "      \n",
        "      #CBL_left_center_medium_1 \n",
        "      CBL_left_center_medium_1 = self.CBL_left_center_medium_1(TCBL_medium,train_flag)\n",
        "\n",
        "      #CBL_left_center_medium_2 \n",
        "      CBL_left_center_medium_2 = self.CBL_left_center_medium_2(CBL_left_center_medium_1,train_flag)\n",
        "\n",
        "      #conv_pos_info_medium \n",
        "      conv_pos_info_medium = self.conv_pos_info_medium(CBL_left_center_medium_2)\n",
        "\n",
        "      #concat conv_prob_medium -- conv_pos_info_medium -- conv_class_medium , out: 40 x 40 x 85\n",
        "      output_medium = tf.keras.layers.concatenate(inputs=[conv_prob_medium,conv_pos_info_medium,conv_class_medium],axis=-1,name=\"output_medium\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# medium output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      #CBL_connect_branch_6 \n",
        "      CBL_connect_branch_6 = self.CBL_connect_branch_6(CBL_5_phase3_branch_6,train_flag)\n",
        "\n",
        "      #concat CBL_connect_branch_6 -- CBL_6_branch_3\n",
        "      concat_CBL_connect_branch_6_CBL_6_branch_3 = tf.keras.layers.concatenate(inputs=[CBL_connect_branch_6,CBL_6_branch_3],axis=-1)\n",
        "\n",
        "      #CBL_1 phase4 \n",
        "      CBL_1_phase4 = self.CBL_1_phase4(concat_CBL_connect_branch_6_CBL_6_branch_3,train_flag)\n",
        "\n",
        "      #CBL_2 phase4 \n",
        "      CBL_2_phase4 = self.CBL_2_phase4(CBL_1_phase4,train_flag)\n",
        "\n",
        "      #CBL_3 phase4\n",
        "      CBL_3_phase4 = self.CBL_3_phase4(CBL_2_phase4,train_flag)\n",
        "\n",
        "      #CBL_4 phase4 \n",
        "      CBL_4_phase4 = self.CBL_4_phase4(CBL_3_phase4,train_flag)\n",
        "\n",
        "      #CBL_5 phase4 \n",
        "      CBL_5_phase4 = self.CBL_5_phase4(CBL_4_phase4,train_flag)\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# large output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "      \n",
        "      #TCBL_large \n",
        "      TCBL_large = self.TCBL_large(CBL_5_phase4,train_flag)\n",
        "\n",
        "      #prob info\n",
        "\n",
        "      #CBL_prob_class_large_1 \n",
        "      CBL_prob_class_large_1 = self.CBL_prob_class_large_1(TCBL_large,train_flag)\n",
        "\n",
        "      #CBL_prob_class_large_2\n",
        "      CBL_prob_class_large_2 = self.CBL_prob_class_large_2(CBL_prob_class_large_1,train_flag)\n",
        "\n",
        "      #conv_prob_large \n",
        "      conv_prob_large = self.conv_prob_large(CBL_prob_class_large_2)\n",
        "\n",
        "      #conv_class_large \n",
        "      conv_class_large = self.conv_class_large(CBL_prob_class_large_2)\n",
        "\n",
        "      #Reg info\n",
        "      \n",
        "      #CBL_left_center_large_1 \n",
        "      CBL_left_center_large_1 = self.CBL_left_center_large_1(TCBL_large,train_flag)\n",
        "\n",
        "      #CBL_left_center_large_2 \n",
        "      CBL_left_center_large_2 = self.CBL_left_center_large_2(CBL_left_center_large_1,train_flag)\n",
        "\n",
        "      #conv_pos_info_large \n",
        "      conv_pos_info_large = self.conv_pos_info_large(CBL_left_center_large_2) \n",
        "\n",
        "      #concat conv_prob_large -- conv_pos_info_large -- conv_class_large , out: 20 x 20 x 85\n",
        "      output_large = tf.keras.layers.concatenate(inputs=[conv_prob_large,conv_pos_info_large,conv_class_large],axis=-1,name=\"output_large\")\n",
        "\n",
        "      #$#$#$#$#$#$#$#$#$#$#$#$#$# large output $#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
        "\n",
        "      return [output_large,output_medium,output_small]\n",
        "\n",
        "   def graph_model(self,dim):\n",
        "\n",
        "      x = tf.keras.layers.Input(shape=dim)\n",
        "      \n",
        "      return tf.keras.Model(inputs=x,outputs=self.call(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usga9mVHpXf4"
      },
      "source": [
        "initializer = tf.random_normal_initializer()\n",
        "k = tf.Variable(initial_value=initializer(shape=(10,640,640,3),dtype=\"float32\"))\n",
        "in_x = tf.keras.layers.Input(shape=tuple(k.get_shape().as_list()[1:]))\n",
        "\n",
        "y = alpha_model()\n",
        "model = y.graph_model(tuple(in_x.get_shape().as_list()[1:]))\n",
        "plot_model(model,show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcsmrGjCfjpE"
      },
      "source": [
        "class alpha_loss(tf.keras.losses.Loss):\n",
        "\n",
        "  def __init__(self,gamma = 2,**kwargs):\n",
        "\n",
        "    super(alpha_loss,self).__init__(**kwargs)\n",
        "\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def call(self,y_true,y_pred):\n",
        "\n",
        "    \"\"\"\n",
        "    y_true -- (batch_size,H,W,info) -- info [prob,x_left,y_left,x_center,y_center,class]\n",
        "    y_pred -- (batch_size,H,W,info) -- info [prob,x_left,y_left,x_center,y_center,class]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #get object mask\n",
        "    object_mask = K.cast(y_true[:,:,:,0:1],K.dtype(y_pred))\n",
        "\n",
        "    object_mask_bool = K.cast(object_mask,dtype=tf.bool)\n",
        "\n",
        "    #get ignore mask\n",
        "    ignore_mask  = tf.TensorArray(K.dtype(y_pred),size=1,dynamic_size=True)\n",
        "\n",
        "    ignore_mask = get_ignore_mask(ignore_mask,y_true,y_pred,object_mask_bool)\n",
        "\n",
        "    #get prob\n",
        "    prob_true = y_true[:,:,:,0:1]\n",
        "    prob_pred = y_pred[:,:,:,0:1]\n",
        "\n",
        "    prob_true = K.cast(prob_true,K.dtype(prob_pred))\n",
        "\n",
        "    #get class\n",
        "    class_true = y_true[:,:,:,5:]\n",
        "    class_pred = y_pred[:,:,:,5:]\n",
        "\n",
        "    class_true = K.cast(class_true,K.dtype(class_pred))\n",
        "\n",
        "    #****************** Focal loss ******************\n",
        "\n",
        "    #get batch size\n",
        "    m = K.cast(K.shape(y_pred)[0],K.dtype(y_pred))\n",
        "\n",
        "    #clip the prediction\n",
        "    #prob_pred = K.clip(prob_pred,min_value = 0.0, max_value = 1.0)\n",
        "    #class_pred = K.clip(class_pred,min_value = 0.0, max_value = 1.0)\n",
        "\n",
        "    #prob focal loss\n",
        "    loss_tensor_prob =  - ( (1 - prob_pred[:,:,:,:])**self.gamma ) * prob_true[:,:,:,:] * tf.math.log( prob_pred[:,:,:,:] + 1e-18 ) - ( prob_pred[:,:,:,:] ** self.gamma ) * ( 1 - prob_true[:,:,:,:] ) * tf.math.log( 1 - prob_pred[:,:,:,:] + 1e-18 )\n",
        "    \n",
        "    pos_loss_tensor_prob = loss_tensor_prob[:,:,:,:] * object_mask[:,:,:,:]\n",
        "    neg_loss_tensor_prob = loss_tensor_prob[:,:,:,:] * (1 - object_mask[:,:,:,:]) * ignore_mask[:,:,:,:]\n",
        "    \n",
        "    prob_focal_loss = K.sum( (pos_loss_tensor_prob[:,:,:,:] + neg_loss_tensor_prob[:,:,:,:]) )/ m\n",
        "\n",
        "    #class focal loss\n",
        "    loss_tensor_class =  - ( (1 - class_pred[:,:,:,:])**self.gamma ) * class_true[:,:,:,:] * tf.math.log( class_pred[:,:,:,:] + 1e-18 ) - ( class_pred[:,:,:,:] ** self.gamma) * ( 1 - class_true[:,:,:,:] ) * tf.math.log( 1 - class_pred[:,:,:,:] + 1e-18 )\n",
        "    class_focal_loss = K.sum(loss_tensor_class[:,:,:,:]*object_mask[:,:,:,:]) / m\n",
        "\n",
        "\n",
        "    #****************** Focal loss ******************\n",
        "\n",
        "    #get reg left -- (x,y)\n",
        "    reg_left_true = y_true[:,:,:,1:3] \n",
        "    reg_left_pred = y_pred[:,:,:,1:3]\n",
        "\n",
        "    reg_left_true = K.cast(reg_left_true,K.dtype(reg_left_pred))\n",
        "\n",
        "    #get reg center -- (x,y)\n",
        "    reg_center_true = y_true[:,:,:,3:5] \n",
        "    reg_center_pred = y_pred[:,:,:,3:5]\n",
        "\n",
        "    reg_center_true = K.cast(reg_center_true,K.dtype(reg_center_pred))\n",
        "\n",
        "    #calculate width x height of anchor box\n",
        "    reg_wh_true = (reg_center_true[:,:,:,:] - reg_left_true[:,:,:,:])*2\n",
        "    reg_wh_pred = (reg_center_pred[:,:,:,:] - reg_left_pred[:,:,:,:])*2\n",
        "\n",
        "    #get reg right\n",
        "    reg_right_true = reg_left_true[:,:,:,:] + reg_wh_true[:,:,:,:]\n",
        "    reg_right_pred = reg_left_pred[:,:,:,:] + reg_wh_pred[:,:,:,:]\n",
        "\n",
        "    #get reg width\n",
        "    reg_width_true = reg_wh_true[:,:,:,0:1] \n",
        "    reg_width_pred = reg_wh_pred[:,:,:,0:1]\n",
        "\n",
        "    #get reg height\n",
        "    reg_height_true = reg_wh_true[:,:,:,1:2]\n",
        "    reg_height_pred = reg_wh_pred[:,:,:,1:2]\n",
        "\n",
        "    \n",
        "    #****************** Box Scale Entropy ******************\n",
        "    \n",
        "    #calculate mini width\n",
        "    reg_width_mini = tf.math.minimum(reg_width_true,reg_width_pred)\n",
        "\n",
        "    reg_width_mini = K.square(reg_width_mini)\n",
        "    \n",
        "    #calculate max width\n",
        "    reg_width_max = tf.math.maximum(reg_width_true,reg_width_pred)\n",
        "\n",
        "    reg_width_max = K.square(reg_width_max)\n",
        "    \n",
        "\n",
        "    #calculate width scale\n",
        "    reg_width_scale = reg_width_mini[:,:,:,:] / ( reg_width_max[:,:,:,:] + 1e-10 ) \n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "    \n",
        "    #calculate mini height\n",
        "    reg_height_mini = tf.math.minimum(reg_height_true,reg_height_pred)\n",
        "\n",
        "    reg_height_mini = K.square(reg_height_mini)\n",
        "\n",
        "    #calculate max height\n",
        "    reg_height_max = tf.math.maximum(reg_height_true,reg_height_pred)\n",
        "\n",
        "    reg_height_max = K.square(reg_height_max)\n",
        "\n",
        "\n",
        "    #calculate height scale\n",
        "    reg_height_scale =  reg_height_mini[:,:,:,:] / ( reg_height_max[:,:,:,:] + 1e-10 )\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "\n",
        "    #Box Scale Entropy\n",
        "    loss_tensor_Box_Scale_Entropy = - tf.math.log(reg_width_scale[:,:,:,:] + 1e-18)  -  tf.math.log(reg_height_scale[:,:,:,:] + 1e-18)\n",
        "\n",
        "    box_scale_entropy_loss = K.sum(loss_tensor_Box_Scale_Entropy[:,:,:,:] * object_mask[:,:,:,:] ) / m\n",
        "    \n",
        "\n",
        "    #****************** Box Scale Entropy ******************\n",
        "    \n",
        "    #****************** IOU loss ******************\n",
        "    \n",
        "    #calculate IOU  \n",
        "\n",
        "    #calculate intersection left\n",
        "    reg_left_intersection = tf.math.maximum(reg_left_true,reg_left_pred)\n",
        "\n",
        "    #calculate intersection right\n",
        "    reg_right_intersection = tf.math.minimum(reg_right_true,reg_right_pred)\n",
        "\n",
        "    #calibrate\n",
        "    #reg_right_intersection = tf.where((reg_left_intersection>reg_right_intersection),reg_left_intersection,reg_right_intersection) #-- same meaning\n",
        "    reg_right_intersection = tf.math.maximum(reg_left_intersection,reg_right_intersection) #-- same meaning\n",
        "\n",
        "    #intersection width height\n",
        "    intersection_wh = reg_right_intersection[:,:,:,:] - reg_left_intersection[:,:,:,:]\n",
        "\n",
        "    #intersection area\n",
        "    intersection_area = intersection_wh[:,:,:,0:1] * intersection_wh[:,:,:,1:2]\n",
        "\n",
        "    #union area\n",
        "    true_area = reg_wh_true[:,:,:,0:1] * reg_wh_true[:,:,:,1:2]\n",
        "    pred_area = reg_wh_pred[:,:,:,0:1] * reg_wh_pred[:,:,:,1:2]\n",
        "\n",
        "    union_area = true_area[:,:,:,:] + pred_area[:,:,:,:] - intersection_area[:,:,:,:]\n",
        "\n",
        "    #calculate iou \n",
        "    iou_val = intersection_area[:,:,:,:] / ( union_area[:,:,:,:] +  1e-10 )\n",
        "\n",
        "    #iou loss\n",
        "    loss_tensor_iou = - tf.math.log( iou_val[:,:,:,:] + 1e-18 ) * object_mask[:,:,:,:] \n",
        "\n",
        "    iou_loss = K.sum( loss_tensor_iou ) / m\n",
        "\n",
        "    #****************** IOU loss ******************\n",
        "    \n",
        "    #****************** Loc loss ******************\n",
        "    loss_tensor_loc = ( K.square(reg_left_true[:,:,:,:] - reg_left_pred[:,:,:,:]) + K.square(reg_center_true[:,:,:,:] - reg_center_pred[:,:,:,:]) ) * object_mask[:,:,:,:] \n",
        "\n",
        "    #loc loss\n",
        "    loc_loss = K.sum(loss_tensor_loc) / m\n",
        "\n",
        "    #****************** Loc loss ******************\n",
        "\n",
        "    #calculate reg loss\n",
        "    reg_loss = loc_loss + iou_loss + box_scale_entropy_loss\n",
        " \n",
        "    #calculate loss\n",
        "    loss = prob_focal_loss + class_focal_loss + reg_loss\n",
        " \n",
        "    return loss\n",
        "\n",
        "def get_ignore_mask(ignore_mask,y_true,y_pred,object_mask_bool,ignore_threshold = 0.5):\n",
        "\n",
        "  \"\"\"\n",
        "  y_true -- (batch_size,H,W,info) -- info [prob,x_left,y_left,x_center,y_center,class]\n",
        "  y_pred -- (batch_size,H,W,info) -- info [prob,x_left,y_left,x_center,y_center,class]\n",
        "  ignore_mask -- TensorArray\n",
        "  \"\"\"\n",
        "  #set up\n",
        "  y_true = K.cast(y_true,K.dtype(y_pred))\n",
        "\n",
        "  m = K.cast(K.shape(y_true)[0],tf.int32)\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  i = K.cast(i,K.dtype(m))\n",
        "\n",
        "  #loop via each batch\n",
        "  while i < m:\n",
        "\n",
        "    #get true box -- shape (n,4)\n",
        "    true_box = tf.boolean_mask(y_true[i,:,:,1:5],object_mask_bool[i,:,:,0])\n",
        "\n",
        "    #get true box -- shape (1,n,4)\n",
        "    true_box = true_box[tf.newaxis,:,:]\n",
        "\n",
        "    #get y_pred_batch -- shape (h,w,4)\n",
        "    y_pred_batch = y_pred[i,:,:,1:5]\n",
        "    \n",
        "    #get y_pred_batch -- shape (h,w,1,4)\n",
        "    y_pred_batch = y_pred_batch[:,:,tf.newaxis,:]\n",
        "\n",
        "    #****************** IOU ******************\n",
        "\n",
        "    #batch pred -- (h,w,1,4)\n",
        "    batch_pred_left_xy =  y_pred_batch[:,:,:,0:2]\n",
        "    batch_pred_center_xy = y_pred_batch[:,:,:,2:4]\n",
        "    \n",
        "    batch_pred_wh = (batch_pred_center_xy[:,:,:,:] - batch_pred_left_xy[:,:,:,:])*2\n",
        "\n",
        "    batch_pred_right_xy = batch_pred_left_xy[:,:,:,:] + batch_pred_wh[:,:,:,:] \n",
        "\n",
        "    #batch true -- (1,n,4)\n",
        "    batch_true_left_xy = true_box[:,:,0:2]\n",
        "    batch_true_center_xy = true_box[:,:,2:4]\n",
        "\n",
        "    batch_true_wh =  (batch_true_center_xy[:,:,:] - batch_true_left_xy[:,:,:])*2\n",
        "\n",
        "    batch_true_right_xy = batch_true_left_xy[:,:,:] + batch_true_wh[:,:,:]\n",
        "\n",
        "    #intersection -- (h,w,n,2)\n",
        "    batch_intersection_left = K.maximum(batch_pred_left_xy,batch_true_left_xy)\n",
        "    batch_intersection_right = K.minimum(batch_pred_right_xy,batch_true_right_xy)\n",
        "\n",
        "    #calibrate \n",
        "    batch_intersection_right = K.maximum(batch_intersection_right,batch_intersection_left)\n",
        "\n",
        "    #batch intersection wh -- (h,w,n,2)\n",
        "    batch_intersection_wh = batch_intersection_right[:,:,:,:] - batch_intersection_left[:,:,:,:]\n",
        "    \n",
        "    #batch intersection area -- (h,w,n)\n",
        "    batch_intersection_area = batch_intersection_wh[:,:,:,0] * batch_intersection_wh[:,:,:,1]\n",
        "\n",
        "    #batch union area -- (h,w,n)\n",
        "    batch_true_union_area = batch_true_wh[:,:,0] * batch_true_wh[:,:,1]\n",
        "    batch_pred_union_area = batch_pred_wh[:,:,:,0] * batch_pred_wh[:,:,:,1]\n",
        "\n",
        "    batch_union_area = (batch_true_union_area + batch_pred_union_area) - batch_intersection_area\n",
        "\n",
        "    #calculate iou -- (h,w,n)\n",
        "    batch_iou = batch_intersection_area / batch_union_area\n",
        "\n",
        "    #****************** IOU ******************\n",
        "\n",
        "    #get best iou -- (h,w,1)\n",
        "    best_iou = K.max(batch_iou,axis=-1,keepdims=True)\n",
        "\n",
        "    #ignore value -- (h,w,1)\n",
        "    ignore_val = K.cast(best_iou < ignore_threshold,K.dtype(y_pred))\n",
        "\n",
        "    #update ignore mask\n",
        "    ignore_mask = ignore_mask.write(i,ignore_val)\n",
        "\n",
        "    #update i\n",
        "    i = i + 1\n",
        "\n",
        "  #stack to tensor -- (m,h,w,1)\n",
        "  ignore_mask = ignore_mask.stack()\n",
        "\n",
        "  return ignore_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyZsdFRpkz4E"
      },
      "source": [
        "def preprocess_class(input_path,save_path ,name=\"class_map.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   MS COCO 2017 Dataset\n",
        "\n",
        "   return dict\n",
        "   \"\"\"\n",
        "   dataset = pd.read_csv(input_path)\n",
        "\n",
        "   #get class map\n",
        "   class_array = dataset.iloc[:,3]\n",
        "   class_map = {}\n",
        "   idx = 0\n",
        "   \n",
        "   for i in range(class_array.shape[0]):\n",
        "\n",
        "      if not (class_array[i] in class_map.keys()):\n",
        "\n",
        "         class_map[class_array[i]] = idx\n",
        "         idx = idx + 1\n",
        "\n",
        "   #save class map \n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "      \n",
        "     file.write(json.dumps(class_map))\n",
        "\n",
        "   return class_map\n",
        "\n",
        "def preprocess_bbox_info(path,path_pos,path_hw,name_pos=\"bbox_pos.txt\",name_hw=\"bbox_hw.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   MS COCO 2017 Dataset\n",
        "\n",
        "   return numpy.ndarray,bbox_pos,bbox_hw\n",
        "   \"\"\"\n",
        "\n",
        "   dataset = pd.read_csv(path)\n",
        "\n",
        "   \"\"\"\n",
        "   #find center\n",
        "   dataset[\"center_x\"] = (dataset[\"xmax\"] + dataset[\"xmin\"])/2\n",
        "   dataset[\"center_y\"] = (dataset[\"ymax\"] + dataset[\"ymin\"])/2\n",
        "   \"\"\"\n",
        "\n",
        "   #get positional data\n",
        "   bbox_pos = dataset.iloc[:,4:].to_numpy()\n",
        "\n",
        "   #save bbox_pos\n",
        "   with open(f\"{path_pos}/{name_pos}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(bbox_pos.tolist()))\n",
        "\n",
        "   #construct bbox_hw (m,2) : h -> 0 , w -> 1\n",
        "   m = bbox_pos.shape[0]\n",
        "   bbox_hw = np.zeros((m,2))\n",
        "\n",
        "   for i in range(m):\n",
        "\n",
        "      bbox_hw[i,0] = bbox_pos[i,3] - bbox_pos[i,1]\n",
        "      bbox_hw[i,1] = bbox_pos[i,2] - bbox_pos[i,0]\n",
        "\n",
        "\n",
        "   #save bbox_hw \n",
        "   with open(f\"{path_hw}/{name_hw}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(bbox_hw.tolist()))\n",
        "\n",
        "\n",
        "   return bbox_pos,bbox_hw\n",
        "\n",
        "\n",
        "def preprocess_pre_define_anchor_box(bbox_hw,save_path,K=9,name=\"anchors.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   bbox_hw -- numpy.ndarray (m,2)\n",
        "   \"\"\"\n",
        "\n",
        "   anchors = Kmean(bbox_hw,K)\n",
        "\n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(anchors.tolist()))\n",
        "\n",
        "   \n",
        "   return anchors\n",
        "\n",
        "def preprocessing_label(input_path,save_path,name=\"gt_dataset.txt\"):\n",
        "\n",
        "   \"\"\"\n",
        "   save dict as {obj1:[[class,xmin,ymin,xcenter,ycenter],[class,xmin,ymin,xcenter,ycenter],...],obj2:...} (for each key)\n",
        "   \"\"\"\n",
        "\n",
        "   #read csv file\n",
        "   dataset = pd.read_csv(input_path)\n",
        "   m = dataset.shape[0]\n",
        "\n",
        "   #calibrate bbox pos\n",
        "   dataset[\"xmin\"] = dataset[\"xmin\"] + (640 - dataset[\"width\"] )//2\n",
        "   dataset[\"xmax\"] = dataset[\"xmax\"] + (640 - dataset[\"width\"] )//2\n",
        "\n",
        "   dataset[\"ymin\"] = dataset[\"ymin\"] + (640 - dataset[\"height\"] )//2\n",
        "   dataset[\"ymax\"] = dataset[\"ymax\"] + (640 - dataset[\"height\"] )//2\n",
        "\n",
        "   #calculate center\n",
        "   dataset[\"xcenter\"] = (dataset[\"xmin\"] + dataset[\"xmax\"])/2\n",
        "   dataset[\"ycenter\"] = (dataset[\"ymin\"] + dataset[\"ymax\"])/2\n",
        "\n",
        "   #Group the label\n",
        "   gt_dataset = {}\n",
        "   \n",
        "   for i in range(m):\n",
        "\n",
        "      filename = dataset.iloc[i,0]\n",
        "      tmp = []\n",
        "\n",
        "      #class\n",
        "      tmp.append(dataset.iloc[i,3])\n",
        "\n",
        "      #xmin xmax ymin ymax\n",
        "      tmp.append(dataset.iloc[i,4].item())\n",
        "      tmp.append(dataset.iloc[i,5].item())\n",
        "      tmp.append(dataset.iloc[i,8].item())\n",
        "      tmp.append(dataset.iloc[i,9].item())\n",
        "\n",
        "      if not ( filename in gt_dataset.keys()  ):\n",
        "\n",
        "         gt_dataset[filename] = []\n",
        "         \n",
        "      gt_dataset[filename].append(tmp)\n",
        "\n",
        "\n",
        "   #save dataset\n",
        "   with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps(gt_dataset))\n",
        "\n",
        "      file.close()\n",
        "\n",
        "   return gt_dataset\n",
        "   \n",
        "def preprocess_image(img,standard_shape=(640,640)):\n",
        "\n",
        "   \"\"\"\n",
        "   img -- numpy.ndarray\n",
        "   standard_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   #get h,w\n",
        "   height,width = standard_shape\n",
        "\n",
        "   #get pad size\n",
        "   padH = (height - img.shape[0])//2\n",
        "   padW = (width - img.shape[1])//2\n",
        "\n",
        "   #pad img\n",
        "   diff_H = height - img.shape[0]\n",
        "   diff_W = width - img.shape[1]\n",
        "   \n",
        "   if (diff_H % 2 ) == 0 and (diff_W % 2) == 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) != 0 and (diff_W % 2) == 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH+1),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) == 0 and (diff_W % 2) != 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH),(padW,padW+1),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   elif (diff_H % 2 ) != 0 and (diff_W % 2) != 0:\n",
        "\n",
        "      img_pad = np.pad(img,((padH,padH+1),(padW,padW+1),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "   return img_pad\n",
        "   \n",
        "\n",
        "def preprocess_y_true(input_path,save_path,anchors,class_map,input_shape = (640,640),pos_info_format = [(76,76,255),(38,38,255),(19,19,255)],bbox_type=3):\n",
        "\n",
        "   \"\"\"\n",
        "   anchors -- numpy.ndarray (K,2)\n",
        "   input_shape -- (x,y)\n",
        "   \"\"\"\n",
        "\n",
        "   #read csv file\n",
        "   dataset = pd.read_csv(input_path)\n",
        "   m = dataset.shape[0]\n",
        "\n",
        "   #get center info\n",
        "   dataset[\"x_center\"] = (dataset[\"xmin\"] + dataset[\"xmax\"])/2\n",
        "   dataset[\"y_center\"] = (dataset[\"ymin\"] + dataset[\"ymax\"])/2\n",
        "\n",
        "   #calibrate bbox pos\n",
        "   dataset[\"xmin\"] = dataset[\"xmin\"] + (640 - dataset[\"width\"] )//2\n",
        "   dataset[\"xmax\"] = dataset[\"xmax\"] + (640 - dataset[\"width\"] )//2\n",
        "\n",
        "   dataset[\"ymin\"] = dataset[\"ymin\"] + (640 - dataset[\"height\"] )//2\n",
        "   dataset[\"ymax\"] = dataset[\"ymax\"] + (640 - dataset[\"height\"] )//2\n",
        "\n",
        "   #set up\n",
        "   prev_name = \"#\"\n",
        "   pos_format_size = len(pos_info_format)\n",
        "\n",
        "   #loop through dataset\n",
        "   for i in range(m):\n",
        "\n",
        "      curr_name = dataset.iloc[i,0]\n",
        "      class_name = dataset.iloc[i,3]\n",
        "\n",
        "      if curr_name == prev_name:\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "      \n",
        "      elif curr_name != prev_name and prev_name != \"#\":\n",
        "\n",
        "         #save prev pos info\n",
        "         for q in range(pos_format_size):\n",
        "            \n",
        "            h_size = pos_info_format[q][0]\n",
        "            w_size = pos_info_format[q][1]\n",
        "            \n",
        "            with open(f\"{save_path}/{curr_name}_{h_size}x{w_size}.txt\",\"w\") as file:\n",
        "\n",
        "               file.write(json.dumps((pos_info[q]).tolist()))\n",
        "\n",
        "               file.close()\n",
        "\n",
        "         #crreate new pos info\n",
        "         pos_info = [np.zeros(pos_info_format[0]),np.zeros(pos_info_format[1]),np.zeros(pos_info_format[2])]\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "\n",
        "      elif prev_name == \"#\":\n",
        "\n",
        "         #crreate new pos info\n",
        "         pos_info = [np.zeros(pos_info_format[0]),np.zeros(pos_info_format[1]),np.zeros(pos_info_format[2])]\n",
        "\n",
        "         #update pos info\n",
        "         update_pos_info(pos_info,dataset.iloc[i,8].item(),dataset.iloc[i,9].item(),dataset.iloc[i,4].item(),dataset.iloc[i,5].item(),dataset.iloc[i,6].item(),dataset.iloc[i,7].item(),class_map[dataset.iloc[i,3]],anchors,image_shape=input_shape,bbox_type=bbox_type,feature_size=85)\n",
        "\n",
        "      #update prev_name\n",
        "      prev_name = curr_name\n",
        "\n",
        "   #save last obj\n",
        "   for q in range(pos_format_size):\n",
        "   \n",
        "      h_size = pos_info_format[q][0]\n",
        "      w_size = pos_info_format[q][1]\n",
        "      \n",
        "      with open(f\"{save_path}/{curr_name}_{h_size}x{w_size}.txt\",\"w\") as file:\n",
        "\n",
        "         file.write(json.dumps((pos_info[q]).tolist()))\n",
        "\n",
        "         file.close()\n",
        "\n",
        "\n",
        "#@jit(nopython=True)\n",
        "def update_pos_info(pos_info,center_x,center_y,xmin,ymin,xmax,ymax,class_index,anchors,image_shape = (640,640),bbox_type = 3,feature_size = 85):\n",
        "\n",
        "   \"\"\"\n",
        "   center_info -- (x,y)\n",
        "   pos_info -- list containing numpy.ndarray\n",
        "   obj_pos -- numpy.ndarray (1,4)\n",
        "   anchors -- numpy.ndarray (K,2)\n",
        "   image_shape -- (x,y)\n",
        "   \"\"\"\n",
        "   \n",
        "   #object bbox h w\n",
        "   bbox_h = xmax - xmin\n",
        "   bbox_w = ymax - ymin\n",
        "   \n",
        "   #find bext anchor index\n",
        "   max_index = 0\n",
        "   max_iou = 0\n",
        "   \n",
        "   for i in range(anchors.shape[0]):\n",
        "\n",
        "      min_h = np.minimum(anchors[i,0],bbox_h).item()\n",
        "      min_w = np.minimum(anchors[i,1],bbox_w).item()\n",
        "\n",
        "      #intersection\n",
        "      intersection_area = min_w * min_h\n",
        "\n",
        "      #union\n",
        "      union_area = bbox_h * bbox_w  + anchors[i,0] * anchors[i,1] - intersection_area\n",
        "\n",
        "      #iou\n",
        "      cur_iou = intersection_area / union_area\n",
        "\n",
        "      if cur_iou > max_iou:\n",
        "\n",
        "         max_iou = cur_iou\n",
        "\n",
        "         max_index = i\n",
        "\n",
        "   #size of particular type\n",
        "   type_size = np.int64(anchors.shape[0]/bbox_type).item()\n",
        "   \n",
        "   #best box index -- dim 1 (determine which type of box)\n",
        "   best_anchor_index = np.int64(max_index/type_size).item()\n",
        "      \n",
        "   #best box index -- dim 2 (in a particular type of box , determine sub class of particular type )\n",
        "   sub_class_index = max_index % type_size\n",
        "\n",
        "   #update info (prob,xmin,ymin,xmax,ymax,class)\n",
        "   #feature mapping\n",
        "   feature_per_image_pixel_x = pos_info[best_anchor_index].shape[0] / image_shape[0]\n",
        "   feature_per_image_pixel_y = pos_info[best_anchor_index].shape[1] / image_shape[1]\n",
        "\n",
        "   target_x = np.int64(center_x * feature_per_image_pixel_x).item()\n",
        "   target_y = np.int64(center_y * feature_per_image_pixel_y).item()\n",
        "\n",
        "   #prob\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,0] = 1\n",
        "\n",
        "   #xmin,ymin,xmax,ymax\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,1] = xmin\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,2] = ymin\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,3] = xmax\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,4] = ymax\n",
        "\n",
        "   #class\n",
        "   (pos_info[best_anchor_index])[target_y,target_x,(class_index+5)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_oXa58fUcG"
      },
      "source": [
        "#data augmentation\n",
        "def data_aug(img):\n",
        "\n",
        "   idx = np.random.randint(0,4)\n",
        "\n",
        "   if idx == 0:\n",
        "\n",
        "      h  = tune_odd(np.random.randint(1,17))\n",
        "      w  = tune_odd(np.random.randint(1,17))\n",
        "\n",
        "      kernel_shape = (h,w)\n",
        "\n",
        "      img = GaussianBlur(img,kernel_shape)\n",
        "\n",
        "   elif idx == 1:\n",
        "\n",
        "      img = GaussianNoise(img)\n",
        "\n",
        "   elif idx == 2:\n",
        "\n",
        "      color_aug_seq = [ i for i in range(4)]\n",
        "      random.shuffle(color_aug_seq)\n",
        "\n",
        "      for i in color_aug_seq:\n",
        "\n",
        "         if i == 0:\n",
        "\n",
        "            img = random_brightness(img)\n",
        "\n",
        "         elif i == 1:\n",
        "\n",
        "            img = random_saturation(img)\n",
        "\n",
        "         elif i == 2:\n",
        "\n",
        "            img = random_contrast(img)\n",
        "\n",
        "         elif i == 3:\n",
        "\n",
        "            img = random_hue(img)\n",
        "\n",
        "   elif idx == 3:\n",
        "\n",
        "      for i in range(16):\n",
        "\n",
        "         h = np.random.randint(8,81)\n",
        "         w = np.random.randint(8,81)\n",
        "\n",
        "         kernel_size = (h,w)\n",
        "\n",
        "         img = random_erase(img,kernel_size)\n",
        "\n",
        "      \n",
        "   return img\n",
        "\n",
        "@jit(nopython=True)\n",
        "def tune_odd(val):\n",
        "\n",
        "   if (val % 2) == 0:\n",
        "\n",
        "      val = val + 1\n",
        "\n",
        "   return val\n",
        "\n",
        "#Gaussian Blur\n",
        "def GaussianBlur(img,kernel_shape,sigma=0):\n",
        "\n",
        "   \"\"\"\n",
        "   img -- numpy array\n",
        "   kernel_shape -- (int,int)\n",
        "   sigma -- real number\n",
        "   \"\"\"\n",
        "\n",
        "   return cv2.GaussianBlur(img,kernel_shape,sigma)\n",
        "\n",
        "\n",
        "##Gaussian Noise\n",
        "@jit(nopython=True)\n",
        "def ext_operator(val):\n",
        "\n",
        "  if val > 255 :\n",
        "\n",
        "    val = 255\n",
        "\n",
        "  elif val < 0:\n",
        "\n",
        "    val = 0\n",
        "\n",
        "  return val\n",
        "\n",
        "@jit(nopython=True)\n",
        "def GaussianNoise(img,low=8,high=64):\n",
        "\n",
        "  nH,nW,nC = img.shape\n",
        "\n",
        "  for h in range(nH):\n",
        "\n",
        "    for w in range(nW):\n",
        "\n",
        "      for c in range(nC):\n",
        "\n",
        "        factor = np.random.randint(low,high)\n",
        "\n",
        "        loc = factor * np.random.random()\n",
        "\n",
        "        scale = factor * np.random.random()\n",
        "\n",
        "        noise = np.random.normal(loc,scale)\n",
        "\n",
        "        img[h,w,c] = ext_operator(img[h,w,c]+noise)\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "#brightness\n",
        "def random_brightness(img):\n",
        "\n",
        "   return tf.image.random_brightness(img,0.4).numpy()\n",
        "\n",
        "\n",
        "#saturation\n",
        "def random_saturation(img):\n",
        "\n",
        "   return tf.image.random_saturation(img,1.5,8.0).numpy()\n",
        "\n",
        "#contrast\n",
        "def random_contrast(img):\n",
        "\n",
        "   return tf.image.random_contrast(img,1.5,8.0).numpy()\n",
        "\n",
        "#hue\n",
        "def random_hue(img):\n",
        "\n",
        "   return tf.image.random_hue(img,0.4).numpy()\n",
        "\n",
        "#erase\n",
        "@jit(nopython=True)\n",
        "def random_erase(img,kernel_size=(64,64)):\n",
        "\n",
        "   nH,nW,_ = img.shape\n",
        "   fH,fW = kernel_size\n",
        "\n",
        "   new_nH = nH - fH + 1\n",
        "   new_nW = nW - fW + 1\n",
        "\n",
        "   nH_start = np.random.randint(0,new_nH)\n",
        "   nW_start = np.random.randint(0,new_nW)\n",
        "\n",
        "   nH_end = nH_start + fH\n",
        "   nW_end = nW_start + fW\n",
        "\n",
        "   erase_region = img[nH_start:nH_end,nW_start:nW_end,:].copy()\n",
        "\n",
        "   for h in range(nH_start,nH_end):\n",
        "\n",
        "      for w in range(nW_start,nW_end):\n",
        "         \n",
        "         random_h = np.random.randint(0,fH)\n",
        "         random_w = np.random.randint(0,fW)\n",
        "\n",
        "         img[h,w,:] = erase_region[random_h,random_w,:].copy()\n",
        "\n",
        "\n",
        "   return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3DP1jp0mI2k"
      },
      "source": [
        "def Kmean_IOU(bbox_hw,K=3,threshold=1e-8,max_iterations=2000):\n",
        "\n",
        "   \"\"\"\n",
        "   bbox_hw -- (m,2) : h -> 0 , w -> 1\n",
        "\n",
        "   return -- [large,medium ,small] <-- list(list)\n",
        "   \"\"\"\n",
        "\n",
        "   m = bbox_hw.shape[0]\n",
        "\n",
        "   #ONLY for K = 3 \n",
        "   if m == 1:\n",
        "\n",
        "      return [ [0] , [] , [] ]\n",
        "\n",
        "   elif m == 2:\n",
        "\n",
        "      bbox_1_area = bbox_hw[0,0] * bbox_hw[0,1]\n",
        "      bbox_2_area = bbox_hw[1,0] * bbox_hw[1,1]\n",
        "\n",
        "      if bbox_1_area > bbox_2_area:\n",
        "         \n",
        "         return [ [0] , [1] , [] ]\n",
        "\n",
        "      else:\n",
        "\n",
        "         return [ [1] , [0] , [] ]\n",
        "\n",
        "   #initialize anchors (K,2)\n",
        "   anchors = np.zeros((K,2))\n",
        "   for i in range(K):\n",
        "\n",
        "      anchors[i,:] = bbox_hw[i,:].copy()\n",
        "   \n",
        "   #calculate Kmean\n",
        "   iteration_i = 0\n",
        "   \n",
        "   while iteration_i <= max_iterations:\n",
        "\n",
        "      #store the data by index\n",
        "      output_list = [[] for i in range(K)]\n",
        "\n",
        "      #classify bbox \n",
        "      for i in range(m):\n",
        "\n",
        "         class_id = best_anchor(bbox_hw[i,:].reshape(1,2),anchors)\n",
        "\n",
        "         output_list[class_id].append(i)\n",
        "\n",
        "      #update anchor box\n",
        "      sum_diff = 0\n",
        "      \n",
        "      for i in range(K):\n",
        "         \n",
        "         new_h,new_w = update_anchor_x(bbox_hw,output_list[i],anchors[i,:].reshape(1,2))\n",
        "\n",
        "         \"\"\"\n",
        "         #sum the changes for h and w of new anchor box \n",
        "         sum_diff = sum_diff + ((new_h-anchors[i,0])**2 + (new_w - anchors[i,1])**2)**(0.5)\n",
        "         \"\"\"\n",
        "         #sum the changes for h and w of new anchor box - compare iou\n",
        "         min_h = min(new_h,anchors[i,0])\n",
        "         min_w = min(new_w,anchors[i,1])\n",
        "\n",
        "         intersection = min_h * min_w\n",
        "         union = new_h * new_w + anchors[i,0] * anchors[i,1] - intersection\n",
        "\n",
        "         sum_diff = sum_diff + (1 - intersection / (union + 1e-8) )\n",
        "\n",
        "         #update anchor box\n",
        "         anchors[i,0] = new_h\n",
        "         anchors[i,1] = new_w\n",
        "\n",
        "      if sum_diff < threshold:\n",
        "\n",
        "         return rearrange_output_list_to_correct_order(bbox_hw,output_list)\n",
        "\n",
        "      #update iteration\n",
        "      iteration_i = iteration_i + 1\n",
        "      \n",
        "   return rearrange_output_list_to_correct_order(bbox_hw,output_list)\n",
        "         \n",
        "\n",
        "def rearrange_output_list_to_correct_order(bbox_hw,output_list):\n",
        "\n",
        "   #corrected output_list\n",
        "   corrected_output_list = []\n",
        "   \n",
        "   n =len(output_list)\n",
        "   #find area sum\n",
        "   sample_area_sum = []\n",
        "   for i in range(n):\n",
        "\n",
        "      area = find_area_sum(bbox_hw,output_list[i])\n",
        "      sample_area_sum.append((i,area))\n",
        "\n",
        "   #sort the list\n",
        "   sample_area_sum.sort(key=lambda x:x[1],reverse=True)\n",
        "\n",
        "   #correct output list\n",
        "   for ele in sample_area_sum:\n",
        "\n",
        "      corrected_output_list.append(output_list[ele[0]])\n",
        "\n",
        "   return corrected_output_list\n",
        "   \n",
        "\n",
        "def find_area_sum(bbox_hw,output_list_x):\n",
        "\n",
        "   area = 0\n",
        "\n",
        "   for i in output_list_x:\n",
        "\n",
        "      area = area + bbox_hw[i,0] * bbox_hw[i,1]\n",
        "\n",
        "   return area\n",
        "      \n",
        "   \n",
        "def update_anchor_x(bbox_hw,output_list_x,anchors_x):\n",
        "\n",
        "   \"\"\"\n",
        "   output_list_x -- [ n elements]\n",
        "   anchor_x -- (1,2) : h -> 0 , w -> 1\n",
        "   \"\"\"\n",
        "\n",
        "   sum_w = 0\n",
        "   sum_h = 0\n",
        "\n",
        "   n = len(output_list_x)\n",
        "\n",
        "   for i in output_list_x:\n",
        "\n",
        "      sum_h = sum_h + bbox_hw[i,0]\n",
        "      sum_w = sum_w + bbox_hw[i,1]\n",
        "\n",
        "   if n == 0:\n",
        "\n",
        "      return anchors_x[0,0],anchors_x[0,1]\n",
        "\n",
        "   mean_h = sum_h/n\n",
        "   mean_w = sum_w/n\n",
        "\n",
        "   return mean_h,mean_w\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def best_anchor(box,anchors):\n",
        "   \n",
        "  \"\"\"\n",
        "  box -- (1,2) :h -> 0 , w -> 1\n",
        "\n",
        "  anchors -- (K,2)\n",
        "\n",
        "  return class_idx\n",
        "  \"\"\"\n",
        "  \n",
        "  max_iou = 0\n",
        "  max_index = 0\n",
        "\n",
        "  K = anchors.shape[0]\n",
        "\n",
        "  for i in range(K):\n",
        "\n",
        "    min_h = np.minimum(box[0,0],anchors[i,0])\n",
        "    min_w = np.minimum(box[0,1],anchors[i,1])\n",
        "\n",
        "    intersection_area = min_h * min_w\n",
        "\n",
        "    union_area = box[0,0] * box[0,1] + anchors[i,0] * anchors[i,1] - intersection_area\n",
        "\n",
        "    cur_iou = intersection_area / (union_area + 1e-8)\n",
        "\n",
        "    if cur_iou > max_iou:\n",
        "\n",
        "      max_iou = cur_iou\n",
        "\n",
        "      max_index = i\n",
        "\n",
        "    \n",
        "  return max_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSeHsWPVogua"
      },
      "source": [
        "#data augmentation\n",
        "def data_aug(img):\n",
        "\n",
        "   idx = np.random.randint(0,4)\n",
        "\n",
        "   if idx == 0:\n",
        "\n",
        "      h  = tune_odd(np.random.randint(1,17))\n",
        "      w  = tune_odd(np.random.randint(1,17))\n",
        "\n",
        "      kernel_shape = (h,w)\n",
        "\n",
        "      img = GaussianBlur(img,kernel_shape)\n",
        "\n",
        "   elif idx == 1:\n",
        "\n",
        "      img = GaussianNoise(img)\n",
        "\n",
        "   elif idx == 2:\n",
        "\n",
        "      color_aug_seq = [ i for i in range(4)]\n",
        "      random.shuffle(color_aug_seq)\n",
        "\n",
        "      for i in color_aug_seq:\n",
        "\n",
        "         if i == 0:\n",
        "\n",
        "            img = random_brightness(img)\n",
        "\n",
        "         elif i == 1:\n",
        "\n",
        "            img = random_saturation(img)\n",
        "\n",
        "         elif i == 2:\n",
        "\n",
        "            img = random_contrast(img)\n",
        "\n",
        "         elif i == 3:\n",
        "\n",
        "            img = random_hue(img)\n",
        "\n",
        "   elif idx == 3:\n",
        "\n",
        "      for i in range(16):\n",
        "\n",
        "         h = np.random.randint(8,81)\n",
        "         w = np.random.randint(8,81)\n",
        "\n",
        "         kernel_size = (h,w)\n",
        "\n",
        "         img = random_erase(img,kernel_size)\n",
        "\n",
        "      \n",
        "   return img\n",
        "\n",
        "@jit(nopython=True)\n",
        "def tune_odd(val):\n",
        "\n",
        "   if (val % 2) == 0:\n",
        "\n",
        "      val = val + 1\n",
        "\n",
        "   return val\n",
        "\n",
        "#Gaussian Blur\n",
        "def GaussianBlur(img,kernel_shape,sigma=0):\n",
        "\n",
        "   \"\"\"\n",
        "   img -- numpy array\n",
        "   kernel_shape -- (int,int)\n",
        "   sigma -- real number\n",
        "   \"\"\"\n",
        "\n",
        "   return cv2.GaussianBlur(img,kernel_shape,sigma)\n",
        "\n",
        "\n",
        "##Gaussian Noise\n",
        "@jit(nopython=True)\n",
        "def ext_operator(val):\n",
        "\n",
        "  if val > 255 :\n",
        "\n",
        "    val = 255\n",
        "\n",
        "  elif val < 0:\n",
        "\n",
        "    val = 0\n",
        "\n",
        "  return val\n",
        "\n",
        "@jit(nopython=True)\n",
        "def GaussianNoise(img,low=8,high=64):\n",
        "\n",
        "  nH,nW,nC = img.shape\n",
        "\n",
        "  for h in range(nH):\n",
        "\n",
        "    for w in range(nW):\n",
        "\n",
        "      for c in range(nC):\n",
        "\n",
        "        factor = np.random.randint(low,high)\n",
        "\n",
        "        loc = factor * np.random.random()\n",
        "\n",
        "        scale = factor * np.random.random()\n",
        "\n",
        "        noise = np.random.normal(loc,scale)\n",
        "\n",
        "        img[h,w,c] = ext_operator(img[h,w,c]+noise)\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "#brightness\n",
        "def random_brightness(img):\n",
        "\n",
        "   return tf.image.random_brightness(img,0.4).numpy()\n",
        "\n",
        "\n",
        "#saturation\n",
        "def random_saturation(img):\n",
        "\n",
        "   return tf.image.random_saturation(img,1.5,8.0).numpy()\n",
        "\n",
        "#contrast\n",
        "def random_contrast(img):\n",
        "\n",
        "   return tf.image.random_contrast(img,1.5,8.0).numpy()\n",
        "\n",
        "#hue\n",
        "def random_hue(img):\n",
        "\n",
        "   return tf.image.random_hue(img,0.4).numpy()\n",
        "\n",
        "#erase\n",
        "@jit(nopython=True)\n",
        "def random_erase(img,kernel_size=(64,64)):\n",
        "\n",
        "   nH,nW,_ = img.shape\n",
        "   fH,fW = kernel_size\n",
        "\n",
        "   new_nH = nH - fH + 1\n",
        "   new_nW = nW - fW + 1\n",
        "\n",
        "   nH_start = np.random.randint(0,new_nH)\n",
        "   nW_start = np.random.randint(0,new_nW)\n",
        "\n",
        "   nH_end = nH_start + fH\n",
        "   nW_end = nW_start + fW\n",
        "\n",
        "   erase_region = img[nH_start:nH_end,nW_start:nW_end,:].copy()\n",
        "\n",
        "   for h in range(nH_start,nH_end):\n",
        "\n",
        "      for w in range(nW_start,nW_end):\n",
        "         \n",
        "         random_h = np.random.randint(0,fH)\n",
        "         random_w = np.random.randint(0,fW)\n",
        "\n",
        "         img[h,w,:] = erase_region[random_h,random_w,:].copy()\n",
        "\n",
        "\n",
        "   return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmDNlWm8l7FF"
      },
      "source": [
        "#generator\n",
        "def get_gt_data(batch_size,img_info,class_info,img_path,img_shape = (640,640),aug_flag=False):\n",
        "\n",
        "   \"\"\"\n",
        "   #img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   \n",
        "   img_list_shuffled = list(img_info.keys())\n",
        "   \n",
        "   random.shuffle(img_list_shuffled)\n",
        "\n",
        "   m = len(img_list_shuffled)\n",
        "\n",
        "   idx = 0\n",
        "\n",
        "   while m >= batch_size:\n",
        "\n",
        "      \"\"\"\n",
        "      #check remaining sample\n",
        "      #if m < batch_size:\n",
        "\n",
        "         #break\n",
        "      \"\"\"\n",
        "      \n",
        "      #get name list\n",
        "      name_list = []\n",
        "\n",
        "      for i in range(idx*batch_size,(idx+1)*batch_size):\n",
        "\n",
        "         name_list.append(img_list_shuffled[i])\n",
        "\n",
        "\n",
        "      #get image data -- np.array\n",
        "      img_data = get_image_data(name_list,img_path,img_shape,aug_flag)\n",
        "\n",
        "      #get y_true data -- tuple (np.array,np.array,np.array)\n",
        "      label = get_y_true(name_list,img_info,class_info,img_shape)\n",
        "\n",
        "      #update remaining sample\n",
        "      m = m - batch_size\n",
        "      idx = idx + 1\n",
        "\n",
        "      yield img_data,label\n",
        "\n",
        "\n",
        "      \n",
        "def get_image_data(name_list,img_path,img_shape=(640,640),aug_flag=False):\n",
        "\n",
        "   \"\"\"\n",
        "   return numpy.ndarray\n",
        "   \"\"\"\n",
        "\n",
        "   img_data = []\n",
        "\n",
        "   for name in name_list:\n",
        "\n",
        "      #img -- numpy.ndarray\n",
        "      img = cv2.imread(f\"{img_path}/{name}\")\n",
        "\n",
        "      #calibrate image\n",
        "      img = preprocess_image(img,img_shape)\n",
        "\n",
        "      #img = cv2.resize(img,(128,128))\n",
        "      #data augmentation\n",
        "      if aug_flag:\n",
        "\n",
        "         img = data_aug(img)\n",
        "\n",
        "      #save img\n",
        "      img_data.append(img)\n",
        "\n",
        "   img_data = np.array(img_data)\n",
        "\n",
        "   return img_data\n",
        "\n",
        "\n",
        "def get_y_true(name_list,img_info,class_info,img_shape = (640,640)):\n",
        "\n",
        "   \"\"\"\n",
        "   name_list -- list\n",
        "   img_info -- dict -- {obj1:[[class,xmin,ymin,xcenter,ycenter],[class,xmin,ymin,xcenter,ycenter],...],obj2:...} (for each key)\n",
        "   class_info -- dict\n",
        "   standard_scale -- dict (small , medium , large)\n",
        "   img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   #initialize y_true\n",
        "   small_true = []\n",
        "   medium_true = []\n",
        "   large_true = []\n",
        "   \n",
        "   \n",
        "   for name in name_list:\n",
        "\n",
        "\n",
        "      #initialize y_true extra dim will be removed when it is saved (it is used for overlap region checking)\n",
        "      obj_small_true = np.zeros((80,80,85))\n",
        "      obj_medium_true = np.zeros((40,40,85))\n",
        "      obj_large_true = np.zeros((20,20,85))\n",
        "\n",
        "      #obj_small_true = np.zeros((16,16,91))\n",
        "      #obj_medium_true = np.zeros((8,8,91))\n",
        "      #obj_large_true = np.zeros((4,4,91))\n",
        "\n",
        "      #get (obj_info -- list)\n",
        "      obj_info = img_info[name]\n",
        "\n",
        "      n = len(obj_info)\n",
        "\n",
        "      #initial bbox hw\n",
        "      bbox_hw = np.zeros((n,2))\n",
        "\n",
        "      #loop via all object in the image (obj -- list) to fill bbox_hw\n",
        "      for i in range(n):\n",
        "\n",
        "         bbox_hw[i,0] = (obj_info[i][4] - obj_info[i][2])*2\n",
        "         bbox_hw[i,1] = (obj_info[i][3] - obj_info[i][1])*2\n",
        "\n",
        "      #get cluster index\n",
        "      cluster_idx = Kmean_IOU(bbox_hw)\n",
        "      \n",
        "      #update y_true\n",
        "      obj_small_true,obj_medium_true,obj_large_true = update_y_true(obj_info,class_info,cluster_idx,obj_small_true,obj_medium_true,obj_large_true,img_shape = (640,640))\n",
        "      \n",
        "      #save image info\n",
        "      small_true.append(obj_small_true[:,:,:])\n",
        "      medium_true.append(obj_medium_true[:,:,:])\n",
        "      large_true.append(obj_large_true[:,:,:])\n",
        "\n",
        "   #convert y_true to numpy array\n",
        "   small_true = np.array(small_true)\n",
        "   medium_true = np.array(medium_true)\n",
        "   large_true = np.array(large_true)\n",
        "\n",
        "   return (large_true,medium_true,small_true)\n",
        "\n",
        "   \n",
        "def update_y_true(obj_info,class_info,cluster_idx,obj_small_true,obj_medium_true,obj_large_true,img_shape = (640,640)):\n",
        "\n",
        "   \"\"\"\n",
        "   obj -- list [class,xmin,ymin,xcenter,ycenter]\n",
        "   img_shape -- (height,width)\n",
        "   \"\"\"\n",
        "   \"\"\"\n",
        "   _,xmin,ymin,xcenter,ycenter = obj\n",
        "\n",
        "   #avoid x,y division by zeros for ratio calculation\n",
        "   xmin = xmin + 1e-18\n",
        "   ymin = ymin + 1e-18\n",
        "   xcenter = xcenter + 1e-18\n",
        "   ycenter = ycenter + 1e-18\n",
        "\n",
        "   width = (xcenter - xmin) * 2\n",
        "   height = (ycenter - ymin) * 2\n",
        "\n",
        "   xmax = xmin + width\n",
        "   ymax = ymin + height\n",
        "\n",
        "   area  = width * height\n",
        "   \"\"\"\n",
        "   #update large obj\n",
        "   for i in cluster_idx[0]:\n",
        "\n",
        "      #get obj info\n",
        "      class_name,xmin,ymin,xcenter,ycenter = obj_info[i]\n",
        "\n",
        "      class_id = class_info[class_name]\n",
        "\n",
        "      xmax = (xcenter - xmin) * 2 + xmin\n",
        "      ymax = (ycenter - ymin) * 2 + ymin\n",
        "\n",
        "      #set up\n",
        "      step_h = obj_large_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_large_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter)\n",
        "      \n",
        "      #prob\n",
        "      obj_large_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "      #xmin,ymin\n",
        "      obj_large_true[h_pos,w_pos,1] = xmin \n",
        "      obj_large_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "      #xcenter,ycenter\n",
        "      obj_large_true[h_pos,w_pos,3] = xcenter \n",
        "      obj_large_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "      #class\n",
        "      obj_large_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "      #multiple positive\n",
        "      obj_large_true = multiple_positive_labeling(obj_large_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "\n",
        "\n",
        "   #update medium obj\n",
        "   for i in cluster_idx[1]:\n",
        "\n",
        "      #get obj info\n",
        "      class_name,xmin,ymin,xcenter,ycenter = obj_info[i]\n",
        "\n",
        "      class_id = class_info[class_name]\n",
        "\n",
        "      xmax = (xcenter - xmin) * 2 + xmin\n",
        "      ymax = (ycenter - ymin) * 2 + ymin\n",
        "\n",
        "      #set up\n",
        "      step_h = obj_medium_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_medium_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter) \n",
        "         \n",
        "      #prob\n",
        "      obj_medium_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "      #xmin,ymin\n",
        "      obj_medium_true[h_pos,w_pos,1] = xmin \n",
        "      obj_medium_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "      #xcenter,ycenter\n",
        "      obj_medium_true[h_pos,w_pos,3] = xcenter \n",
        "      obj_medium_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "      #class\n",
        "      obj_medium_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "      #multiple positive\n",
        "      obj_medium_true = multiple_positive_labeling(obj_medium_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "\n",
        "\n",
        "   #update small obj\n",
        "   for i in cluster_idx[2]:\n",
        "\n",
        "      #get obj info\n",
        "      class_name,xmin,ymin,xcenter,ycenter = obj_info[i]\n",
        "\n",
        "      class_id = class_info[class_name]\n",
        "\n",
        "      xmax = (xcenter - xmin) * 2 + xmin\n",
        "      ymax = (ycenter - ymin) * 2 + ymin\n",
        "\n",
        "      #set up\n",
        "      step_h = obj_small_true.shape[0] / img_shape[0]\n",
        "      step_w = obj_small_true.shape[1] / img_shape[1]\n",
        "\n",
        "      h_pos = int(step_h * ycenter)\n",
        "      w_pos = int(step_w * xcenter)\n",
        "      \n",
        "      #prob\n",
        "      obj_small_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "      #xmin,ymin\n",
        "      obj_small_true[h_pos,w_pos,1] = xmin \n",
        "      obj_small_true[h_pos,w_pos,2] = ymin \n",
        "\n",
        "      #xcenter,ycenter\n",
        "      obj_small_true[h_pos,w_pos,3] = xcenter \n",
        "      obj_small_true[h_pos,w_pos,4] = ycenter \n",
        "\n",
        "      #class\n",
        "      obj_small_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "\n",
        "      #multiple positive\n",
        "      obj_small_true = multiple_positive_labeling(obj_small_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h)\n",
        "                 \n",
        "   return obj_small_true,obj_medium_true,obj_large_true\n",
        "\n",
        "            \n",
        "@jit(nopython=True)  \n",
        "def multiple_positive_labeling(y_true,class_id,xmin,ymin,xmax,ymax,xcenter,ycenter,step_w,step_h):\n",
        "\n",
        "   \"\"\"\n",
        "   y_true -- numpy array\n",
        "   \"\"\"\n",
        "\n",
        "   xlow = int(xmin*step_w)\n",
        "   ylow = int(ymin*step_h)\n",
        "\n",
        "   xhigh = int(xmax*step_w)\n",
        "   yhigh = int(ymax*step_h)\n",
        "   \n",
        "   w_pos_init = int(xcenter*step_w - 32*step_w)\n",
        "   h_pos_init = int(ycenter*step_h - 32*step_h)\n",
        "\n",
        "   w_max = int(w_pos_init + 3*32*step_w)\n",
        "   h_max = int(h_pos_init + 3*32*step_h)\n",
        "\n",
        "   n = 0\n",
        "\n",
        "   for w_pos in range(w_pos_init,w_max):\n",
        "\n",
        "      for h_pos in range(h_pos_init,h_max):\n",
        "\n",
        "         if n >= 5:\n",
        "\n",
        "           break\n",
        "\n",
        "         if (y_true[h_pos,w_pos,0] == 0) and (w_pos > xlow) and (w_pos < xhigh) and (h_pos > ylow) and (h_pos < yhigh):\n",
        "\n",
        "            #prob\n",
        "            y_true[h_pos,w_pos,0] = 1\n",
        "\n",
        "            #xmin,ymin\n",
        "            y_true[h_pos,w_pos,1] = xmin + y_true[h_pos,w_pos,1] \n",
        "            y_true[h_pos,w_pos,2] = ymin + y_true[h_pos,w_pos,2]\n",
        "\n",
        "            #xcenter,ycenter\n",
        "            y_true[h_pos,w_pos,3] = xcenter + y_true[h_pos,w_pos,3]\n",
        "            y_true[h_pos,w_pos,4] = ycenter + y_true[h_pos,w_pos,4]\n",
        "\n",
        "            #class\n",
        "            y_true[h_pos,w_pos,5+class_id] = 1\n",
        "\n",
        "            n = n + 1\n",
        "\n",
        "\n",
        "   return y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QILjQw-aXg9v"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiRB-rLim8u"
      },
      "source": [
        "#find current path\n",
        "cur_path = os.getcwd()\n",
        "\n",
        "#define strategy\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "\n",
        "#define model,loss,optimizer\n",
        "with strategy.scope():\n",
        "\n",
        "   #define loss object\n",
        "   loss_object = alpha_loss(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "   #define compute loss\n",
        "   def compute_loss(labels,predictions):\n",
        "\n",
        "      #large\n",
        "      large_obj_loss = loss_object(labels[0],predictions[0])\n",
        "\n",
        "      #medium\n",
        "      medium_obj_loss = loss_object(labels[1],predictions[1])\n",
        "\n",
        "      #small\n",
        "      small_obj_loss = loss_object(labels[2],predictions[2])\n",
        "\n",
        "      #total loss\n",
        "      total_loss = large_obj_loss + medium_obj_loss + small_obj_loss\n",
        "\n",
        "      return total_loss \n",
        "\n",
        "   #define learning rate scheduler\n",
        "   #lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001,decay_steps=5,alpha=1e-5)\n",
        "\n",
        "   #define optimizer\n",
        "   #optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
        "   optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "   #define model\n",
        "   model = alpha_model()\n",
        "   #model.load_weights(f\"{cur_path}/gdrive/MyDrive/model_weights\")\n",
        "   #model = tf.keras.models.load_model(f\"{cur_path}/gdrive/MyDrive/model\")\n",
        "   \n",
        "#step function\n",
        "@tf.function\n",
        "def distributed_train_step(data_inputs):\n",
        "\n",
        "   per_replica_losses = strategy.run(train_step,args=(data_inputs,))\n",
        "\n",
        "   return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_losses,axis=None)\n",
        "\n",
        "\n",
        "def train_step(inputs):\n",
        "\n",
        "   images,labels = inputs\n",
        "\n",
        "   with tf.GradientTape() as tape:\n",
        "\n",
        "      predictions = model(images,train_flag=True)\n",
        "      \n",
        "      loss = compute_loss(labels,predictions)\n",
        "\n",
        "   gradients = tape.gradient(loss,model.trainable_weights)\n",
        "\n",
        "   optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n",
        "\n",
        "   return loss\n",
        "\n",
        "batch_size_per_replica = 4\n",
        "\n",
        "EPOCHS = 300\n",
        "\n",
        "#preprocessing class\n",
        "input_path = f\"{cur_path}/gdrive/MyDrive/annotations/train_annotations.csv\"\n",
        "save_path = f\"{cur_path}/gdrive/MyDrive/data\"\n",
        "\n",
        "class_train_info = preprocess_class(input_path,save_path)\n",
        "\n",
        "#preprocessing label\n",
        "input_path = f\"{cur_path}/gdrive/MyDrive/annotations/test_annotations.csv\"\n",
        "save_path = f\"{cur_path}/gdrive/MyDrive/data\"\n",
        "\n",
        "img_train_info = preprocessing_label(input_path,save_path)\n",
        "\n",
        "#train image path\n",
        "img_train_path = f\"{cur_path}/gdrive/MyDrive/img\"\n",
        "\n",
        "img_shape = (640,640)\n",
        "standard_scale=(19360,66930)\n",
        "\n",
        "#get number of sample m\n",
        "m = len(list(img_train_info.keys()))\n",
        "\n",
        "#dataset size\n",
        "global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "buffer_size = global_batch_size * 2\n",
        "\n",
        "#total step per epochs\n",
        "total_step_per_epoch = int(m/buffer_size)\n",
        "\n",
        "\n",
        "#aug flag\n",
        "aug_flag = False\n",
        "\n",
        "#train\n",
        "for i  in range(EPOCHS):\n",
        "\n",
        "   total_loss = 0.0\n",
        "\n",
        "   if (i+1) < 295:\n",
        "\n",
        "     aug_flag = True\n",
        "\n",
        "   else:\n",
        "\n",
        "     aug_flag = False\n",
        "\n",
        "   #get data \n",
        "   for train_images, train_labels in get_gt_data(buffer_size,img_train_info,class_train_info,img_train_path,img_shape,aug_flag):\n",
        "\n",
        "      #cast data\n",
        "      train_images = train_images.astype(np.float64)\n",
        "\n",
        "      # Create Datasets from the batches\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size).batch(global_batch_size)\n",
        "\n",
        "      #create distributed dataset\n",
        "      train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "\n",
        "      #Do training\n",
        "      for batch in train_dist_dataset:\n",
        "\n",
        "         total_loss = total_loss + distributed_train_step(batch) \n",
        "   \n",
        "   total_loss = total_loss \n",
        "   \n",
        "   print(f\"Epoch {i+1} , Loss: {total_loss}\")\n",
        "\n",
        "   #save model for each epoch\n",
        "   if ((i+1)%10) == 0:\n",
        "\n",
        "     #model.save(f\"{cur_path}/gdrive/MyDrive/model\")\n",
        "     model.save_weights(f\"{cur_path}/gdrive/MyDrive/model_weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWqOutS8rbr"
      },
      "source": [
        "#tf.keras.models.save_model(model,f\"{cur_path}/gdrive/MyDrive/model\")\n",
        "model.save(f\"{cur_path}/gdrive/MyDrive/model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dwMEyq6NKxF"
      },
      "source": [
        "def draw_anchor_box(img,feat,reversed_class_map,class_color_map):\n",
        "\n",
        "  \"\"\"\n",
        "  img -- numpy_array\n",
        "  feat -- list [class,left_x,left_y,center_x,center_y]\n",
        "  \"\"\"\n",
        "\n",
        "  left_x = feat[1]\n",
        "  left_y = feat[2]\n",
        "\n",
        "  width = (feat[3]-feat[1])*2\n",
        "  height = (feat[4]-feat[2])*2\n",
        "\n",
        "  right_x = feat[1] + width\n",
        "  right_y = feat[2] + height\n",
        "\n",
        "  updated_img = cv2.rectangle(img,(int(left_x),int(left_y)),(int(right_x),int(right_y)),class_color_map[feat[0]],2)\n",
        "\n",
        "  updated_img = cv2.putText(updated_img,reversed_class_map[feat[0]],(int(left_x),int(left_y)),cv2.FONT_HERSHEY_DUPLEX,0.8,class_color_map[feat[0]],2,cv2.LINE_AA)\n",
        "\n",
        "  return updated_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnWBieqNAz3O"
      },
      "source": [
        "@tf.function\n",
        "def step_predict(img,model):\n",
        "\n",
        "   predictions = model(img,training=False)\n",
        "\n",
        "   return predictions\n",
        "\n",
        "def load_model(model_path):\n",
        "\n",
        "   #define model\n",
        "   #model = tf.keras.models.load_model(model_path)\n",
        "   model = alpha_model()\n",
        "   #model.load_weights(f\"{cur_path}/gdrive/MyDrive/model_weights\")\n",
        "   model.load_weights(model_path)\n",
        "\n",
        "   return model\n",
        "\n",
        "def get_image_generator(batch_size,image_path,standard_shape=(640,640)):\n",
        "\n",
        "   filename_list = os.listdir(image_path)\n",
        "\n",
        "   idx = 0\n",
        "\n",
        "   while True:\n",
        "\n",
        "      img_data = []\n",
        "\n",
        "      batch_filename_list = filename_list[ idx * batch_size : (idx+1) * batch_size ]\n",
        "\n",
        "      if len(batch_filename_list) == 0:\n",
        "\n",
        "         break\n",
        "\n",
        "      for name in batch_filename_list:\n",
        "         \n",
        "         #read image\n",
        "         img = cv2.imread(f\"{image_path}/{name}\").astype(np.float64)\n",
        "\n",
        "         #pad image\n",
        "         img = preprocess_image(img,standard_shape)\n",
        "\n",
        "         #save img\n",
        "         img_data.append(img)\n",
        "\n",
        "      img_data = np.array(img_data)\n",
        "\n",
        "      yield img_data\n",
        "\n",
        "      idx = idx + 1\n",
        "\n",
        "def predict(model,image_path,result_path,reversed_class_map,class_color_map,confidence_threshold=0.8,diou_threshold=0.4,batch_size=8,standard_shape=(640,640)):\n",
        "\n",
        "   img_idx = 0\n",
        "\n",
        "   #get batch of image\n",
        "   for img_data in get_image_generator(batch_size,image_path,standard_shape) :\n",
        "\n",
        "      #get prediction\n",
        "      y = step_predict(img_data,model)\n",
        "\n",
        "      #analysis and draw data\n",
        "      for i in range(batch_size):\n",
        "\n",
        "         img = img_data[i].copy()\n",
        "\n",
        "         #large object\n",
        "         large_obj = y[0][i].numpy()\n",
        "         large_confirmed_anchor_box = get_confirmed_anchor_box(large_obj,confidence_threshold)\n",
        "         \n",
        "         #medium object\n",
        "         medium_obj = y[1][i].numpy()\n",
        "         medium_confirmed_anchor_box = get_confirmed_anchor_box(medium_obj,confidence_threshold)\n",
        "         \n",
        "         #small object\n",
        "         small_obj = y[2][i].numpy()\n",
        "         small_confirmed_anchor_box = get_confirmed_anchor_box(small_obj,confidence_threshold)\n",
        "\n",
        "         #final anchor box -- final_anchor_box -- list: [ (prob1,feat_vec1) , (prob2,feat_vec2) , ... ]\n",
        "         final_anchor_box = large_confirmed_anchor_box + medium_confirmed_anchor_box + small_confirmed_anchor_box\n",
        "\n",
        "         #sort in descending order\n",
        "         final_anchor_box.sort(key=lambda x:x[0],reverse=True)\n",
        "\n",
        "         #nms\n",
        "         selected_object = non_max_supression(final_anchor_box,diou_threshold)\n",
        "         \n",
        "         #draw prediction\n",
        "         img = draw_box_based_on_feat(img,selected_object,reversed_class_map,class_color_map)\n",
        "\n",
        "         #save the result\n",
        "         cv2.imwrite(f\"{result_path}/res_{img_idx}.jpg\",img)\n",
        "\n",
        "         #update img_idx\n",
        "         img_idx = img_idx + 1\n",
        "\n",
        "      \"\"\"\n",
        "      print(type(y))\n",
        "\n",
        "      print(type(y[0]))\n",
        "      \"\"\"\n",
        "      \n",
        "      yield y\n",
        "      \n",
        "   \n",
        "\n",
        "   with open(f\"{os.getcwd()}/gdrive/MyDrive/data/predict_small.txt\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps((small_obj).tolist()))\n",
        "\n",
        "      file.close()\n",
        "\n",
        "   with open(f\"{os.getcwd()}/gdrive/MyDrive/data/predict_medium.txt\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps((medium_obj).tolist()))\n",
        "\n",
        "      file.close()\n",
        "\n",
        "   with open(f\"{os.getcwd()}/gdrive/MyDrive/data/predict_large.txt\",\"w\") as file:\n",
        "\n",
        "      file.write(json.dumps((large_obj).tolist()))\n",
        "\n",
        "      file.close()   \n",
        "   \n",
        "   \n",
        "def draw_box_based_on_feat(img,selected_object,reversed_class_map,class_color_map):\n",
        "\n",
        "\n",
        "   for obj in selected_object:\n",
        "\n",
        "      class_idx = np.argmax(obj[5:])\n",
        "\n",
        "      #set feat\n",
        "      feat = [class_idx,obj[1],obj[2],obj[3],obj[4]]\n",
        "\n",
        "      #draw box\n",
        "      img = draw_anchor_box(img,feat,reversed_class_map,class_color_map)\n",
        "\n",
        "\n",
        "   return img\n",
        "\n",
        "\n",
        "def analyse_feature(feat,confidence_threshold=0.8,diou_threshold=0.4):\n",
        "\n",
        "   \"\"\"\n",
        "   feat -- numpy array\n",
        "   \"\"\"\n",
        "   \n",
        "   #filtering high prob anchor box - confirmed_anchor_box -- list: [ (prob1,feat_vec1) , (prob2,feat_vec2) , ... ]\n",
        "   confirmed_anchor_box = get_confirmed_anchor_box(feat,confidence_threshold)\n",
        "\n",
        "   #sort in descending order according to prob * class\n",
        "   confirmed_anchor_box.sort(key=lambda x:x[0],reverse=True)\n",
        "   \n",
        "   #non-max suppression\n",
        "   selected_object = non_max_supression(confirmed_anchor_box,diou_threshold)\n",
        "\n",
        "   return selected_object\n",
        "      \n",
        "\n",
        "def get_confirmed_anchor_box(feat,confidence_threshold=0.8):\n",
        "\n",
        "   nH,nW,nC = feat.shape\n",
        "\n",
        "   confirmed_anchor_box = []\n",
        "   \n",
        "   for h in range(nH):\n",
        "\n",
        "      for w in range(nW):\n",
        "\n",
        "         feat_vec = feat[h,w,:].copy()\n",
        "         \n",
        "         class_idx = np.argmax(feat_vec[5:])\n",
        "         \n",
        "         prob = feat_vec[5+class_idx] * feat_vec[0]\n",
        "\n",
        "         if prob >= confidence_threshold:\n",
        "\n",
        "            confirmed_anchor_box.append((prob,feat_vec))\n",
        "\n",
        "   return confirmed_anchor_box\n",
        "\n",
        "\n",
        "def non_max_supression(confirmed_anchor_box,diou_threshold=0.4):\n",
        "\n",
        "   \"\"\"\n",
        "   confirmed_anchor_box -- list: [ (prob1,feat_vec1) , (prob2,feat_vec2) , ... ]\n",
        "   \"\"\"\n",
        "\n",
        "   selected_object = []\n",
        "   m = len(confirmed_anchor_box)\n",
        "\n",
        "   while m != 0:\n",
        "\n",
        "      #get and save largest fect_vec -- feat_vec: numpy.array\n",
        "      feat_vec = confirmed_anchor_box[0][1]\n",
        "      selected_object.append(feat_vec)\n",
        "\n",
        "      #pop it from confirmed_anchor_box\n",
        "      confirmed_anchor_box.pop(0)\n",
        "\n",
        "      #get confirmed_anchor_box length\n",
        "      m = len(confirmed_anchor_box)\n",
        "\n",
        "      #compare it with diou\n",
        "      idx = 0\n",
        "      \n",
        "      for i in range(m):\n",
        "\n",
        "         #get diou\n",
        "         diou_val = DIOU(feat_vec,confirmed_anchor_box[idx][1])\n",
        "\n",
        "         if diou_val > diou_threshold:\n",
        "\n",
        "            confirmed_anchor_box.pop(idx)\n",
        "\n",
        "         else:\n",
        "\n",
        "            idx = idx + 1\n",
        "      \n",
        "      m = len(confirmed_anchor_box)\n",
        "\n",
        "   return selected_object\n",
        "\n",
        "@jit(nopython=True)\n",
        "def DIOU(feat_1,feat_2):\n",
        "\n",
        "   #get left pos\n",
        "   left_1 = feat_1[1:3]\n",
        "   left_2 = feat_2[1:3]\n",
        "\n",
        "   #get center\n",
        "   center_1 = feat_1[3:5]\n",
        "   center_2 = feat_2[3:5]\n",
        "\n",
        "   #get width , height\n",
        "   wh_1 = (center_1[:] - left_1[:])*2\n",
        "   wh_2 = (center_2[:] - left_2[:])*2\n",
        "\n",
        "   #get right pos\n",
        "   right_1 = left_1[:] + wh_1[:]\n",
        "   right_2 = left_2[:] + wh_2[:]\n",
        "\n",
        "   ################## IOU ##################\n",
        "   left_intersection = np.maximum(left_1,left_2)\n",
        "   \n",
        "   right_intersection = np.minimum(right_1,right_2)\n",
        "   right_intersection = np.maximum(left_intersection,right_intersection)\n",
        "\n",
        "   wh_intersection = right_intersection[:] - left_intersection[:]\n",
        "\n",
        "   intersection_area = wh_intersection[0] * wh_intersection[1]\n",
        "\n",
        "   union_area = wh_1[0] * wh_1[1] + wh_2[0] * wh_2[1] - intersection_area\n",
        "   \n",
        "   iou_val  = intersection_area / (union_area + 1e-10)\n",
        "\n",
        "   ################## IOU ##################\n",
        "\n",
        "   ################## distance ratio ##################\n",
        "   outermost_left = np.minimum(left_1,left_2)\n",
        "   outermost_right = np.maximum(right_1,right_2)\n",
        "\n",
        "   outermost_distance = np.sum(np.square(outermost_right[:] - outermost_left[:]))\n",
        "\n",
        "   center_distance = np.sum(np.square(center_1[:] - center_2[:]))\n",
        "\n",
        "   distance_ratio = center_distance/(outermost_distance+1e-10)\n",
        "\n",
        "   ################## distance ratio ##################\n",
        "\n",
        "   diou_val = iou_val - distance_ratio\n",
        "\n",
        "   return diou_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0SyNNGvqU75"
      },
      "source": [
        "def preprocess_class_color_map(class_info,save_path,name=\"class_color_map.txt\"):\n",
        "\n",
        "  num_class = len(list(class_info.keys()))\n",
        "\n",
        "  step = int(256*3/num_class)\n",
        "\n",
        "  class_color_map = {}\n",
        "  x,y,z = 0,0,0\n",
        "\n",
        "  for k in class_info.values():\n",
        "\n",
        "    if x < 255:\n",
        "\n",
        "      x = x + step\n",
        "\n",
        "    elif y < 255:\n",
        "\n",
        "      y = y + step\n",
        "\n",
        "    elif z < 255:\n",
        "\n",
        "      z = z + step\n",
        "\n",
        "    if x > 255:\n",
        "\n",
        "      x = 255\n",
        "\n",
        "    elif y > 255:\n",
        "\n",
        "      y = 255\n",
        "\n",
        "    elif z > 255:\n",
        "\n",
        "      z = 255\n",
        "\n",
        "    class_color_map[k] = (x,y,z)\n",
        "\n",
        "  with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "    file.write(json.dumps(class_color_map))\n",
        "\n",
        "    file.close()\n",
        "\n",
        "  return class_color_map\n",
        "\n",
        "def reverse_class_info(class_info,save_path,name=\"reversed_class_map.txt\"):\n",
        "\n",
        "  reversed_class_map = {}\n",
        "\n",
        "  for k in class_info.keys():\n",
        "\n",
        "    val = class_info[k]\n",
        "\n",
        "    reversed_class_map[val] = k\n",
        "\n",
        "  with open(f\"{save_path}/{name}\",\"w\") as file:\n",
        "\n",
        "    file.write(json.dumps(reversed_class_map))\n",
        "\n",
        "    file.close()\n",
        "\n",
        "  return reversed_class_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH1zUvtSNj2e"
      },
      "source": [
        "import time\n",
        "path = os.getcwd()\n",
        "\n",
        "data_path =  f\"{path}/gdrive/MyDrive/data\"\n",
        "\n",
        "#get class info\n",
        "file = open(f\"{data_path}/class_map.txt\")\n",
        "class_info = json.load(file)\n",
        "file.close()\n",
        "\n",
        "class_color_map = preprocess_class_color_map(class_info,data_path)\n",
        "reversed_class_info = reverse_class_info(class_info,data_path)\n",
        "\n",
        "\n",
        "model_path = f\"{path}/gdrive/MyDrive/model_weights\"\n",
        "image_path = f\"{path}/gdrive/MyDrive/pending_to_analysis\"\n",
        "result_path = f\"{path}/gdrive/MyDrive/result\"\n",
        "\n",
        "model = load_model(model_path)\n",
        "\n",
        "start = time.time()\n",
        "for y in predict(model,image_path,result_path,reversed_class_info,class_color_map,confidence_threshold=0.6,diou_threshold=0.5,batch_size=1):\n",
        "\n",
        "  print(f\"FPS: {1/(time.time()-start)}\")\n",
        "  start = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}