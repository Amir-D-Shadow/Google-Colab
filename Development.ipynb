{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Development.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv1qDZAp8x4oFh+1vOe/3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amir-D-Shadow/Google-Colab/blob/main/Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQKQdph3A3JT"
      },
      "source": [
        "from numba import cuda,float64,int64\n",
        "import numpy as np\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeBFQsqZBL32"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:],float64[:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def func3D(A,dZ,W,stride,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  A -- n_H_prev + 2 * opadH, n_W_prev + 2*opadW,n_C_prev\n",
        "  dZ -- n_H,n_W,n_C\n",
        "  W -- fH,fW,n_C_prev,n_C\n",
        "  \"\"\"\n",
        "\n",
        "  n_H = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  n_W = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  n_C = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (n_H < Hlim) and (n_W < Wlim) and (n_C < Clim):\n",
        "\n",
        "    fH,fW,n_C_prev,_ = W.shape\n",
        "\n",
        "    for h in range(fH):\n",
        "\n",
        "      for w in range(fW):\n",
        "\n",
        "        for c in range(n_C_prev):\n",
        "\n",
        "          IMG_H = n_H * stride + h\n",
        "          IMG_W = n_W * stride + w\n",
        "\n",
        "          A[IMG_H,IMG_W,c] = A[IMG_H,IMG_W,c] + W[h,w,c,n_C] * dZ[n_H,n_W,n_C]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNL0z6qOQ1t5",
        "outputId": "e5bb9dbc-4832-4a20-e89a-044b248af7bf"
      },
      "source": [
        "opadH,opadW,stride = 2,2,1\n",
        "m = 2\n",
        "number_of_filters = 1\n",
        "\n",
        "tmp = np.zeros((m,1,1,1)).astype(np.float64)\n",
        "W = np.ones((2,2,1,number_of_filters)).astype(np.float64)\n",
        "dZ = np.ones((m,2,2,number_of_filters)).astype(np.float64)\n",
        "A = np.pad(tmp,((0,0),(opadH,opadH),(opadW,opadW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "m,n_H,n_W,n_C = dZ.shape \n",
        "\n",
        "threadsperblock = (2,2,2)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "W_device = cuda.to_device(W)\n",
        "\n",
        "for i in range(m):\n",
        "\n",
        "  dZ_device = cuda.to_device(dZ[i,:,:,:].copy())\n",
        "  A_device = cuda.to_device(A[i,:,:,:].copy())\n",
        "\n",
        "  func3D[blockspergrid,threadsperblock](A_device,dZ_device,W_device,stride,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "  A[i,:,:,:] = A_device.copy_to_host()\n",
        "\n",
        "res = A[:,opadH:-opadH,opadW:-opadW,:]\n",
        "\n",
        "print(res)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[1.]]]\n",
            "\n",
            "\n",
            " [[[1.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMSItj3nW-h"
      },
      "source": [
        "@cuda.jit\n",
        "def test3D(A,B,out,Hlim,Wlim,Clim):\n",
        "\n",
        "  x = cuda.threadIdx.x + cuda.blockDim.x*cuda.blockIdx.x\n",
        "  y = cuda.threadIdx.y + cuda.blockDim.y*cuda.blockIdx.y\n",
        "  z = cuda.threadIdx.z + cuda.blockDim.z*cuda.blockIdx.z\n",
        "\n",
        "  if (x<Hlim) and (y<Wlim) and (z<Clim):\n",
        "\n",
        "    out[x,y,z] = A[x,y,z]*B[x,y,z]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlFQbhtxoN67",
        "outputId": "1d9db66f-b98f-4518-fe41-5d1a19ac446f"
      },
      "source": [
        "opadH,opadW,stride = 2,2,1\n",
        "m = 2\n",
        "number_of_filters = 1\n",
        "\n",
        "A = np.full((3,3,3),6)\n",
        "B = np.full((3,3,3),7)\n",
        "\n",
        "n_H,n_W,n_C = A.shape\n",
        "\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B)\n",
        "out_device = cuda.to_device(np.zeros((3,3,3)))\n",
        "\n",
        "threadsperblock = (2,2,2)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "test3D[blockspergrid,threadsperblock](A_device,B_device,out_device,n_H,n_W,n_C)\n",
        "cuda.synchronize()\n",
        "res = out_device.copy_to_host()\n",
        "print(res)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]\n",
            "\n",
            " [[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]\n",
            "\n",
            " [[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_E02IazIHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d79c84a-a5fc-43b3-a73d-b2a3319aa743"
      },
      "source": [
        "a = np.arange(4).reshape(2,2)\n",
        "b = np.zeros((2,2))\n",
        "\n",
        "c = np.array([a,b,b])\n",
        "k1 = np.rot90(c,1,axes=(0,1))\n",
        "k2 = np.rot90(c,1,axes=(1,2))\n",
        "k3 = np.rot90(c,1,axes=(0,2))\n",
        "print(\"c:\",c,\"\\n\")\n",
        "print(\"k1:\",k1,\"\\n\")\n",
        "print(\"k2:\",k2,\"\\n\")\n",
        "print(\"k3\",k3,\"\\n\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c: [[[0. 1.]\n",
            "  [2. 3.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]] \n",
            "\n",
            "k1: [[[2. 3.]\n",
            "  [0. 0.]\n",
            "  [0. 0.]]\n",
            "\n",
            " [[0. 1.]\n",
            "  [0. 0.]\n",
            "  [0. 0.]]] \n",
            "\n",
            "k2: [[[1. 3.]\n",
            "  [0. 2.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]] \n",
            "\n",
            "k3 [[[1. 0. 0.]\n",
            "  [3. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [2. 0. 0.]]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQXSeDq5ok-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1326a61-6eff-4eab-b3c3-1ae5f84f825c"
      },
      "source": [
        "opadH,opadW,stride = 2,2,1\n",
        "m = 2\n",
        "number_of_filters = 1\n",
        "\n",
        "dA = np.full((3,3,3),6)\n",
        "B = np.full((3,3,3),7)\n",
        "\n",
        "n_H,n_W,n_C = A.shape\n",
        "\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B)\n",
        "out_device = cuda.to_device(np.zeros((3,3,3)))\n",
        "\n",
        "threadsperblock = (4,4,4)\n",
        "\n",
        "blockspergrid_H = n_H\n",
        "blockspergrid_W = n_W\n",
        "blockspergrid_C = n_C\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "test3D[blockspergrid,threadsperblock](A_device,B_device,out_device,n_H,n_W,n_C)\n",
        "cuda.synchronize()\n",
        "res = out_device.copy_to_host()\n",
        "print(res)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]\n",
            "\n",
            " [[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]\n",
            "\n",
            " [[42. 42. 42.]\n",
            "  [42. 42. 42.]\n",
            "  [42. 42. 42.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIdHuunw9fYX"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def func3D_new(dA_prev,W,dZ,stride,Hlim,Wlim,Clim):\n",
        "    \n",
        "    \"\"\"\n",
        "    dA_prev -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "    W -- (fH,fW,n_C_prev,n_C)\n",
        "    dZ -- (m,n_H,n_W,n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    IMG_H = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "    IMG_W = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "    IMG_C = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "    \n",
        "    if IMG_H<Hlim and IMG_W<Wlim and IMG_C<Clim:\n",
        "        \n",
        "      fH,fW,n_C_prev,_ = W.shape\n",
        "      m,n_H,n_W,n_C = dZ.shape\n",
        "      for i in range(m):\n",
        "        \n",
        "        for h in range(fH):\n",
        "\n",
        "          for w in range(fW):\n",
        "            \n",
        "            for c in range(n_C):\n",
        "\n",
        "              dA_prev[i,IMG_H,IMG_W,IMG_C] = dA_prev[i,IMG_H,IMG_W,IMG_C] + W[h,w,IMG_C,c] * dZ[i,h,w,c]\n",
        "                    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFqXgHnBKRv"
      },
      "source": [
        "def zero_padding(img,padH,padW):\n",
        "\n",
        "        \"\"\"\n",
        "        img : numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "        pad : amount of padding around each image on vertical and horizontal dimensions\n",
        "        \"\"\"\n",
        "\n",
        "        return np.pad(img,((0,0),(padH,padH),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "\n",
        "def same_padding(img,stride,fH,fW):\n",
        "    \n",
        "        \"\"\"\n",
        "        img : numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "        fH -- Kernel Size (height)\n",
        "        fW -- Kernel Size (width)\n",
        "        s -- Stride\n",
        "        \"\"\"\n",
        "        n_H, n_W = img.shape[1],img.shape[2]\n",
        "        padH = int(math.ceil(((n_H-1)*stride+fH-n_H)/2))\n",
        "        padW = int(math.ceil(((n_W-1)*stride+fW-n_W)/2))\n",
        "\n",
        "        img_same_pad = zero_padding(img,padH,padW)\n",
        "\n",
        "        return img_same_pad,padH,padW"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5Hv3mu-92re",
        "outputId": "a69ce158-8b85-4938-e9f9-8bcec8d0e263"
      },
      "source": [
        "opadH,opadW,stride = 1,1,1\n",
        "m = 1\n",
        "number_of_filters = 1\n",
        "\n",
        "dA_prev = np.full((m,1,1,1),0).astype(np.float64)\n",
        "dA_prev_pad = np.pad(dA_prev,((0,0),(opadH,opadH),(opadW,opadW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "_,n_H_prev,n_W_prev,n_C_prev = dA_prev_pad.shape\n",
        "\n",
        "W = np.full((2,2,1,number_of_filters),1).astype(np.float64)\n",
        "fH,fW,n_C_prev,n_C = W.shape\n",
        "\n",
        "n_H = int((n_H_prev-fH+2*opadH)/stride)+1\n",
        "n_W = int((n_W_prev-fW+2*opadW)/stride)+1\n",
        "dZ = np.full((m,n_H,n_W,n_C),1).astype(np.float64)\n",
        "\n",
        "\n",
        "\n",
        "threadsperblock = (2,2,2)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H_prev/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W_prev/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "dA_prev_pad_device = cuda.to_device(dA_prev_pad)\n",
        "dZ_device = cuda.to_device(dZ)\n",
        "W_device = cuda.to_device(W)\n",
        "cuda.synchronize()\n",
        "    \n",
        "func3D_new[blockspergrid,threadsperblock](dA_prev_pad_device,W_device,dZ_device,stride,n_H_prev,n_W_prev,n_C_prev)\n",
        "cuda.synchronize()\n",
        "    \n",
        "    \n",
        "dA_prev_pad = dA_prev_pad_device.copy_to_host()   \n",
        "dA_prev = dA_prev_pad[:,opadH:-opadH,opadW:-opadW,:] \n",
        "\n",
        "print(dA_prev_pad.shape)\n",
        "print(dA_prev)\n",
        "\n",
        "print(dA_prev_pad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 3, 1)\n",
            "[[[[4.]]]]\n",
            "[[[[4.]\n",
            "   [4.]\n",
            "   [4.]]\n",
            "\n",
            "  [[4.]\n",
            "   [4.]\n",
            "   [4.]]\n",
            "\n",
            "  [[4.]\n",
            "   [4.]\n",
            "   [4.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8m137oaGoD8"
      },
      "source": [
        "@cuda.jit\n",
        "def add(dA_prev,Hlim,Wlim,Clim):\n",
        "\n",
        "    IMG_H = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "    IMG_W = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "    IMG_C = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "    if IMG_H<Hlim and IMG_W<Wlim and IMG_C<Clim:\n",
        "\n",
        "      m = dA_prev.shape[0]\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        dA_prev[i,IMG_H,IMG_W,IMG_C] = dA_prev[i,IMG_H,IMG_W,IMG_C] + 10\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_UzDU4jG_bH",
        "outputId": "6053778e-3ad1-4fca-8ed6-8fb8762629bf"
      },
      "source": [
        "dA_prev = np.full((2,2,2,2),0)\n",
        "\n",
        "m,n_H_prev,n_W_prev,n_C_prev = dA_prev.shape\n",
        "\n",
        "threadsperblock = (2,2,2)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H_prev/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W_prev/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "dA_prev_device = cuda.to_device(dA_prev)\n",
        "cuda.synchronize()\n",
        "\n",
        "add[blockspergrid,threadsperblock](dA_prev_device,n_H_prev,n_W_prev,n_C_prev)\n",
        "\n",
        "dA_prev = dA_prev_device.copy_to_host()\n",
        "print(dA_prev)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[10 10]\n",
            "   [10 10]]\n",
            "\n",
            "  [[10 10]\n",
            "   [10 10]]]\n",
            "\n",
            "\n",
            " [[[10 10]\n",
            "   [10 10]]\n",
            "\n",
            "  [[10 10]\n",
            "   [10 10]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgP6b5gCeody",
        "outputId": "50eee134-71d6-4771-cb33-4d794539f138"
      },
      "source": [
        "nH = 5\n",
        "nW = 5\n",
        "\n",
        "array = np.arange(25).reshape(5,5)\n",
        "sum = np.zeros_like(array)\n",
        "\n",
        "#get n_h tail\n",
        "n_h_tail = nH - 1\n",
        "\n",
        "for n_h in range(nH):\n",
        "\n",
        "  if n_h > n_h_tail:\n",
        "\n",
        "    break\n",
        "\n",
        "  #get n_w tail\n",
        "  n_w_tail = nW - 1\n",
        "\n",
        "  for n_w in range(nW):\n",
        "    \n",
        "    if n_w > n_w_tail:\n",
        "\n",
        "      break\n",
        "\n",
        "    if (n_w == n_w_tail) and (n_h_tail == n_h):\n",
        "\n",
        "      sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "\n",
        "    else:\n",
        "\n",
        "      #calculate head \n",
        "      sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "\n",
        "      #calculate tail\n",
        "      sum[n_h_tail,n_w_tail] = sum[n_h_tail,n_w_tail] + array[n_h_tail,n_w_tail]\n",
        "      \n",
        "    n_w_tail = n_w_tail - 1\n",
        "\n",
        "  n_h_tail = n_h_tail - 1\n",
        "\n",
        "print(array)\n",
        "print(\"\\n\")\n",
        "print(sum)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]]\n",
            "\n",
            "\n",
            "[[ 0  1  2  0  0]\n",
            " [ 5  6  7  0  0]\n",
            " [10 11 12 13 14]\n",
            " [ 0  0 17 18 19]\n",
            " [ 0  0 22 23 24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha6KcwbuikoF",
        "outputId": "ef3252ce-482a-464f-8180-5c7535d55b87"
      },
      "source": [
        "nH = 5\n",
        "nW = 5\n",
        "\n",
        "array = np.arange(25).reshape(5,5)\n",
        "sum = np.zeros_like(array)\n",
        "\n",
        "\n",
        "for n_h in range(nH):\n",
        "\n",
        "  #get n_w tail\n",
        "  n_w_tail = nW - 1\n",
        "\n",
        "  for n_w in range(nW):\n",
        "    \n",
        "    if n_w > n_w_tail:\n",
        "\n",
        "      break\n",
        "\n",
        "    if (n_w == n_w_tail):\n",
        "\n",
        "      sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "\n",
        "    else:\n",
        "\n",
        "      #calculate head \n",
        "      sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "\n",
        "      #calculate tail\n",
        "      sum[n_h,n_w_tail] = sum[n_h,n_w_tail] + array[n_h,n_w_tail]\n",
        "      \n",
        "    n_w_tail = n_w_tail - 1\n",
        "\n",
        "\n",
        "print(array)\n",
        "print(\"\\n\")\n",
        "print(sum)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]]\n",
            "\n",
            "\n",
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRwLmP-Dir_7",
        "outputId": "3b87abdd-1f5b-4836-eb6b-34676ca5c694"
      },
      "source": [
        "nH = 3\n",
        "nW = 5\n",
        "\n",
        "#array = np.arange(25).reshape(5,5)\n",
        "#array = np.arange(12).reshape(3,4)\n",
        "array = np.arange(15).reshape(3,5)\n",
        "sum = np.zeros_like(array)\n",
        "\n",
        "n_h_tail = nH - 1\n",
        "\n",
        "for n_h in range(nH):\n",
        "\n",
        "  if n_h > n_h_tail:\n",
        "\n",
        "    break\n",
        "\n",
        "  #get n_w tail\n",
        "  n_w_tail = nW - 1\n",
        "\n",
        "  for n_w in range(nW):\n",
        "    \n",
        "    if n_w > n_w_tail:\n",
        "\n",
        "      break\n",
        "\n",
        "    if (n_w == n_w_tail) or (n_h == n_h_tail): \n",
        "\n",
        "        if (n_h == n_h_tail) and (n_w == n_w_tail):\n",
        "\n",
        "          sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]#sum[n_h,n_w] = sum[n_h,n_w]/2\n",
        "\n",
        "        else:\n",
        "\n",
        "          sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "          sum[n_h_tail,n_w_tail] = sum[n_h_tail,n_w_tail] + array[n_h_tail,n_w_tail]\n",
        "\n",
        "    else:\n",
        "\n",
        "      #calculate head \n",
        "      sum[n_h,n_w] = sum[n_h,n_w] + array[n_h,n_w]\n",
        "\n",
        "      #calculate tail\n",
        "      sum[n_h_tail,n_w_tail] = sum[n_h_tail,n_w_tail] + array[n_h_tail,n_w_tail]\n",
        "\n",
        "      #calculate top left hand corner\n",
        "      sum[n_h_tail,n_w] = sum[n_h_tail,n_w] + array[n_h_tail,n_w]\n",
        "\n",
        "      #calculate botton left hand corner\n",
        "      sum[n_h,n_w_tail] = sum[n_h,n_w_tail] + array[n_h,n_w_tail]\n",
        "      \n",
        "    n_w_tail = n_w_tail - 1\n",
        "\n",
        "  n_h_tail = n_h_tail -1\n",
        "\n",
        "print(array)\n",
        "print(\"\\n\")\n",
        "print(sum)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]]\n",
            "\n",
            "\n",
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tAfSykTwAmH"
      },
      "source": [
        "#BatchNormalization\n",
        "\n",
        "#Helper Functions\n",
        "\n",
        "#sum\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:],int64,int64,int64,int64\")\n",
        "def sum_axis0_3D(x,sum_x,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  x -- (m,n_H,n_W,n_C)\n",
        "  sum_x -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    #sum over each sample via different axis\n",
        "    for i in range(m):\n",
        "\n",
        "      sum_x[nh,nw,nc] = sum_x[nh,nw,nc] + x[i,nh,nw,nc]\n",
        "\n",
        "#mean\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:],int64,int64,int64,int64\")     \n",
        "def mean_axis0_3D(x,mean_x,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  x -- (m,n_H,n_W,n_C)\n",
        "  mu -- mean\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      mean_x[nh,nw,nc] = mean_x[nh,nw,nc] + x[i,nh,nw,nc]/m\n",
        "\n",
        "#variance\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:],float64[:,:,:],int64,int64,int64,int64\")\n",
        "def var_axis0_3D(x,var_x,mean_x,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  x -- (m,n_H,n_W,n_C)\n",
        "  var_x -- (n_H,n_W,n_C)\n",
        "  mean_x -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      var_x[nh,nw,nc] = var_x[nh,nw,nc] + (x[i,nh,nw,nc] - mean_x[nh,nw,nc])*(x[i,nh,nw,nc] - mean_x[nh,nw,nc])/m\n",
        "      \n",
        "\n",
        "#Z_norm\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:],float64[:,:,:],float64,int64,int64,int64,int64\")\n",
        "def Z_norm_3D(x,z_norm,var_x,mean_x,epsilon,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  x -- (m,n_H,n_W,n_C)\n",
        "  z_norm -- (m,n_H,n_W,n_C)\n",
        "  var_x -- (n_H,n_W,n_C)\n",
        "  mean_x -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      z_norm[i,nh,nw,nc] = (x[i,nh,nw,nc] - mean_x[nh,nw,nc])/math.sqrt(var_x[nh,nw,nc]+epsilon)\n",
        "\n",
        "#Z_S\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:],float64[:,:,:],int64,int64,int64,int64\")\n",
        "def Z_S_3D(z_s,z_norm,gamma,beta,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  z_s -- (m,n_H,n_W,n_C)\n",
        "  z_norm -- (m,n_H,n_W,n_C)\n",
        "  gamma -- (n_H,n_W,n_C)\n",
        "  beta -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      z_s[i,nh,nw,nc] = gamma[nh,nw,nc] * z_norm[i,nh,nw,nc] + beta[nh,nw,nc]\n",
        "\n",
        "#Exponentially Weighted Average\n",
        "@cuda.jit(\"float64[:,:,:],float64[:,:,:],float64,int64,int64,int64\")\n",
        "def exp_weighted_avg_3D(running_x,x,momentum,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  running_x -- (n_H,n_W,n_C)\n",
        "  x -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    running_x[nh,nw,nc] = momentum * running_x[nh,nw,nc] + (1 - momentum) * x[nh,nw,nc]\n",
        "\n",
        "# derivative variance\n",
        "@cuda.jit(\"float64[:,:,:],float64[:,:,:,:],float64[:,:,:],float64[:,:,:],float64[:,:,:,:],float64,int64,int64,int64,int64\")\n",
        "def dev_var_3D(dvar,z,mean_z,var_z,dz_norm,epsilon,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dvar -- (n_H,n_W,n_C)\n",
        "  z -- (m,n_H,n_W,n_C)\n",
        "  mean_z -- (n_H,n_W,n_C)\n",
        "  var_z -- (n_H,n_W,n_C)\n",
        "  dz_norm -- (m,n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "  \n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    std_z = math.pow(var_z[nh,nw,nc]+epsilon,1.5)\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      dvar[nh,nw,nc] = dvar[nh,nw,nc] + (-(z[i,nh,nw,nc]-mean_z[nh,nw,nc])*dz_norm[i,nh,nw,nc]/(2*std_z))\n",
        "  \n",
        "\n",
        "# derivative mean\n",
        "@cuda.jit(\"float64[:,:,:],float64[:,:,:],float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:],float64[:,:,:],float64,int64,int64,int64,int64\")\n",
        "def dev_mean_3D(dmean,dvar,dz_norm,z,mean_z,var_z,epsilon,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dmean -- (n_H,n_W,n_C)\n",
        "  dvar -- (n_H,n_W,n_C)\n",
        "  dz_norm -- (m,n_H,n_W,n_C)\n",
        "  z -- (m,n_H,n_W,n_C)\n",
        "  mean_z -- (n_H,n_W,n_C)\n",
        "  var_z -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "  \n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    std_z = math.pow(var_z[nh,nw,nc]+epsilon,1.5)\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      dmean[nh,nw,nc] = dmean[nh,nw,nc] + (-dz_norm[i,nh,nw,nc]/std_z) + (-2*(z[i,nh,nw,nc]-mean_z[nh,nw,nc])*dvar[nh,nw,nc]/m) \n",
        "\n",
        "# derivative dz\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:],float64[:,:,:],float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:],float64[:,:,:],float64,int64,int64,int64,int64\")\n",
        "def dev_z_3D(dz,dmean,dvar,dz_norm,z,mean_z,var_z,epsilon,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dz -- (m,n_H,n_W,n_C)\n",
        "  dmean -- (n_H,n_W,n_C)\n",
        "  dvar -- (n_H,n_W,n_C)\n",
        "  dz_norm -- (m,n_H,n_W,n_C)\n",
        "  z -- (m,n_H,n_W,n_C)\n",
        "  mean_z -- (n_H,n_W,n_C)\n",
        "  var_z -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    std_z = math.sqrt(var_z[nh,nw,nc]+epsilon)\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      dz[i,nh,nw,nc] = dmean[nh,nw,nc]/m + 2*(z[i,nh,nw,nc]-mean_z[nh,nw,nc])*dvar[nh,nw,nc]/m + dz_norm[i,nh,nw,nc]/std_z\n",
        "\n",
        "# derivative dgamma\n",
        "@cuda.jit(\"float64[:,:,:],float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def dev_gamma_3D(dgamma,dz_s,z_norm,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dgamma -- (n_H,n_W,n_C)\n",
        "  dz_s -- (m,n_H,n_W,n_C)\n",
        "  z_norm -- (m,n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      dgamma[nh,nw,nc] = dgamma[nh,nw,nc] + z_norm[i,nh,nw,nc] * dz_s[i,nh,nw,nc]\n",
        "\n",
        "#derivative Z_NORM\n",
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:],int64,int64,int64,int64\")\n",
        "def dev_z_norm(dz_norm,dz_s,gamma,m,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dz_norm -- (m,n_H,n_W,n_C)\n",
        "  dz_s -- (m,n_H,n_W,n_C)\n",
        "  gamma -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  nh = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  nw = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  nc = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (nh < Hlim) and (nw < Wlim) and (nc < Clim):\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      dz_norm[i,nh,nw,nc] = gamma[nh,nw,nc] * dz_s[i,nh,nw,nc]\n",
        "  \n",
        "\n",
        "#BN Forward\n",
        "def BatchNormalization_Forward3D(Z,batch_para,running_para,epsilon = 1e-10,threadsperblock = (8,8,8),mode=\"TRAIN\"):\n",
        "\n",
        "  \"\"\"\n",
        "  Z -- (m,n_H,n_W,n_C)\n",
        "  running_para -- {running_mean,running_var,momentum}\n",
        "  batch_para -- {gamma,beta}\n",
        "  gamma -- (n_H,n_W,n_C)\n",
        "  beta -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  #set up\n",
        "  running_mean = running_para[\"running_mean\"]\n",
        "  running_var = running_para[\"running_var\"]\n",
        "  momentum = running_para[\"momentum\"]\n",
        "\n",
        "  gamma = batch_para[\"gamma\"]\n",
        "  beta = batch_para[\"beta\"]\n",
        "\n",
        "  #Get shape Z\n",
        "  m,n_H,n_W,n_C = Z.shape\n",
        "  Z_device = cuda.to_device(Z)\n",
        "\n",
        "  #blockspergrid\n",
        "  blockspergrid_H = int(math.ceil(n_H/threadsperblock[0]))\n",
        "  blockspergrid_W = int(math.ceil(n_W/threadsperblock[1]))\n",
        "  blockspergrid_C = int(math.ceil(n_C/threadsperblock[2]))\n",
        "\n",
        "  blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "  #Forward\n",
        "  if mode == \"TRAIN\":\n",
        "\n",
        "    #calculate mean\n",
        "    mean_x = np.zeros((n_H,n_W,n_C))\n",
        "    mean_x_device = cuda.to_device(mean_x)\n",
        "    mean_axis0_3D[blockspergrid,threadsperblock](Z_device,mean_x_device,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    mean_x = mean_x_device.copy_to_host()\n",
        "\n",
        "    #calculate var\n",
        "    var_x = np.zeros((n_H,n_W,n_C))\n",
        "    var_x_device = cuda.to_device(var_x)\n",
        "    var_axis0_3D[blockspergrid,threadsperblock](Z_device,var_x_device,mean_x_device,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    var_x = var_x_device.copy_to_host()\n",
        "\n",
        "    #update running mean\n",
        "    running_mean_device = cuda.to_device(running_mean)\n",
        "    exp_weighted_avg_3D[blockspergrid,threadsperblock](running_mean_device,mean_x_device,momentum,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    running_para[\"running_mean\"] = running_mean_device.copy_to_host()\n",
        "\n",
        "    #update running var\n",
        "    running_var_device = cuda.to_device(running_var)\n",
        "    exp_weighted_avg_3D[blockspergrid,threadsperblock](running_var_device,var_x_device,momentum,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    running_para[\"running_var\"] = running_var_device.copy_to_host()\n",
        "    \n",
        "    #Calculate Z_norm\n",
        "    Z_NORM = np.zeros_like(Z)\n",
        "    Z_NORM_device = cuda.to_device(Z_NORM)\n",
        "    Z_norm_3D[blockspergrid,threadsperblock](Z_device,Z_NORM_device,var_x_device,mean_x_device,epsilon,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    Z_NORM = Z_NORM_device.copy_to_host()\n",
        "\n",
        "    #Calculate Z_S\n",
        "    Z_S = np.zeros_like(Z)\n",
        "    Z_S_device = cuda.to_device(Z_S)\n",
        "    Z_S_3D[blockspergrid,threadsperblock](Z_S_device,Z_NORM_device,gamma,beta,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    Z_S = Z_S_device.copy_to_host()\n",
        "\n",
        "    #Save cache for backpropagation\n",
        "    cacheL = (Z,Z_NORM,Z_S,gamma,beta,epsilon,mean_x,var_x)\n",
        "\n",
        "  else:\n",
        "\n",
        "    #calculate Z_norm\n",
        "    running_mean_device = cuda.to_device(running_mean)\n",
        "    running_var_device = cuda.to_device(running_var)\n",
        "\n",
        "    Z_NORM = np.zeros_like(Z)\n",
        "    Z_NORM_device = cuda.to_device(Z_NORM)\n",
        "    Z_norm_3D[blockspergrid,threadsperblock](Z_device,Z_NORM_device,running_var_x_device,running_mean_x_device,epsilon,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "\n",
        "    #Calculate Z_S\n",
        "    Z_S = np.zeros_like(Z)\n",
        "    Z_S_device = cuda.to_device(Z_S)\n",
        "    Z_S_3D[blockspergrid,threadsperblock](Z_S_device,Z_NORM_device,gamma,beta,m,n_H,n_W,n_C)\n",
        "    cuda.synchronize()\n",
        "    Z_S = Z_S_device.copy_to_host()\n",
        "\n",
        "    cacheL = ()\n",
        "\n",
        "  return Z_S,cacheL\n",
        "\n",
        "#BN Backward\n",
        "def BatchNormalization_Backward3D(dZ_S,cacheL,threadsperblock=(8,8,8)):\n",
        "\n",
        "  \"\"\"\n",
        "  dZ_S -- (m,n_H,n_W,n_C)\n",
        "  cacheL -- (Z,Z_NORM,Z_S,gamma,beta,epsilon,mean_x,var_x)\n",
        "  gamma -- (n_H,n_W,n_C)\n",
        "  beta -- (n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "  #Get cache\n",
        "  Z,Z_NORM,Z_S,gamma,beta,epsilon,mean_x,var_x = cacheL\n",
        "\n",
        "  #Move gamma and beta to GPU\n",
        "  gamma_device = cuda.to_device(gamma)\n",
        "  beta_device = cuda.to_device(beta)\n",
        "\n",
        "  #Move Z,Z_S,Z_NORM to GPU\n",
        "  Z_S_device = cuda.to_device(Z_S)\n",
        "  Z_NORM_device = cuda.to_device(Z_NORM)\n",
        "  Z_device = cuda.to_device(Z)\n",
        "\n",
        "  #Move mean_x,var_x to gpu\n",
        "  mean_x_device = cuda.to_device(mean_x)\n",
        "  var_x_device = cuda.to_device(var_x)\n",
        "  \n",
        "  #Get shape dZ_S\n",
        "  m,n_H,n_W,n_C = dZ_S.shape\n",
        "  dZ_S_device = cuda.to_device(dZ_S)\n",
        "\n",
        "  #Define Blockspergrid\n",
        "  blockspergrid_H = int(math.ceil(n_H/threadsperblock[0]))\n",
        "  blockspergrid_W = int(math.ceil(n_W/threadsperblock[1]))\n",
        "  blockspergrid_C = int(math.ceil(n_C/threadsperblock[2]))\n",
        "\n",
        "  blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "  #dZ_NORM\n",
        "  dZ_NORM = np.zeros_like(dZ_S)\n",
        "  dZ_NORM_device = cuda.to_device(dZ_NORM)\n",
        "  dev_z_norm[blockspergrid,threadsperblock](dZ_NORM_device,dZ_S_device,gamma_device,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #dbeta\n",
        "  dbeta = np.zeros_like(beta)\n",
        "  dbeta_device = cuda.to_device(dbeta)\n",
        "  sum_axis0_3D[blockspergrid,threadsperblock](dZ_S_device,dbeta_device,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "  dbeta = dbeta_device.copy_to_host()\n",
        "\n",
        "  #dgamma\n",
        "  dgamma = np.zeros_like(gamma)\n",
        "  dgamma_device = cuda.to_device(dgamma)\n",
        "  dev_gamma_3D[blockspergrid,threadsperblock](dgamma_device,dZ_S_device,Z_NORM_device,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "  dgamma = dgamma_device.copy_to_host()\n",
        "\n",
        "  #dvar\n",
        "  dvar = np.zeros_like(var_x)\n",
        "  dvar_device = cuda.to_device(dvar)\n",
        "  dev_var_3D[blockspergrid,threadsperblock](dvar_device,Z_device,mean_x_device,var_x_device,dZ_NORM_device,epsilon,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #dmean\n",
        "  dmean = np.zeros_like(mean_x)\n",
        "  dmean_device = cuda.to_device(dmean)\n",
        "  dev_mean_3D[blockspergrid,threadsperblock](dmean_device,dvar_device,dZ_NORM_device,Z_device,mean_x_device,var_x_device,epsilon,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #dZ\n",
        "  dZ = np.zeros_like(Z)\n",
        "  dZ_device = cuda.to_device(dZ)\n",
        "  dev_z_3D[blockspergrid,threadsperblock](dZ_device,dmean_device,dvar_device,dZ_NORM_device,Z_device,mean_x_device,var_x_device,epsilon,m,n_H,n_W,n_C)\n",
        "  cuda.synchronize()\n",
        "  dZ = dZ_device.copy_to_host()\n",
        "\n",
        "  return dZ,dgamma,dbeta"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cujicsGAxFRc",
        "outputId": "9e558f7f-70bd-4fa8-d431-ab58b6995861"
      },
      "source": [
        "Z = np.random.randn(10,1080,1920,3)\n",
        "Z_device = cuda.to_device(Z)\n",
        "\n",
        "m,n_H,n_W,n_C = Z.shape\n",
        "\n",
        "batch_para = {}\n",
        "running_para = {}\n",
        "\n",
        "running_para[\"running_mean\"] = np.zeros((n_H,n_W,n_C)).astype(\"float64\")\n",
        "running_para[\"running_var\"] = np.zeros((n_H,n_W,n_C)).astype(\"float64\")\n",
        "running_para[\"momentum\"] = 0.9\n",
        "\n",
        "batch_para[\"gamma\"] = np.random.randn(n_H,n_W,n_C)\n",
        "batch_para[\"beta\"] = np.random.randn(n_H,n_W,n_C)\n",
        "\n",
        "#GPU\n",
        "gpu_time = time.time()\n",
        "Z_S,cacheL = BatchNormalization_Forward3D(Z,batch_para,running_para)\n",
        "print(f\"GPU: {time.time()-gpu_time} \")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: 0.7095785140991211 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEenePmr41sQ",
        "outputId": "b562c9e7-a34e-4ad5-cb68-91c2c6f3cfc6"
      },
      "source": [
        "Z = np.random.randn(10,1080,1920,3)\n",
        "dZ_S = np.random.randn(10,1080,1920,3)\n",
        "m,n_H,n_W,n_C = Z.shape\n",
        "\n",
        "batch_para = {}\n",
        "running_para = {}\n",
        "\n",
        "running_para[\"running_mean\"] = np.zeros((n_H,n_W,n_C)).astype(\"float64\")\n",
        "running_para[\"running_var\"] = np.zeros((n_H,n_W,n_C)).astype(\"float64\")\n",
        "running_para[\"momentum\"] = 0.9\n",
        "\n",
        "batch_para[\"gamma\"] = np.random.randn(n_H,n_W,n_C)\n",
        "batch_para[\"beta\"] = np.random.randn(n_H,n_W,n_C)\n",
        "\n",
        "#GPU\n",
        "Z_S,cacheL = BatchNormalization_Forward3D(Z,batch_para,running_para)\n",
        "gpu_time = time.time()\n",
        "dZ,dgamma,dbeta = BatchNormalization_Backward3D(dZ_S,cacheL,threadsperblock=(8,8,8))\n",
        "print(f\"GPU: {time.time()-gpu_time} \")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: 1.050276279449463 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}